{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72cc2445-152c-48f0-a63a-a4c6550a4433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b890c9-2c03-439a-8e5e-9149febb7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size,config.d_model)\n",
    "        \n",
    "    def embed(self,x,position_ids=None,token_type_ids=None):\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "    \n",
    "    def unembed(self,x):\n",
    "        return torch.matmul(x,self.embedding.weight.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0d73ed-6193-484b-9e36-e303a20e85ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps \n",
    "        self.d_model = config.d_model\n",
    "        self.weight = nn.Parameter(torch.ones(self.d_model))\n",
    "        \n",
    "    def _norm(self, x):\n",
    "        return x / (torch.sqrt((x ** 2).mean(dim=-1, keepdim=True) + self.eps))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.weight * self._norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c011728c-5821-4901-a6cf-69f8b5301875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff,bias=False)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model,bias=False)\n",
    "        self.fc3 = nn.Linear(config.d_model, config.d_ff,bias=False)\n",
    "        self.act = F.silu\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.fc1(x)\n",
    "        x2 = self.fc3(x)\n",
    "        return self.fc2(self.act(x1) * self.act(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380bc2c2-f621-4c9f-8ef4-55ff7d77d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x,n_rep):\n",
    "    batch_size,seq_len,n_heads,head_dim = x.size()\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return x.unsqueeze(-2).expand(batch_size,seq_len,n_rep,n_heads,head_dim).reshape(batch_size,seq_len,n_heads*n_rep,head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915951fa-e6d5-4b94-b347-fce0d4c94365",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearlyScaledRotaryEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, scaling_factor=1.0, base=10000.0, device=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "        self._linear_scaling_factor = scaling_factor\n",
    "        self.inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2, device=device).float() / d_model))\n",
    "        self._seq_len_cached = 0\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "        self.scale = None\n",
    "    \n",
    "    def update_cache(self, seq_len, device=None, dtype=None):\n",
    "        if (seq_len > self._seq_len_cached or \n",
    "            self._cos_cached is None or \n",
    "            self._cos_cached.device != device or \n",
    "            self._cos_cached.dtype != dtype):\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=device, dtype=dtype) / self._linear_scaling_factor\n",
    "            freqs = torch.outer(t, self.inv_freq.to(device=device, dtype=dtype))\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x is assumed to be of shape [B, L, n_heads, head_dim] with head_dim == self.d_model\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        self.update_cache(seq_len, device=device, dtype=dtype)\n",
    "        cos, sin = self._cos_cached, self._sin_cached\n",
    "        \n",
    "        head_dim = x.size(-1)\n",
    "        split_dim = head_dim // 2\n",
    "        \n",
    "        # Split the last dimension into two halves.\n",
    "        x1 = x[..., :split_dim]  # shape: [B, L, n_heads, split_dim]\n",
    "        x2 = x[..., split_dim:]  # shape: [B, L, n_heads, split_dim]\n",
    "        \n",
    "        # Cos and sin have shape [L, d_model/2]; slice to [L, split_dim] and add extra dimensions:\n",
    "        cos = cos[:seq_len, :split_dim].unsqueeze(0).unsqueeze(2)  # [1, L, 1, split_dim]\n",
    "        sin = sin[:seq_len, :split_dim].unsqueeze(0).unsqueeze(2)  # [1, L, 1, split_dim]\n",
    "        \n",
    "        x1_rot = x1 * cos - x2 * sin\n",
    "        x2_rot = x1 * sin + x2 * cos\n",
    "        \n",
    "        return torch.cat([x1_rot, x2_rot], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e504f39a-1bfb-4066-9c77-07322f4c0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedMHA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads_q = config.n_heads_q\n",
    "        self.head_dim = self.d_model // self.n_heads_q\n",
    "        self.n_rep = self.n_heads_q // config.n_heads_kv\n",
    "        \n",
    "        self.wq = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wk = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wv = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wo = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        \n",
    "        self.rotary_embed = LinearlyScaledRotaryEmbedding(self.head_dim,base=10000.0,device=None)\n",
    "        \n",
    "    def forward(self,x,cache=None):\n",
    "        batch_size,seq_len,d_model = x.size()\n",
    "        q = self.wq(x).view(batch_size,seq_len,self.n_heads_q,self.head_dim)\n",
    "        k = self.wk(x).view(batch_size,seq_len,self.n_heads_q,self.head_dim)\n",
    "        v = self.wv(x).view(batch_size,seq_len,self.n_heads_q,self.head_dim)\n",
    "        \n",
    "        xq = self.rotary_embed(q)\n",
    "        xk = self.rotary_embed(k)\n",
    "        \n",
    "        if cache is not None:\n",
    "            if \"k\" in cache and cache[\"k\"] is not None:\n",
    "                k = torch.cat([cache[\"k\"], k], dim=1)  \n",
    "                v = torch.cat([cache[\"v\"], v], dim=1)\n",
    "            cache[\"k\"] = k\n",
    "            cache[\"v\"] = v\n",
    "        \n",
    "        xq = repeat_kv(xq,self.n_rep)\n",
    "        xk = repeat_kv(xk,self.n_rep)\n",
    "        \n",
    "        xq = xq.transpose(1,2)\n",
    "        xk = xk.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        attn = (xq @ xk.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(attn,dim=-1)\n",
    "        x = attn @ v\n",
    "        x = x.transpose(1,2).reshape(batch_size,seq_len,self.d_model)\n",
    "        return self.wo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99aa8680-8d64-4a51-89b2-c77cdbee2218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.mha = GroupedMHA(config)\n",
    "        self.pre_norm = RMSNorm(config)\n",
    "        self.post_norm = RMSNorm(config)\n",
    "        self.mlp = GatedMLP(config)\n",
    "        \n",
    "    def forward(self, x, padding_mask=None):\n",
    "        if isinstance(padding_mask, torch.Tensor):\n",
    "            x = x * padding_mask[..., None]\n",
    "        attn_out = self.mha(self.pre_norm(x))\n",
    "        x = x + attn_out\n",
    "        if isinstance(padding_mask, torch.Tensor):\n",
    "            x = x * padding_mask[..., None]\n",
    "        mlp_out = self.mlp(self.post_norm(x))\n",
    "        return x + mlp_out, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a476762-78c5-44ad-aae3-b4a324dbe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyenaCascade(nn.Module):\n",
    "    def __init__(self, config, hyena_filter_group=None, fir_inner_filter_length=None):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.short_filter_length = config.short_filter_length\n",
    "        # Use the projected channel dimension (which is 3*d_model) if not explicitly provided.\n",
    "        self.hyena_filter_group = hyena_filter_group or config.d_model  \n",
    "        self.state_size = config.state_size\n",
    "        \n",
    "        self.short_filter_weight = nn.Parameter(torch.randn(3 * config.d_model, 1, config.short_filter_length))\n",
    "        self.short_filter_bias = (nn.Parameter(torch.randn(3 * config.d_model))\n",
    "                                  if config.short_filter_bias else None)\n",
    "        \n",
    "        self.log_poles = nn.Parameter(torch.randn(self.hyena_filter_group, config.state_size)*0.01)\n",
    "        self.residues = nn.Parameter(torch.randn(self.hyena_filter_group, config.state_size))\n",
    "        self.D = nn.Parameter(torch.zeros(config.d_model))\n",
    "        self.h = None\n",
    "        self.t = None\n",
    "        \n",
    "    def update_time(self, seq_len, device):\n",
    "        if self.t is None or self.t.shape[-1] != seq_len:\n",
    "            self.t = torch.arange(seq_len, device=device).unsqueeze(0).unsqueeze(0)\n",
    "        else:\n",
    "            self.t = self.t[..., :seq_len]\n",
    "            \n",
    "    def compute_filter(self, seq_len, device):\n",
    "        self.update_time(seq_len, device)\n",
    "        h = (self.residues.unsqueeze(-1) * (self.log_poles.unsqueeze(-1) * self.t).exp()).sum(dim=1)\n",
    "        h = h.mean(dim=0, keepdim=True)\n",
    "        return h  \n",
    "    \n",
    "    def forward(self, x, inference_params=None, padding_mask=None):\n",
    "        batch_size, seq_len, in_channels = x.size()\n",
    "        x_t = x.transpose(1, 2)  \n",
    "        pad = (self.short_filter_length - 1) // 2\n",
    "        conv_out = F.conv1d(x_t, self.short_filter_weight, bias=self.short_filter_bias,\n",
    "                            padding=pad, groups=in_channels)\n",
    "        conv_out = conv_out.transpose(1, 2)  \n",
    "        conv_out = conv_out[..., :self.d_model]\n",
    "        \n",
    "        if self.h is None or self.h.shape[1] < seq_len:\n",
    "            self.h = self.compute_filter(seq_len, device=x.device).detach()  \n",
    "        h_exp = self.h.unsqueeze(-1)\n",
    "        x_val = x[..., :self.d_model]\n",
    "        y = conv_out * h_exp + x_val * self.D\n",
    "        if padding_mask is not None:\n",
    "            y = y * padding_mask.unsqueeze(-1)\n",
    "        return y, inference_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffb529f-e99f-4040-995f-ea60792e2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvBlock(nn.Module):\n",
    "    def __init__(self,config,hyena_filter_group=None,fir_inner_filter_length=None):\n",
    "        super().__init__()\n",
    "        self.pre_norm = RMSNorm(config)\n",
    "        self.post_norm = RMSNorm(config)\n",
    "        self.hyena = HyenaCascade(config,hyena_filter_group=hyena_filter_group,fir_inner_filter_length=fir_inner_filter_length)\n",
    "        self.mlp = GatedMLP(config)\n",
    "        self.proj = nn.Linear(config.d_model,3 * config.d_model,bias=True)\n",
    "        self.out_filter_dense = nn.Linear(config.d_model,config.d_model,bias=True)\n",
    "        \n",
    "    def proj_norm(self,x):\n",
    "        return self.proj(self.pre_norm(x))\n",
    "    \n",
    "    def res_mlp_norm(self,x):\n",
    "        return self.mlp(self.post_norm(x)) + x\n",
    "    \n",
    "    def forward(self,x,inference_params=None,padding_mask=None):\n",
    "        x = self.proj_norm(x)\n",
    "        if isinstance(padding_mask,torch.Tensor):\n",
    "            x = x * padding_mask[...,None]\n",
    "        x,inference_params = self.hyena(x,inference_params,padding_mask)\n",
    "        if isinstance(padding_mask,torch.Tensor):\n",
    "            x = x * padding_mask[...,None]\n",
    "        x = self.res_mlp_norm(x)\n",
    "        return self.out_filter_dense(x),inference_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772e5929-4727-48ec-830b-60ffd08f321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.mha = AttentionBlock(config)\n",
    "        self.pre_norm = RMSNorm(config)\n",
    "        self.post_norm = RMSNorm(config)\n",
    "        self.gated_conv = GatedConvBlock(config)\n",
    "        \n",
    "    def forward(self,x,padding_mask=None):\n",
    "        x = self.pre_norm(x)\n",
    "        x,_ = self.mha(x,padding_mask=padding_mask)\n",
    "        x = self.post_norm(x)\n",
    "        x,_ = self.gated_conv(x,padding_mask=padding_mask)\n",
    "        return x,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5ad5c4-1261-4766-9fc3-9b9864cfa661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StripedHyena(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed = Embedding(config)\n",
    "        self.norm = RMSNorm(config)\n",
    "        self.unembed = self.embed if config.get(\"tie_embed\") else Embedding(config)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "    def forward(self,x,padding_mask=None):\n",
    "        x = self.embed.embed(x)\n",
    "        if padding_mask is not None:\n",
    "            x = x * padding_mask.unsqueeze(-1)\n",
    "        for block in self.blocks:\n",
    "            x,_ = block(x,padding_mask=padding_mask)\n",
    "        x = self.norm(x)\n",
    "        x = self.unembed.unembed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b32f77c-aebd-47ad-b746-fce81ac53127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \"__main__\":\\n    # Define a simple configuration class.\\n    class Config:\\n        d_model = 128\\n        n_heads_q = 8\\n        n_heads_kv = 4\\n        vocab_size = 10000\\n        eps = 1e-5\\n        short_filter_length = 21\\n        state_size = 16\\n        n_layers = 4\\n        use_gated_conv = True\\n        tie_embed = False\\n        n_heads_kv = 8\\n        use_flashfft = False\\n        short_filter_bias = True\\n        device = \"cuda\"  # Change to \"cuda\" if GPU is available.\\n        inner_size_multiple_of = 64\\n        d_ff = 512  # Feed-forward dimension for GatedMLP\\n        \\n        def get(self, key, default=None):\\n            return getattr(self, key, default)\\n    \\n    config = Config()\\n    \\n    # Create dummy input: batch size 2, sequence length 50.\\n    dummy_input = torch.randint(0, config.vocab_size, (32, 128))\\n    # Create a padding mask (all ones means no padding).\\n    padding_mask = torch.ones(32, 128, dtype=torch.bool)\\n    \\n    # Instantiate the model.\\n    model = StripedHyena(config)\\n    \\n    # Forward pass.\\n    output = model(dummy_input, padding_mask=padding_mask)\\n    print(\"Output shape:\", output.shape)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''if __name__ == \"__main__\":\n",
    "    # Define a simple configuration class.\n",
    "    class Config:\n",
    "        d_model = 128\n",
    "        n_heads_q = 8\n",
    "        n_heads_kv = 4\n",
    "        vocab_size = 10000\n",
    "        eps = 1e-5\n",
    "        short_filter_length = 21\n",
    "        state_size = 16\n",
    "        n_layers = 4\n",
    "        use_gated_conv = True\n",
    "        tie_embed = False\n",
    "        n_heads_kv = 8\n",
    "        use_flashfft = False\n",
    "        short_filter_bias = True\n",
    "        device = \"cuda\"  # Change to \"cuda\" if GPU is available.\n",
    "        inner_size_multiple_of = 64\n",
    "        d_ff = 512  # Feed-forward dimension for GatedMLP\n",
    "        \n",
    "        def get(self, key, default=None):\n",
    "            return getattr(self, key, default)\n",
    "    \n",
    "    config = Config()\n",
    "    \n",
    "    # Create dummy input: batch size 2, sequence length 50.\n",
    "    dummy_input = torch.randint(0, config.vocab_size, (32, 128))\n",
    "    # Create a padding mask (all ones means no padding).\n",
    "    padding_mask = torch.ones(32, 128, dtype=torch.bool)\n",
    "    \n",
    "    # Instantiate the model.\n",
    "    model = StripedHyena(config)\n",
    "    \n",
    "    # Forward pass.\n",
    "    output = model(dummy_input, padding_mask=padding_mask)\n",
    "    print(\"Output shape:\", output.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdce50a2-a9ed-48e2-b932-d4bf37e0f57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 18014 samples\n",
      "Custom vocabulary size: 50972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 7339712\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 29.4444\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 24.9200\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 21.2122\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████████████████| 563/563 [01:43<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss: 17.7561\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████| 563/563 [01:43<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average loss: 14.5567\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 average loss: 11.4847\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 average loss: 8.5223\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 average loss: 6.4583\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|████████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 average loss: 5.8095\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 average loss: 5.5103\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 average loss: 5.2804\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 average loss: 5.0876\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 average loss: 4.9167\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 average loss: 4.7623\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 average loss: 4.6187\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|███████████████████████████████| 563/563 [01:42<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 average loss: 4.4840\n",
      "Sample generated text: The meaning of life is\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17:  22%|██████▋                        | 122/563 [00:22<01:21,  5.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    150\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(shift_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, shift_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), shift_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 151\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    152\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    154\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import math\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# Custom Tokenizer (Simple Example)\n",
    "# -------------------------------\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, texts, vocab_size=200000, pad_token=\"<pad>\", unk_token=\"<unk>\"):\n",
    "        self.pad_token = pad_token\n",
    "        self.unk_token = unk_token\n",
    "        tokens = []\n",
    "        for text in texts:\n",
    "            tokens.extend(text.split())\n",
    "        freq = {}\n",
    "        for token in tokens:\n",
    "            freq[token] = freq.get(token, 0) + 1\n",
    "        sorted_tokens = sorted(freq.items(), key=lambda x: x[1], reverse=True)[:vocab_size-2]\n",
    "        self.vocab = {pad_token: 0, unk_token: 1}\n",
    "        idx = 2\n",
    "        for token, _ in sorted_tokens:\n",
    "            self.vocab[token] = idx\n",
    "            idx += 1\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def encode(self, text, max_length=128, padding=\"max_length\", truncation=True):\n",
    "        tokens = text.split()\n",
    "        token_ids = [self.vocab.get(token, self.vocab[self.unk_token]) for token in tokens]\n",
    "        if truncation and len(token_ids) > max_length:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        if padding == \"max_length\":\n",
    "            pad_len = max_length - len(token_ids)\n",
    "            token_ids = token_ids + [self.vocab[self.pad_token]] * pad_len\n",
    "            attention_mask = [1]*min(len(tokens), max_length) + [0]*pad_len\n",
    "        else:\n",
    "            attention_mask = [1]*len(token_ids)\n",
    "        return {\"input_ids\": token_ids, \"attention_mask\": attention_mask}\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        # Stop at the pad token if found.\n",
    "        tokens = []\n",
    "        for t in token_ids:\n",
    "            if t == self.vocab[self.pad_token]:\n",
    "                break\n",
    "            tokens.append(inv_vocab.get(t, self.unk_token))\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "class Config:\n",
    "    d_model = 64              # Reduced hidden dimension\n",
    "    n_heads_q = 4             # Reduced number of heads\n",
    "    n_heads_kv = 4  \n",
    "    vocab_size = 200000      # Will be updated after building vocabulary\n",
    "    eps = 1e-5\n",
    "    short_filter_length = 15  # Smaller filter length\n",
    "    state_size = 8            # Smaller state size\n",
    "    n_layers = 6              # Fewer layers\n",
    "    use_gated_conv = True\n",
    "    tie_embed = False\n",
    "    use_flashfft = False\n",
    "    short_filter_bias = True\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inner_size_multiple_of = 32\n",
    "    d_ff = 256               # Reduced feed-forward dimension\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        return getattr(self, key, default)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train[:1%]\")\n",
    "print(\"Loaded dataset with\", len(dataset), \"samples\")\n",
    "\n",
    "all_texts = [sample[\"text\"] for sample in dataset if sample[\"text\"] is not None]\n",
    "tokenizer = CustomTokenizer(all_texts, vocab_size=200000)  # Smaller vocab for demonstration\n",
    "print(\"Custom vocabulary size:\", tokenizer.vocab_size)\n",
    "config.vocab_size = tokenizer.vocab_size\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for text in examples[\"text\"]:\n",
    "        encoded = tokenizer.encode(text, max_length=128, padding=\"max_length\", truncation=True)\n",
    "        outputs[\"input_ids\"].append(encoded[\"input_ids\"])\n",
    "        outputs[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
    "    return outputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Sample Generation Function\n",
    "# -------------------------------\n",
    "def generate_text(model, tokenizer, prompt, config, max_length=128, temperature=1.0, top_k=4, top_p=0.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoded = tokenizer.encode(prompt, max_length=128, padding=\"max_length\", truncation=True)\n",
    "        input_ids = torch.tensor(encoded[\"input_ids\"]).unsqueeze(0).to(config.device)\n",
    "        for _ in range(max_length - input_ids.size(1)):\n",
    "            logits = model(input_ids)[..., :]  # [1, L, vocab_size] if unembedding projects to vocab\n",
    "            next_logits = logits[:, -1, :] / temperature  # Apply temperature scaling\n",
    "            \n",
    "            # Optionally, apply top-k or top-p filtering here.\n",
    "            # For greedy sampling, simply choose the highest probability token:\n",
    "            next_token = torch.argmax(next_logits, dim=-1).unsqueeze(0)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        model.train()\n",
    "        return tokenizer.decode(input_ids.squeeze(0).tolist())\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Initialize Model and Optimizer\n",
    "# -------------------------------\n",
    "model = StripedHyena(config)\n",
    "model.to(config.device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total trainable parameters:\", count_parameters(model))\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop (Language Modeling)\n",
    "# -------------------------------\n",
    "model.train()\n",
    "num_epochs = 100\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(config.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, padding_mask=attention_mask)\n",
    "        \n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = input_ids[:, 1:].contiguous()\n",
    "        \n",
    "        loss = F.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "    scheduler.step(avg_loss)\n",
    "    # Sample generation after each epoch.\n",
    "    prompt = \"The meaning of life is\"\n",
    "    generated_text = generate_text(model, tokenizer, prompt, config, max_length=128)\n",
    "    print(\"Sample generated text:\", generated_text)\n",
    "    \n",
    "    # Save the model checkpoint.\n",
    "    torch.save(model.state_dict(), f\"striped_hyena_epoch_{epoch+1}.pt\")\n",
    "\n",
    "# Plot the loss history.\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs+1), loss_history, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"training_loss.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64708fb6-e5ef-42fe-b6b0-169f3ac111c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
