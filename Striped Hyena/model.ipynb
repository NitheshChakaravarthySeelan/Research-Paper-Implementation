{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734bfd29-3eba-4f15-9ee9-137398e1a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc8d23f-3d64-4ecd-b94b-5406aff5a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.eps = float(config.eps)\n",
    "        self.weight = nn.Parameter(torch.ones(config.d_model))\n",
    "\n",
    "    def _norm(self,x):\n",
    "        return x * torch.rsqrt(torch.mean(x**2,dim=-1,keepdim=True)+self.eps)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57581abd-d24e-4055-a010-dda24250bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelGatedMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.d_ff = config.d_ff\n",
    "        self.act = F.silu\n",
    "        self.gate = nn.Linear(config.d_model,config.d_ff,bias=False)\n",
    "        self.fc1 = nn.Linear(config.d_model,config.d_ff,bias=False)\n",
    "        self.fc2 = nn.Linear(config.d_ff,config.d_model,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y = self.fc2(self.act(self.gate(x) * self.fc1(x)))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ccc9413-78a4-4ba3-9047-8e37f797d845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearlyScaledRotaryEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.device = config.device\n",
    "        self._linear_scaling_factor = config.scaling_factor\n",
    "        self.inv_freq = 1.0 / (config.base ** (torch.arange(0,self.d_model,2,device=self.device).float() / self.d_model))\n",
    "        self._seq_len_cached = 0\n",
    "        self._cos_cached = None\n",
    "        self._sin_cached = None\n",
    "\n",
    "    def update_cache(self,seq_len,device=None,dtype=None):\n",
    "        if seq_len > self._seq_len_cached or self._cos_cached is None or self._cos_cached.device != device or self._cos_cached.dtype != dtype:\n",
    "            self._seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len,device=device,dtype=dtype) / self._linear_scaling_factor\n",
    "            freqs = torch.outer(t,self.inv_freq.to(dtype))\n",
    "            self._cos_cached = torch.cos(freqs).to(dtype)\n",
    "            self._sin_cached = torch.sin(freqs).to(dtype)\n",
    "\n",
    "    def forward(self,x):\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        self.update_cache(seq_len,device=device,dtype=dtype)\n",
    "        cos,sin = self._cos_cached,self._sin_cached\n",
    "\n",
    "        d_model = x.size(-1)\n",
    "        split_dim = d_model // 2\n",
    "        x1 = x[...,:split_dim]\n",
    "        x2 = x[...,split_dim:]\n",
    "\n",
    "        cos = cos[:seq_len,:].unsqueeze(0)\n",
    "        sin = sin[:seq_len,:].unsqueeze(0)\n",
    "\n",
    "        x1_rot = x1 * cos - x2 * sin\n",
    "        x2_rot = x1 * sin - x2 * cos\n",
    "\n",
    "        return torch.cat([x1_rot,x2_rot],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684145e2-0a15-477b-8082-97a47656113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        self.wq = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wk = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wv = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.wo = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.rotary_embedding = LinearlyScaledRotaryEmbedding(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,_ = x.shape\n",
    "        q = self.wq(x).view(batch_size,seq_len,self.n_heads,self.d_head)\n",
    "        k = self.wk(x).view(batch_size,seq_len,self.n_heads,self.d_head)\n",
    "        v = self.wv(x).view(batch_size,seq_len,self.n_heads,self.d_head)\n",
    "\n",
    "        q = self.roatry_embedding(q)\n",
    "        k = self.rotary_embedding(k)\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        k = k.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "        attn_score = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.d_model)\n",
    "        attn_weight = F.softmax(attn_score,dim=-1)\n",
    "        attn_output = torch.matmul(attn_weight,v)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous().view(batch_size,seq_len,self.d_model)\n",
    "        attn_output = self.wo(attn_output)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d608c179-1cf6-41ba-b5e7-3f253542605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f221197-080f-4000-bc92-605d17bccf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(config)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "        self.mlp = ParallelGatedMLP(config)\n",
    "\n",
    "    def forward(self,x,padding_mask = None):\n",
    "        if isinstance(padding_mask,torch.Tensor):\n",
    "            x = x * padding_mask.unsqueeze(-1)\n",
    "        attn_out = self.mha(self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        if isinstance(padding_mask,torch.Tensor):\n",
    "            x = x * padding_mask.unsqueeze(-1)\n",
    "        mlp_out = self.mlp(self.norm2(x))\n",
    "        return x + mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ee8d4ca-d511-4266-bab2-4397a4a67315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelHyenaFilter(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.short_filter_length = config.short_filter_length\n",
    "        self.short_filter_weight = nn.Parameter(torch.randn(config.d_model,1,config.short_filter_length))\n",
    "        self.short_filter_bias = (nn.Parameter(torch.randn(config.d_model)) if config.short_filter_bias else None)\n",
    "        self.D = nn.Parameter(torch.zeros(config.d_model))\n",
    "        self.state_size = config.state_size\n",
    "\n",
    "        poles = torch.randn(self.d_model,config.state_size,1,2)\n",
    "        poles[...,0] = 1e-2 * torch.randn(self.d_model,config.state_size,1)\n",
    "        poles[...,1] = 1e-3 * torch.randn(self.d_model,config.state_size,1)\n",
    "        self.poles = nn.Parameter(poles)\n",
    "        self.residues = nn.Parameter(torch.randn(self.d_model,config.state_size,1,2))\n",
    "        self.h = None\n",
    "        self.t = None\n",
    "\n",
    "    def update_time(self,L,device):\n",
    "        self.t = torch.arange(L,device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def compute_filter(self,L,device):\n",
    "        self.update_tim(L,device)\n",
    "        residues = torch.view_as_complex(self.residues.float())\n",
    "        log_poles = torch.view_as_complex(self.poles.float()).low()\n",
    "        h = (residues * (log_poles * self.t).exp()).real.sum(dim=1)\n",
    "        h = h.mean(dim=0,keepdim=True)\n",
    "        h = h.unsqueeze(0).transpose(1,2)\n",
    "        return h\n",
    "\n",
    "    def forward(self,x,padding_mask=None):\n",
    "        seq_len = x.shape[1]\n",
    "        device = x.device\n",
    "        x_t = x.transpose(1,2)\n",
    "        z_pre = F.conv1d(x_t,self.short_filter_weight,bias=self.short_filter_bias,padding=(self.short_filter_length -1)//2,groups=3*self.d_model)\n",
    "        z_pre = z_pre.transpose(1,2)\n",
    "        z_pre = z_pre[...,:self.d_model]\n",
    "        if self.h is None or self.h.shape[1]<seq_len:\n",
    "            self.h = self.compute_filter(seq_len,device)\n",
    "        y = z_pre * self.h + x[...,:self.d_model] * self.D\n",
    "        if padding_mask is not None:\n",
    "            y = y.masked_fill(padding_mask.unsqueeze(-1),0)\n",
    "        return y,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c08d42bb-7448-4ad1-8298-4717a4510442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedConvBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "        self.hyena = ParallelHyenaFilter(config)\n",
    "        self.mlp = ParallelGatedMLP(config)\n",
    "        self.proj = nn.Linear(config.d_model,3*config.d_model,bias=True)\n",
    "        self.out_filter_dense = nn.Linear(config.d_model,config.d_model,bias=True)\n",
    "\n",
    "    def forward(self,x,padding_mask=None):\n",
    "        x = self.proj(self.norm1(x))\n",
    "        if isinstance(padding_mask,torch.Tensor):\n",
    "            x = x.masked_fill(padding_mask.unsqueeze(-1),0)\n",
    "        x,_ = self.hyena(x,padding_mask=padding_mask)\n",
    "        x_in = self.out_filter_dense(x) + x\n",
    "        if isinsatnce(padding_mask,torch.Tensor):\n",
    "            x_in = x_in.masked_fill(padding_mask.unsqueeze(-1),0)\n",
    "        y = self.mlp(self.norm2(x_in)) + x_in\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "047fdb91-e08f-4c00-8d68-b71e9fc94cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionBlock(config)\n",
    "        self.conv = GatedConvBlock(config)\n",
    "\n",
    "    def forward(self,x,padding_mask=None):\n",
    "        x = self.attn(x,padding_mask=padding_mask)\n",
    "        x = self.conv(x,padding_mask=padding_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4db04bf4-e7d5-4f5d-8ef5-bbeeda7d084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.d_model = config.d_model\n",
    "        self.unembed = nn.Embedding(self.d_model,self.vocab_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.unembed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2977e494-0262-475f-9608-fc84a447335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StripedHyena(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embed = InputEmbedding(config)\n",
    "        self.norm = RMSNorm(config)\n",
    "        self.unembed = Projection(config)\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layers)])\n",
    "\n",
    "    def forward(self,x,padding_mask=None):\n",
    "        x = self.embed(x)\n",
    "        if padding_mask is not None:\n",
    "            x = x * padding_mask.unsqueeze(-1)\n",
    "        for block in self.blocks:\n",
    "            x = block(x,padding_mask=padding_mask)\n",
    "        x = self.norm(x)\n",
    "        x = self.unembed(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa1c8f37-c931-4d98-82c2-8b2097e269a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyenaConfig:\n",
    "    d_model = 265\n",
    "    seq_len = 128\n",
    "    n_heads = 8\n",
    "    vocab_size = 10000\n",
    "    eps = 1e-5\n",
    "    short_filter_length = 21\n",
    "    state_size = 16\n",
    "    n_layers = 4\n",
    "    tie_embed = False\n",
    "    n_heads_kv = 8\n",
    "    short_filter_bias = True\n",
    "    device = \"cuda\"\n",
    "    d_ff = 1024\n",
    "    d_head = d_model // n_heads\n",
    "    scaling_factor=1.0\n",
    "    base = 10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "65dc2830-366d-4367-8054-d60bf2c8de4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 128, 8, 33]' is invalid for input of size 1085440",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m padding_mask\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m128\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m StripedHyena(config)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inputs,padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[37], line 14\u001b[0m, in \u001b[0;36mStripedHyena.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x,padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[35], line 8\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x,padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m      9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x,padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mAttentionBlock.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding_mask,torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding_mask,torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mMHA.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     15\u001b[0m     batch_size,seq_len,_ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 16\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwq(x)\u001b[38;5;241m.\u001b[39mview(batch_size,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head)\n\u001b[1;32m     17\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(x)\u001b[38;5;241m.\u001b[39mview(batch_size,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head)\n\u001b[1;32m     18\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv(x)\u001b[38;5;241m.\u001b[39mview(batch_size,seq_len,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_head)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 128, 8, 33]' is invalid for input of size 1085440"
     ]
    }
   ],
   "source": [
    "config = HyenaConfig()\n",
    "inputs = torch.randint(0,config.vocab_size,(32,128))\n",
    "padding_mask=torch.ones(32,128,dtype=torch.bool)\n",
    "model = StripedHyena(config)\n",
    "output = model(inputs,padding_mask=padding_mask)\n",
    "print(\"output shape\",output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd2a52-c6ef-45a0-9590-4b167c35ba15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
