{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7cdb08-8e67-4587-a664-09bb87fa2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- Basic Modules --------------------\n",
    "\n",
    "class ByteEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, hash_size):\n",
    "        super().__init__()\n",
    "        self.byte_embed = nn.Embedding(256, d_model)\n",
    "        self.hash_embed = nn.Embedding(hash_size, d_model)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq):\n",
    "        byte_embedding = self.byte_embed(byte_seq)\n",
    "        hash_embedding = self.hash_embed(hash_seq)\n",
    "        return byte_embedding + hash_embedding\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, ff_dim)\n",
    "        self.layer2 = nn.Linear(ff_dim, d_model)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.dropout(self.silu(self.layer1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = FeedForwardLayer(d_model, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return self.norm2(x)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(query_dim, n_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "        self.ff = FeedForwardLayer(query_dim, ff_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query_proj = nn.Linear(query_dim, query_dim)\n",
    "        self.key_proj = nn.Linear(key_dim, query_dim)\n",
    "        self.value_proj = nn.Linear(key_dim, query_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query = self.query_proj(query).permute(1, 0, 2)\n",
    "        key = self.key_proj(key).permute(1, 0, 2)\n",
    "        value = self.value_proj(value).permute(1, 0, 2)\n",
    "        attn_out, _ = self.attention(query, key, value)\n",
    "        attn_out = attn_out.permute(1, 0, 2)\n",
    "        query = query.permute(1, 0, 2)\n",
    "        query = query + self.dropout(attn_out)\n",
    "        query = self.norm(query)\n",
    "        ff_out = self.ff(query)\n",
    "        return query + self.dropout(ff_out)\n",
    "\n",
    "class LocalEncoder(nn.Module):\n",
    "    def __init__(self, byte_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim, key_dim=byte_dim, n_heads=n_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, byte_embeddings, patch_embeddings):\n",
    "        for layer in self.layers:\n",
    "            byte_embeddings = layer(byte_embeddings)\n",
    "        patch_embedding = self.cross_attn(patch_embeddings, byte_embeddings, byte_embeddings)\n",
    "        return patch_embedding\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    def __init__(self, patch_dim, byte_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim, key_dim=patch_dim, n_heads=n_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "        self.output_proj = nn.Linear(byte_dim, 256)\n",
    "\n",
    "    def forward(self, patch_embedding, byte_embedding):\n",
    "        byte_embedding = self.cross_attn(byte_embedding, patch_embedding, patch_embedding)\n",
    "        for layer in self.layers:\n",
    "            byte_embedding = layer(byte_embedding)\n",
    "        return self.output_proj(byte_embedding)\n",
    "\n",
    "def l2_loss(pred, target):\n",
    "    return torch.sum((pred - target) ** 2)\n",
    "\n",
    "# -------------------- Memory Modules --------------------\n",
    "\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\", torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\", torch.zeros(config.d_model, config.d_model))\n",
    "        self.query = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q, self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self, x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        v_pred = torch.matmul(k, self.M)\n",
    "        loss = l2_loss(v_pred, v)\n",
    "        error = v_pred - v\n",
    "        g = 2 * torch.matmul(error.t(), k)\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1 - self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.d_model, num_heads=config.n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        output = []\n",
    "        for i in range(0, seq_len, self.window_size):\n",
    "            x_chunk = x[:, i:i+self.window_size, :]\n",
    "            attn_out, _ = self.attention(x_chunk, x_chunk, x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output, dim=1)\n",
    "\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p, config.d_model))\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# -------------------- MoE Layer --------------------\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=4, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * d_model\n",
    "        self.experts = nn.ModuleList([FeedForwardLayer(d_model, hidden_dim, dropout) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, d_model)\n",
    "        gate_logits = self.gate(x)  # (batch, seq, num_experts)\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)  # (batch, seq, num_experts)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)  # (batch, seq, num_experts, d_model)\n",
    "        gate_probs = gate_probs.unsqueeze(-1)  # (batch, seq, num_experts, 1)\n",
    "        moe_output = torch.sum(gate_probs * expert_outputs, dim=2)  # (batch, seq, d_model)\n",
    "        return moe_output\n",
    "\n",
    "# -------------------- TitanMAG with MoE Integration --------------------\n",
    "\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x_flat = x.reshape(-1, d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens, x], dim=1)\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        out_flat = out.reshape(-1, self.d_model)\n",
    "        long_term = self.long_memory(out_flat)\n",
    "        long_term = long_term.reshape(batch_size, -1, d_model)\n",
    "        output = y * long_term\n",
    "        output = output[:, -seq_len:, :]\n",
    "        return output\n",
    "\n",
    "# -------------------- RMSNorm --------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps \n",
    "        self.weights = nn.Parameter(torch.ones(config.d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        rms = torch.sqrt(mean + self.eps)\n",
    "        return (x / rms) * self.weights\n",
    "\n",
    "# -------------------- EncoderBlock with MoE --------------------\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        config = SimpleNamespace(\n",
    "            d_model = patch_dim,\n",
    "            n_heads = n_heads,\n",
    "            ff_dim = ff_dim,\n",
    "            dropout = dropout,\n",
    "            window_size = 32,\n",
    "            n_layers = 8,\n",
    "            alpha = 0.1,\n",
    "            eta = 0.01,\n",
    "            theta = 0.01,\n",
    "            N_p = 64,\n",
    "            eps = 1e-5\n",
    "        )\n",
    "        self.Titan = TitanMAG(config)\n",
    "        self.MoE = MoELayer(config.d_model, num_experts=8, hidden_dim=config.ff_dim, dropout=config.dropout)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.dropout2 = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_out = x + self.Titan(self.norm1(self.dropout(x)))\n",
    "        out = attn_out + self.MoE(self.norm2(self.dropout2(attn_out)))\n",
    "        return out\n",
    "                                  \n",
    "        \n",
    "# -------------------- LatentGlobalTransformer with MoE Integration --------------------\n",
    "\n",
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(patch_dim, n_heads, ff_dim, n_layers=2, dropout=dropout) for _ in range(n_layers)])\n",
    "    def forward(self, patches, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches)\n",
    "        return patches\n",
    "        \n",
    "\n",
    "\n",
    "# -------------------- ByteLatentTitan --------------------\n",
    "\n",
    "class ByteLatentTitan(nn.Module):\n",
    "    def __init__(self, byte_dim, patch_dim, vocab_size, n_heads, ff_dim, n_encoder, n_decoder, n_global, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim, vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim, n_heads, ff_dim, n_layers=n_encoder, dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim, n_heads, ff_dim, n_layers=n_global, dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim, byte_dim, n_heads, ff_dim, n_layers=n_decoder, dropout=dropout)\n",
    "        self.projection = nn.Linear(byte_dim, patch_dim)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq, patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq, hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings, dim=1, keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings, patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings, byte_embeddings)\n",
    "        return byte_output\n",
    "\n",
    "# -------------------- Sampling Utilities --------------------\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_value = values[-1]\n",
    "        logits[logits < min_value] = -float('Inf')\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = -float('Inf')\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, prompt, max_length, device, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    for _ in range(max_length - prompt.size(1)):\n",
    "        hash_seq = generated.clone()\n",
    "        with torch.no_grad():\n",
    "            output = model(generated, hash_seq, patch_seq=None)\n",
    "        next_logits = output[:, -1, :]  # (1, vocab_size)\n",
    "        next_token = sample_from_logits(next_logits.squeeze(0),\n",
    "                                        temperature=temperature,\n",
    "                                        top_k=top_k,\n",
    "                                        top_p=top_p)\n",
    "        next_token = next_token.unsqueeze(0)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated\n",
    "\n",
    "# -------------------- Dataset & Collate --------------------\n",
    "\n",
    "class WikiByteDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, seq_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        text_bytes = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "        if len(text_bytes) < self.seq_len:\n",
    "            text_bytes = text_bytes + b\" \" * (self.seq_len - len(text_bytes))\n",
    "        else:\n",
    "            start = torch.randint(0, len(text_bytes) - self.seq_len + 1, (1,)).item()\n",
    "            text_bytes = text_bytes[start:start+self.seq_len]\n",
    "        byte_seq = list(text_bytes)\n",
    "        hash_seq = byte_seq.copy()\n",
    "        return {\"byte_seq\": torch.tensor(byte_seq, dtype=torch.long),\n",
    "                \"hash_seq\": torch.tensor(hash_seq, dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    byte_seqs = torch.stack([item['byte_seq'] for item in batch])\n",
    "    hash_seqs = torch.stack([item['hash_seq'] for item in batch])\n",
    "    return byte_seqs, hash_seqs\n",
    "\n",
    "# -------------------- Baseline Evaluation --------------------\n",
    "\n",
    "def compute_baseline_loss(dataset, vocab_size=256, num_samples=100):\n",
    "    counts = np.zeros(vocab_size, dtype=np.float32)\n",
    "    total_tokens = 0\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    for idx in indices:\n",
    "        sample = dataset[int(idx)]\n",
    "        tokens = sample['byte_seq'].numpy()\n",
    "        for t in tokens:\n",
    "            counts[t] += 1\n",
    "        total_tokens += len(tokens)\n",
    "    probs = counts / total_tokens\n",
    "    baseline_loss = -np.sum(probs[probs > 0] * np.log(probs[probs > 0]))\n",
    "    return baseline_loss\n",
    "\n",
    "def evaluate_generation_loss(model, sample, device):\n",
    "    # Evaluate a held-out sample using teacher forcing:\n",
    "    model.eval()\n",
    "    seq = sample['byte_seq'].unsqueeze(0).to(device)  # shape (1, L)\n",
    "    L = seq.size(1)\n",
    "    prompt_length = L // 2\n",
    "    with torch.no_grad():\n",
    "        output = model(seq, seq, patch_seq=None)  # shape (1, L, vocab_size)\n",
    "    logits = output[:, prompt_length:-1, :]  # predictions for positions prompt_length+1 to L\n",
    "    target = seq[:, prompt_length+1:]\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\n",
    "    return loss.item()\n",
    "\n",
    "# -------------------- Checkpoint Functions --------------------\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_path=\"checkpoint.pt\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}.\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path=\"checkpoint.pt\"):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Checkpoint found. Resuming training from epoch {start_epoch}.\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "        return 0\n",
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "\n",
    "def train():\n",
    "    import os\n",
    "    # Hyperparameters adjusted for a larger dataset\n",
    "    byte_dim = 128         \n",
    "    patch_dim = 256       \n",
    "    vocab_size = 256      \n",
    "    n_heads = 8           \n",
    "    ff_dim = 1024         \n",
    "    n_encoder = 4         \n",
    "    n_decoder = 4         \n",
    "    n_global = 8          \n",
    "    dropout = 0.1\n",
    "    seq_len = 64        \n",
    "    batch_size = 128       \n",
    "    epochs = 100           \n",
    "    lr = 1e-5            \n",
    "    \n",
    "    # Sampling hyperparameters\n",
    "    temperature = 1.0  \n",
    "    top_k = 40         \n",
    "    top_p = 0.9        \n",
    "\n",
    "    # Use a larger dataset: 10% slice of English Wikipedia (20220301)\n",
    "    hf_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:5%]\")\n",
    "    dataset = WikiByteDataset(hf_dataset, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    baseline_loss = compute_baseline_loss(dataset, vocab_size=vocab_size, num_samples=200)\n",
    "    print(f\"Baseline (empirical token entropy) Loss: {baseline_loss:.4f}\\n\")\n",
    "    \n",
    "    model = ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim,\n",
    "                              n_encoder, n_decoder, n_global, dropout)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    checkpoint_path = \"checkpoint.pt\"\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "    \n",
    "    train_losses = []\n",
    "    gen_losses = []\n",
    "    \n",
    "    print(\"Starting autoregressive training with MoE and sampling strategies on Wikipedia...\\n\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nEpoch [{epoch+1}] Total Parameters: {total_params}\")\n",
    "        \n",
    "        for byte_seq, hash_seq in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            byte_seq = byte_seq.to(device)\n",
    "            hash_seq = hash_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(byte_seq, hash_seq, patch_seq=None)\n",
    "            logits = output[:, :-1, :]  # Predict tokens 1...end\n",
    "            target = byte_seq[:, 1:]    # Ground truth shifted by one\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}] Average Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Baseline Loss: {baseline_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate generation loss on one held-out sample\n",
    "        sample = dataset[0]\n",
    "        gen_loss = evaluate_generation_loss(model, sample, device)\n",
    "        gen_losses.append(gen_loss)\n",
    "        print(f\"Epoch [{epoch+1}] Generation Loss: {gen_loss:.4f}\")\n",
    "        \n",
    "        # Generate sample text\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prompt_tokens = sample['byte_seq'][:seq_len//2].unsqueeze(0).to(device)\n",
    "            generated = generate_text(model, prompt_tokens, max_length=seq_len, device=device,\n",
    "                                      temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "            generated_list = generated.squeeze(0).cpu().tolist()\n",
    "            try:\n",
    "                generated_text = bytes(generated_list).decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception as e:\n",
    "                generated_text = str(generated_list)\n",
    "            prompt_text = bytes(prompt_tokens.squeeze(0).cpu().tolist()).decode(\"utf-8\", errors=\"replace\")\n",
    "            print(\"\\n--- Sample Generation ---\")\n",
    "            print(\"Prompt:   \", prompt_text)\n",
    "            print(\"Generated:\", generated_text)\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "    \n",
    "    print(\"\\nTraining complete.\")\n",
    "    epochs_range = range(start_epoch+1, epochs+1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs_range, gen_losses, label=\"Generation Loss\", linestyle=\"--\")\n",
    "    plt.axhline(baseline_loss, color='r', linestyle=':', label=\"Baseline Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Generation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1bab37-510d-4dc1-9b22-20e90f6f4111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
