{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27849391-3f25-4b7b-9fe3-47240219116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (empirical token entropy) Loss: 3.0743\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 460\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     train()\n",
      "Cell \u001b[0;32mIn[1], line 414\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m model \u001b[38;5;241m=\u001b[39m ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim,\n\u001b[1;32m    412\u001b[0m                         n_encoder, n_decoder, n_global, dropout)\n\u001b[1;32m    413\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    415\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting autoregressive training with sampling strategies on Tiny Shakespeare...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1327\u001b[0m         device,\n\u001b[1;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1329\u001b[0m         non_blocking,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ByteEmbedding(nn.Module):\n",
    "    def __init__(self,d_model,hash_size):\n",
    "        super().__init__()\n",
    "        self.byte_embed = nn.Embedding(256,d_model)\n",
    "        self.hash_embed = nn.Embedding(hash_size,d_model)\n",
    "\n",
    "    def forward(self,byte_seq,hash_seq):\n",
    "        byte_embedding = self.byte_embed(byte_seq)\n",
    "        hash_embedding = self.hash_embed(hash_seq)\n",
    "        return byte_embedding + hash_embedding\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self,d_model,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model,ff_dim)\n",
    "        self.layer2 = nn.Linear(ff_dim,d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer2(self.dropout(self.gelu(self.layer1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model,n_heads,dropout=dropout)\n",
    "        self.ff = FeedForwardLayer(d_model,ff_dim,dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,attn_mask = None):\n",
    "        attn_out,_ = self.attention(x,x,x,attn_mask=attn_mask) \n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        return self.norm2(x)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self,query_dim,key_dim,n_heads,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(query_dim,n_heads,dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "        self.ff = FeedForwardLayer(query_dim,ff_dim,dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query_proj = nn.Linear(query_dim,query_dim)\n",
    "        self.key_proj = nn.Linear(key_dim,query_dim)\n",
    "        self.value_proj = nn.Linear(key_dim,query_dim)\n",
    "\n",
    "    def forward(self,query,key,value):\n",
    "        query = self.query_proj(query).permute(1,0,2)\n",
    "        key = self.key_proj(key).permute(1,0,2)\n",
    "        value = self.value_proj(value).permute(1,0,2)\n",
    "\n",
    "        attn_out , _ = self.attention(query,key,value)\n",
    "        attn_out = attn_out.permute(1,0,2)\n",
    "        query = query.permute(1,0,2)\n",
    "        query = query + self.dropout(attn_out)\n",
    "        query = self.norm(query)\n",
    "        ff_out = self.ff(query)\n",
    "        return query + self.dropout(ff_out)\n",
    "\n",
    "class LocalEncoder(nn.Module):\n",
    "    def __init__(self,byte_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim,key_dim=byte_dim,n_heads=n_heads,ff_dim=ff_dim,dropout=dropout)\n",
    "\n",
    "    def forward(self,byte_embeddings,patch_embeddings):\n",
    "        for layer in self.layers:\n",
    "            byte_embeddings = layer(byte_embeddings)\n",
    "        patch_embedding = self.cross_attn(patch_embeddings,byte_embeddings,byte_embeddings)\n",
    "        return patch_embedding\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    def __init__(self,patch_dim,byte_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim,key_dim=patch_dim,n_heads=n_heads,ff_dim=ff_dim,dropout=dropout)\n",
    "        self.output_proj = nn.Linear(byte_dim,256)\n",
    "\n",
    "    def forward(self,patch_embedding,byte_embedding):\n",
    "        byte_embedding = self.cross_attn(byte_embedding,patch_embedding,patch_embedding)\n",
    "        for layer in self.layers:\n",
    "            byte_embedding = layer(byte_embedding)\n",
    "        return self.output_proj(byte_embedding)\n",
    "\n",
    "def l2_loss(pred,target):\n",
    "    return torch.sum((pred - target) ** 2)\n",
    "\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\",torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\",torch.zeros(config.d_model,config.d_model))\n",
    "\n",
    "        self.query = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.key = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.value = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self,x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q,self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self,x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        v_pred = torch.matmul(k,self.M)\n",
    "        loss = l2_loss(v_pred,v)\n",
    "        error = v_pred - v\n",
    "\n",
    "        g = 2 * torch.matmul(error.t(),k)\n",
    "\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1-self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim = config.d_model, num_heads = config.n_heads, batch_first= True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,_ = x.size()\n",
    "        output = []\n",
    "\n",
    "        for i in range(0,seq_len,self.window_size):\n",
    "            x_chunk = x[:,i:i+self.window_size,:]\n",
    "            attn_out,_ = self.attention(x_chunk,x_chunk,x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output,dim=1)\n",
    "\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p,config.d_model))\n",
    "\n",
    "    def forward(self,batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size,-1,-1)\n",
    "\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,d_model = x.size()\n",
    "\n",
    "        x_flat = x.reshape(-1,d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        \n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens,x],dim=1)\n",
    "\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        out_flat = out.reshape(-1,self.d_model)\n",
    "        long_term = self.long_memory(out_flat)\n",
    "        long_term = long_term.reshape(batch_size,-1,d_model)\n",
    "\n",
    "        output = y * long_term\n",
    "        output = output[:,-seq_len:,:]\n",
    "        return output\n",
    "\n",
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self,patch_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TitanMAG(patch_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "    def forward(self,patches,attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches,attn_mask=attn_mask)\n",
    "        return patches\n",
    "\n",
    "class ByteLatentTransformer(nn.Module):\n",
    "    def __init__(self,byte_dim,patch_dim,vocab_size,n_heads,ff_dim,n_encoder,n_decoder,n_global,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim,vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim,n_heads,ff_dim,n_layers=n_encoder,dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim,n_heads,ff_dim,n_layers=n_global,dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim,byte_dim,n_heads,ff_dim,n_decoder,dropout=dropout) \n",
    "        self.projection = nn.Linear(byte_dim,patch_dim)\n",
    "\n",
    "    def forward(self,byte_seq,hash_seq,patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq,hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings,dim=1,keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings,patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings,byte_embeddings)\n",
    "        return byte_output\n",
    "\n",
    "from types import SimpleNamespace\n",
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        config = SimpleNamespace(\n",
    "            d_model = patch_dim,\n",
    "            n_heads = n_heads,\n",
    "            ff_dim = ff_dim,\n",
    "            dropout = dropout,\n",
    "            window_size = 16,  \n",
    "            n_layers = 2,      \n",
    "            alpha = 0.1,\n",
    "            eta = 0.01,\n",
    "            theta = 0.01,\n",
    "            N_p = 10\n",
    "        )\n",
    "        self.layers = nn.ModuleList([TitanMAG(config) for _ in range(n_layers)])\n",
    "    def forward(self, patches, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches)\n",
    "        return patches\n",
    "\n",
    "class ByteLatentTitan(nn.Module):\n",
    "    def __init__(self, byte_dim, patch_dim, vocab_size, n_heads, ff_dim, n_encoder, n_decoder, n_global, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim, vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim, n_heads, ff_dim, n_layers=n_encoder, dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim, n_heads, ff_dim, n_layers=n_global, dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim, byte_dim, n_heads, ff_dim, n_layers=n_decoder, dropout=dropout)\n",
    "        self.projection = nn.Linear(byte_dim, patch_dim)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq, patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq, hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings, dim=1, keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings, patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings, byte_embeddings)\n",
    "        return byte_output\n",
    "        \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ===== Sampling Utilities =====\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    # Apply temperature scaling\n",
    "    logits = logits / temperature\n",
    "\n",
    "    # Top-k filtering\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_value = values[-1]\n",
    "        logits[logits < min_value] = -float('Inf')\n",
    "\n",
    "    # Nucleus (top-p) filtering\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = -float('Inf')\n",
    "    \n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, prompt, max_length, device, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    for _ in range(max_length - prompt.size(1)):\n",
    "        hash_seq = generated.clone()\n",
    "        with torch.no_grad():\n",
    "            output = model(generated, hash_seq, patch_seq=None)\n",
    "        # Get logits for the last time step (assumes batch size 1)\n",
    "        next_logits = output[:, -1, :]  # shape: (1, vocab_size)\n",
    "        next_token = sample_from_logits(next_logits.squeeze(0),\n",
    "                                        temperature=temperature,\n",
    "                                        top_k=top_k,\n",
    "                                        top_p=top_p)\n",
    "        next_token = next_token.unsqueeze(0)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated\n",
    "\n",
    "\n",
    "class WikiByteDataset(Dataset):\n",
    "\n",
    "    def __init__(self, hf_dataset, seq_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        text_bytes = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "        if len(text_bytes) < self.seq_len:\n",
    "            text_bytes = text_bytes + b\" \" * (self.seq_len - len(text_bytes))\n",
    "        else:\n",
    "            start = torch.randint(0, len(text_bytes) - self.seq_len + 1, (1,)).item()\n",
    "            text_bytes = text_bytes[start:start+self.seq_len]\n",
    "        byte_seq = list(text_bytes)\n",
    "        hash_seq = byte_seq.copy()\n",
    "        return {\n",
    "            \"byte_seq\": torch.tensor(byte_seq, dtype=torch.long),\n",
    "            \"hash_seq\": torch.tensor(hash_seq, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    byte_seqs = torch.stack([item['byte_seq'] for item in batch])\n",
    "    hash_seqs = torch.stack([item['hash_seq'] for item in batch])\n",
    "    return byte_seqs, hash_seqs\n",
    "\n",
    "def compute_baseline_loss(dataset, vocab_size=256, num_samples=100):\n",
    "    \"\"\"\n",
    "    Computes the empirical cross-entropy (i.e. the entropy) of the token distribution\n",
    "    over a subset of the dataset. This serves as a baseline loss for a next-token\n",
    "    prediction task (if one predicted tokens according to the empirical distribution).\n",
    "    \"\"\"\n",
    "    counts = np.zeros(vocab_size, dtype=np.float32)\n",
    "    total_tokens = 0\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    for idx in indices:\n",
    "        sample = dataset[int(idx)]   # Cast idx to int\n",
    "        tokens = sample['byte_seq'].numpy()\n",
    "        for t in tokens:\n",
    "            counts[t] += 1\n",
    "        total_tokens += len(tokens)\n",
    "    probs = counts / total_tokens\n",
    "    # Compute entropy: H = - sum(p * log(p)) for nonzero p\n",
    "    baseline_loss = -np.sum(probs[probs > 0] * np.log(probs[probs > 0]))\n",
    "    return baseline_loss\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    byte_dim = 256         # Reduced from 128\n",
    "    patch_dim = 512       # Reduced from 256\n",
    "    vocab_size = 256      \n",
    "    n_heads = 8           # Reduced from 8\n",
    "    ff_dim = 2048          # Reduced from 2048\n",
    "    n_encoder = 8         # Reduced from 6\n",
    "    n_decoder = 8         # Reduced from 6\n",
    "    n_global = 12          # Reduced from 12\n",
    "    dropout = 0.1\n",
    "    seq_len = 128        \n",
    "    batch_size = 64       # Reduced batch size for a smaller dataset\n",
    "    epochs = 100           # More epochs may help since the dataset is small\n",
    "    lr = 1e-3             # A slightly higher LR for quicker convergence on a small dataset\n",
    "    \n",
    "    # Sampling hyperparameters\n",
    "    temperature = 1.0  \n",
    "    top_k = 40         \n",
    "    top_p = 0.9        \n",
    "\n",
    "    # Load Tiny Shakespeare dataset\n",
    "    hf_dataset = load_dataset(\"tiny_shakespeare\", split=\"train\")\n",
    "    dataset = WikiByteDataset(hf_dataset, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Compute the baseline loss from the token distribution (lower is better)\n",
    "    baseline_loss = compute_baseline_loss(dataset, vocab_size=vocab_size, num_samples=200)\n",
    "    print(f\"Baseline (empirical token entropy) Loss: {baseline_loss:.4f}\\n\")\n",
    "    \n",
    "    # Instantiate the model (make sure ByteLatentTitan is defined)\n",
    "    model = ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim,\n",
    "                            n_encoder, n_decoder, n_global, dropout)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"Starting autoregressive training with sampling strategies on Tiny Shakespeare...\\n\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nEpoch [{epoch+1}] Total Parameters: {total_params}\")\n",
    "        \n",
    "        for byte_seq, hash_seq in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            byte_seq = byte_seq.to(device)\n",
    "            hash_seq = hash_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(byte_seq, hash_seq, patch_seq=None)\n",
    "            logits = output[:, :-1, :]  # Predict tokens 1...end\n",
    "            target = byte_seq[:, 1:]    # Ground truth shifted by one\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Baseline Loss: {baseline_loss:.4f}\")\n",
    "        \n",
    "        # ----- Autoregressive Generation Sample -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample = dataset[0]\n",
    "            prompt_tokens = sample['byte_seq'][:seq_len//2].unsqueeze(0).to(device)\n",
    "            generated = generate_text(model, prompt_tokens, max_length=300, device=device,\n",
    "                                      temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "            generated_list = generated.squeeze(0).cpu().tolist()\n",
    "            try:\n",
    "                generated_text = bytes(generated_list).decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception as e:\n",
    "                generated_text = str(generated_list)\n",
    "            prompt_text = bytes(prompt_tokens.squeeze(0).cpu().tolist()).decode(\"utf-8\", errors=\"replace\")\n",
    "            print(\"\\n--- Sample Generation ---\")\n",
    "            print(\"Prompt:   \", prompt_text)\n",
    "            print(\"Generated:\", generated_text)\n",
    "    print(\"\\nTraining complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bde5b27-fbe0-405b-a074-aa8714f815b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (empirical token entropy) Loss: 3.3112\n",
      "\n",
      "Starting autoregressive training with MoE and sampling strategies on Wikipedia...\n",
      "\n",
      "\n",
      "Epoch [1] Total Parameters: 14046400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████| 505/505 [06:21<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Average Training Loss: 4.6763\n",
      "Baseline Loss: 3.3112\n",
      "Epoch [1] Generation Loss: 4.2287\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:    on organisational and economic aspects of their ideal society.\n",
      "\n",
      "Mutualism is an 18th-century economic theory that was developed \n",
      "Generated: on organisational and economic aspects of their ideal society.\n",
      "\n",
      "Mutualism is an 18th-century economic theory that was developed tr �  �.� at-��e�ee o� an�i�rr.&� V d �n �ne te � Iio pa�̑uer or��r�ter\u001a �er iu�n��n \u001f�&aeuo.d� ir� draorucat�h1D0e ̷rd r-�h- \n",
      "\n",
      "Epoch [2] Total Parameters: 14046400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|█████████████████████████████| 505/505 [06:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] Average Training Loss: 4.0377\n",
      "Baseline Loss: 3.3112\n",
      "Epoch [2] Generation Loss: 3.7122\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:     of all forms of domination and hierarchy.\n",
      "\n",
      "Tactics \n",
      "Anarchists' tactics take various forms but in general serve two major goals\n",
      "Generated:  of all forms of domination and hierarchy.\n",
      "\n",
      "Tactics \n",
      "Anarchists' tactics take various forms but in general serve two major goalse �a�o�n 2 amh, aoat. ur.iui\n",
      " ith.nn�haa.\u001for\u001f�ma rdh,u iunes pundei Te i.sns th� onn., te n�\n",
      "Sal htht�red h   e ou\n",
      " ore iwnn\n",
      " ar\n",
      "\n",
      "Epoch [3] Total Parameters: 14046400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:  46%|█████████████▍               | 234/505 [02:56<03:24,  1.32it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- Basic Modules --------------------\n",
    "\n",
    "class ByteEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, hash_size):\n",
    "        super().__init__()\n",
    "        self.byte_embed = nn.Embedding(256, d_model)\n",
    "        self.hash_embed = nn.Embedding(hash_size, d_model)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq):\n",
    "        byte_embedding = self.byte_embed(byte_seq)\n",
    "        hash_embedding = self.hash_embed(hash_seq)\n",
    "        return byte_embedding + hash_embedding\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, ff_dim)\n",
    "        self.layer2 = nn.Linear(ff_dim, d_model)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer2(self.dropout(self.silu(self.layer1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ff = FeedForwardLayer(d_model, ff_dim, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_out, _ = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return self.norm2(x)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, query_dim, key_dim, n_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(query_dim, n_heads, dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "        self.ff = FeedForwardLayer(query_dim, ff_dim, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query_proj = nn.Linear(query_dim, query_dim)\n",
    "        self.key_proj = nn.Linear(key_dim, query_dim)\n",
    "        self.value_proj = nn.Linear(key_dim, query_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query = self.query_proj(query).permute(1, 0, 2)\n",
    "        key = self.key_proj(key).permute(1, 0, 2)\n",
    "        value = self.value_proj(value).permute(1, 0, 2)\n",
    "        attn_out, _ = self.attention(query, key, value)\n",
    "        attn_out = attn_out.permute(1, 0, 2)\n",
    "        query = query.permute(1, 0, 2)\n",
    "        query = query + self.dropout(attn_out)\n",
    "        query = self.norm(query)\n",
    "        ff_out = self.ff(query)\n",
    "        return query + self.dropout(ff_out)\n",
    "\n",
    "class LocalEncoder(nn.Module):\n",
    "    def __init__(self, byte_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim, key_dim=byte_dim, n_heads=n_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "\n",
    "    def forward(self, byte_embeddings, patch_embeddings):\n",
    "        for layer in self.layers:\n",
    "            byte_embeddings = layer(byte_embeddings)\n",
    "        patch_embedding = self.cross_attn(patch_embeddings, byte_embeddings, byte_embeddings)\n",
    "        return patch_embedding\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    def __init__(self, patch_dim, byte_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim, n_heads, ff_dim, dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim, key_dim=patch_dim, n_heads=n_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "        self.output_proj = nn.Linear(byte_dim, 256)\n",
    "\n",
    "    def forward(self, patch_embedding, byte_embedding):\n",
    "        byte_embedding = self.cross_attn(byte_embedding, patch_embedding, patch_embedding)\n",
    "        for layer in self.layers:\n",
    "            byte_embedding = layer(byte_embedding)\n",
    "        return self.output_proj(byte_embedding)\n",
    "\n",
    "def l2_loss(pred, target):\n",
    "    return torch.sum((pred - target) ** 2)\n",
    "\n",
    "# -------------------- Memory Modules --------------------\n",
    "\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\", torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\", torch.zeros(config.d_model, config.d_model))\n",
    "        self.query = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q, self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self, x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        v_pred = torch.matmul(k, self.M)\n",
    "        loss = l2_loss(v_pred, v)\n",
    "        error = v_pred - v\n",
    "        g = 2 * torch.matmul(error.t(), k)\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1 - self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.d_model, num_heads=config.n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        output = []\n",
    "        for i in range(0, seq_len, self.window_size):\n",
    "            x_chunk = x[:, i:i+self.window_size, :]\n",
    "            attn_out, _ = self.attention(x_chunk, x_chunk, x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output, dim=1)\n",
    "\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p, config.d_model))\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# -------------------- MoE Layer --------------------\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=4, hidden_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * d_model\n",
    "        self.experts = nn.ModuleList([FeedForwardLayer(d_model, hidden_dim, dropout) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, d_model)\n",
    "        gate_logits = self.gate(x)  # (batch, seq, num_experts)\n",
    "        gate_probs = F.softmax(gate_logits, dim=-1)  # (batch, seq, num_experts)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)  # (batch, seq, num_experts, d_model)\n",
    "        gate_probs = gate_probs.unsqueeze(-1)  # (batch, seq, num_experts, 1)\n",
    "        moe_output = torch.sum(gate_probs * expert_outputs, dim=2)  # (batch, seq, d_model)\n",
    "        return moe_output\n",
    "\n",
    "# -------------------- TitanMAG with MoE Integration --------------------\n",
    "\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x_flat = x.reshape(-1, d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens, x], dim=1)\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        out_flat = out.reshape(-1, self.d_model)\n",
    "        long_term = self.long_memory(out_flat)\n",
    "        long_term = long_term.reshape(batch_size, -1, d_model)\n",
    "        output = y * long_term\n",
    "        output = output[:, -seq_len:, :]\n",
    "        return output\n",
    "\n",
    "# -------------------- RMSNorm --------------------\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps \n",
    "        self.weights = nn.Parameter(torch.ones(config.d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        rms = torch.sqrt(mean + self.eps)\n",
    "        return (x / rms) * self.weights\n",
    "\n",
    "# -------------------- EncoderBlock with MoE --------------------\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        config = SimpleNamespace(\n",
    "            d_model = patch_dim,\n",
    "            n_heads = n_heads,\n",
    "            ff_dim = ff_dim,\n",
    "            dropout = dropout,\n",
    "            window_size = 32,\n",
    "            n_layers = 8,\n",
    "            alpha = 0.1,\n",
    "            eta = 0.01,\n",
    "            theta = 0.01,\n",
    "            N_p = 64,\n",
    "            eps = 1e-5\n",
    "        )\n",
    "        self.Titan = TitanMAG(config)\n",
    "        self.MoE = MoELayer(config.d_model, num_experts=8, hidden_dim=config.ff_dim, dropout=config.dropout)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.dropout2 = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_out = x + self.Titan(self.norm1(self.dropout(x)))\n",
    "        out = attn_out + self.MoE(self.norm2(self.dropout2(attn_out)))\n",
    "        return out\n",
    "                                  \n",
    "        \n",
    "# -------------------- LatentGlobalTransformer with MoE Integration --------------------\n",
    "\n",
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(patch_dim, n_heads, ff_dim, n_layers=2, dropout=dropout) for _ in range(n_layers)])\n",
    "    def forward(self, patches, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches)\n",
    "        return patches\n",
    "        \n",
    "\n",
    "\n",
    "# -------------------- ByteLatentTitan --------------------\n",
    "\n",
    "class ByteLatentTitan(nn.Module):\n",
    "    def __init__(self, byte_dim, patch_dim, vocab_size, n_heads, ff_dim, n_encoder, n_decoder, n_global, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim, vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim, n_heads, ff_dim, n_layers=n_encoder, dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim, n_heads, ff_dim, n_layers=n_global, dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim, byte_dim, n_heads, ff_dim, n_layers=n_decoder, dropout=dropout)\n",
    "        self.projection = nn.Linear(byte_dim, patch_dim)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq, patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq, hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings, dim=1, keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings, patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings, byte_embeddings)\n",
    "        return byte_output\n",
    "\n",
    "# -------------------- Sampling Utilities --------------------\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_value = values[-1]\n",
    "        logits[logits < min_value] = -float('Inf')\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = -float('Inf')\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, prompt, max_length, device, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    for _ in range(max_length - prompt.size(1)):\n",
    "        hash_seq = generated.clone()\n",
    "        with torch.no_grad():\n",
    "            output = model(generated, hash_seq, patch_seq=None)\n",
    "        next_logits = output[:, -1, :]  # (1, vocab_size)\n",
    "        next_token = sample_from_logits(next_logits.squeeze(0),\n",
    "                                        temperature=temperature,\n",
    "                                        top_k=top_k,\n",
    "                                        top_p=top_p)\n",
    "        next_token = next_token.unsqueeze(0)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated\n",
    "\n",
    "# -------------------- Dataset & Collate --------------------\n",
    "\n",
    "class WikiByteDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, seq_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        text_bytes = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "        if len(text_bytes) < self.seq_len:\n",
    "            text_bytes = text_bytes + b\" \" * (self.seq_len - len(text_bytes))\n",
    "        else:\n",
    "            start = torch.randint(0, len(text_bytes) - self.seq_len + 1, (1,)).item()\n",
    "            text_bytes = text_bytes[start:start+self.seq_len]\n",
    "        byte_seq = list(text_bytes)\n",
    "        hash_seq = byte_seq.copy()\n",
    "        return {\"byte_seq\": torch.tensor(byte_seq, dtype=torch.long),\n",
    "                \"hash_seq\": torch.tensor(hash_seq, dtype=torch.long)}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    byte_seqs = torch.stack([item['byte_seq'] for item in batch])\n",
    "    hash_seqs = torch.stack([item['hash_seq'] for item in batch])\n",
    "    return byte_seqs, hash_seqs\n",
    "\n",
    "# -------------------- Baseline Evaluation --------------------\n",
    "\n",
    "def compute_baseline_loss(dataset, vocab_size=256, num_samples=100):\n",
    "    counts = np.zeros(vocab_size, dtype=np.float32)\n",
    "    total_tokens = 0\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    for idx in indices:\n",
    "        sample = dataset[int(idx)]\n",
    "        tokens = sample['byte_seq'].numpy()\n",
    "        for t in tokens:\n",
    "            counts[t] += 1\n",
    "        total_tokens += len(tokens)\n",
    "    probs = counts / total_tokens\n",
    "    baseline_loss = -np.sum(probs[probs > 0] * np.log(probs[probs > 0]))\n",
    "    return baseline_loss\n",
    "\n",
    "def evaluate_generation_loss(model, sample, device):\n",
    "    # Evaluate a held-out sample using teacher forcing:\n",
    "    model.eval()\n",
    "    seq = sample['byte_seq'].unsqueeze(0).to(device)  # shape (1, L)\n",
    "    L = seq.size(1)\n",
    "    prompt_length = L // 2\n",
    "    with torch.no_grad():\n",
    "        output = model(seq, seq, patch_seq=None)  # shape (1, L, vocab_size)\n",
    "    logits = output[:, prompt_length:-1, :]  # predictions for positions prompt_length+1 to L\n",
    "    target = seq[:, prompt_length+1:]\n",
    "    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), target.reshape(-1))\n",
    "    return loss.item()\n",
    "\n",
    "# -------------------- Checkpoint Functions --------------------\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_path=\"checkpoint.pt\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}.\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path=\"checkpoint.pt\"):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"Checkpoint found. Resuming training from epoch {start_epoch}.\")\n",
    "        return start_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "        return 0\n",
    "\n",
    "# -------------------- Training Loop --------------------\n",
    "\n",
    "def train():\n",
    "    import os\n",
    "    # Hyperparameters adjusted for a larger dataset\n",
    "    byte_dim = 64         \n",
    "    patch_dim = 128       \n",
    "    vocab_size = 256      \n",
    "    n_heads = 8           \n",
    "    ff_dim = 512         \n",
    "    n_encoder = 4         \n",
    "    n_decoder = 4         \n",
    "    n_global = 8          \n",
    "    dropout = 0.1\n",
    "    seq_len = 256        \n",
    "    batch_size = 128       \n",
    "    epochs = 10           \n",
    "    lr = 1e-5            \n",
    "    \n",
    "    # Sampling hyperparameters\n",
    "    temperature = 1.0  \n",
    "    top_k = 40         \n",
    "    top_p = 0.9        \n",
    "\n",
    "    # Use a larger dataset: 10% slice of English Wikipedia (20220301)\n",
    "    hf_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\")\n",
    "    dataset = WikiByteDataset(hf_dataset, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    baseline_loss = compute_baseline_loss(dataset, vocab_size=vocab_size, num_samples=200)\n",
    "    print(f\"Baseline (empirical token entropy) Loss: {baseline_loss:.4f}\\n\")\n",
    "    \n",
    "    model = ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim,\n",
    "                              n_encoder, n_decoder, n_global, dropout)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Load checkpoint if available\n",
    "    checkpoint_path = \"checkpoint.pt\"\n",
    "    start_epoch = 0 #load_checkpoint(model, optimizer, checkpoint_path)\n",
    "    \n",
    "    train_losses = []\n",
    "    gen_losses = []\n",
    "    \n",
    "    print(\"Starting autoregressive training with MoE and sampling strategies on Wikipedia...\\n\")\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nEpoch [{epoch+1}] Total Parameters: {total_params}\")\n",
    "        \n",
    "        for byte_seq, hash_seq in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            byte_seq = byte_seq.to(device)\n",
    "            hash_seq = hash_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(byte_seq, hash_seq, patch_seq=None)\n",
    "            logits = output[:, :-1, :]  # Predict tokens 1...end\n",
    "            target = byte_seq[:, 1:]    # Ground truth shifted by one\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}] Average Training Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Baseline Loss: {baseline_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate generation loss on one held-out sample\n",
    "        sample = dataset[0]\n",
    "        gen_loss = evaluate_generation_loss(model, sample, device)\n",
    "        gen_losses.append(gen_loss)\n",
    "        print(f\"Epoch [{epoch+1}] Generation Loss: {gen_loss:.4f}\")\n",
    "        \n",
    "        # Generate sample text\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prompt_tokens = sample['byte_seq'][:seq_len//2].unsqueeze(0).to(device)\n",
    "            generated = generate_text(model, prompt_tokens, max_length=seq_len, device=device,\n",
    "                                      temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "            generated_list = generated.squeeze(0).cpu().tolist()\n",
    "            try:\n",
    "                generated_text = bytes(generated_list).decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception as e:\n",
    "                generated_text = str(generated_list)\n",
    "            prompt_text = bytes(prompt_tokens.squeeze(0).cpu().tolist()).decode(\"utf-8\", errors=\"replace\")\n",
    "            print(\"\\n--- Sample Generation ---\")\n",
    "            print(\"Prompt:   \", prompt_text)\n",
    "            print(\"Generated:\", generated_text)\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        #save_checkpoint(model, optimizer, epoch, checkpoint_path)\n",
    "    \n",
    "    print(\"\\nTraining complete.\")\n",
    "    epochs_range = range(start_epoch+1, epochs+1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs_range, gen_losses, label=\"Generation Loss\", linestyle=\"--\")\n",
    "    plt.axhline(baseline_loss, color='r', linestyle=':', label=\"Baseline Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Generation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a9a27-a3dc-42d4-9f5a-b72056ebea47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
