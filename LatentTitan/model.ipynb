{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9803aa-9f51-4bee-a7e3-cbe9d4e2c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d469166-5f32-43cd-90e4-19b7274f2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteEmbedding(nn.Module):\n",
    "    def __init__(self,d_model,hash_size):\n",
    "        super().__init__()\n",
    "        self.byte_embed = nn.Embedding(256,d_model)\n",
    "        self.hash_embed = nn.Embedding(hash_size,d_model)\n",
    "\n",
    "    def forward(self,byte_seq,hash_seq):\n",
    "        byte_embedding = self.byte_embed(byte_seq)\n",
    "        hash_embedding = self.hash_embed(hash_seq)\n",
    "        return byte_embedding + hash_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97bf2a0-da34-4ade-a7e8-1ab90d9077dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self,d_model,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model,ff_dim)\n",
    "        self.layer2 = nn.Linear(ff_dim,d_model)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer2(self.dropout(self.gelu(self.layer1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a0b420-705c-4ac9-8f1b-8279c6a941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model,n_heads,dropout=dropout)\n",
    "        self.ff = FeedForwardLayer(d_model,ff_dim,dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,attn_mask = None):\n",
    "        attn_out,_ = self.attention(x,x,x,attn_mask=attn_mask) \n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        ff_out = self.ff(x)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        return self.norm2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124fc8d3-19e5-44b0-b44c-813dce9ecf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self,query_dim,key_dim,n_heads,ff_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(query_dim,n_heads,dropout=dropout)\n",
    "        self.norm = nn.LayerNorm(query_dim)\n",
    "        self.ff = FeedForwardLayer(query_dim,ff_dim,dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query_proj = nn.Linear(query_dim,query_dim)\n",
    "        self.key_proj = nn.Linear(key_dim,query_dim)\n",
    "        self.value_proj = nn.Linear(key_dim,query_dim)\n",
    "\n",
    "    def forward(self,query,key,value):\n",
    "        query = self.query_proj(query).permute(1,0,2)\n",
    "        key = self.key_proj(key).permute(1,0,2)\n",
    "        value = self.value_proj(value).permute(1,0,2)\n",
    "\n",
    "        attn_out , _ = self.attention(query,key,value)\n",
    "        attn_out = attn_out.permute(1,0,2)\n",
    "        query = query.permute(1,0,2)\n",
    "        query = query + self.dropout(attn_out)\n",
    "        query = self.norm(query)\n",
    "        ff_out = self.ff(query)\n",
    "        return query + self.dropout(ff_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff3a64e-8cde-4f85-921c-216752425995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEncoder(nn.Module):\n",
    "    def __init__(self,byte_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim,key_dim=byte_dim,n_heads=n_heads,ff_dim=ff_dim,dropout=dropout)\n",
    "\n",
    "    def forward(self,byte_embeddings,patch_embeddings):\n",
    "        for layer in self.layers:\n",
    "            byte_embeddings = layer(byte_embeddings)\n",
    "        patch_embedding = self.cross_attn(patch_embeddings,byte_embeddings,byte_embeddings)\n",
    "        return patch_embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79e2f7e-9598-47a5-87ad-97fb31cbfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDecoder(nn.Module):\n",
    "    def __init__(self,patch_dim,byte_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerBlock(byte_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "        self.cross_attn = CrossAttentionBlock(query_dim=byte_dim,key_dim=patch_dim,n_heads=n_heads,ff_dim=ff_dim,dropout=dropout)\n",
    "        self.output_proj = nn.Linear(byte_dim,256)\n",
    "\n",
    "    def forward(self,patch_embedding,byte_embedding):\n",
    "        byte_embedding = self.cross_attn(byte_embedding,patch_embedding,patch_embedding)\n",
    "        for layer in self.layers:\n",
    "            byte_embedding = layer(byte_embedding)\n",
    "        return self.output_proj(byte_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdd10be-63ee-4368-879b-4b8bf41c8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(pred,target):\n",
    "    return torch.sum((pred - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "139c2639-ab27-440c-83d1-6a0d53a2f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\",torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\",torch.zeros(config.d_model,config.d_model))\n",
    "\n",
    "        self.query = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.key = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.value = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self,x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q,self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self,x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        v_pred = torch.matmul(k,self.M)\n",
    "        loss = l2_loss(v_pred,v)\n",
    "        error = v_pred - v\n",
    "\n",
    "        g = 2 * torch.matmul(error.t(),k)\n",
    "\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1-self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee0ef00-9135-44ca-8232-bb884f63e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim = config.d_model, num_heads = config.n_heads, batch_first= True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,_ = x.size()\n",
    "        output = []\n",
    "\n",
    "        for i in range(0,seq_len,self.window_size):\n",
    "            x_chunk = x[:,i:i+self.window_size,:]\n",
    "            attn_out,_ = self.attention(x_chunk,x_chunk,x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output,dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33afbce5-b124-4fd5-bed6-9e5c18fe9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p,config.d_model))\n",
    "\n",
    "    def forward(self,batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7dfe5a-bdc1-4b06-bc4b-5c68b8711e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,d_model = x.size()\n",
    "\n",
    "        x_flat = x.reshape(-1,d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        \n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens,x],dim=1)\n",
    "\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        out_flat = out.reshape(-1,self.d_model)\n",
    "        long_term = self.long_memory(out_flat)\n",
    "        long_term = long_term.reshape(batch_size,-1,d_model)\n",
    "\n",
    "        output = y * long_term\n",
    "        output = output[:,-seq_len:,:]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7efc4ecb-2433-48e4-91d9-0f58dd044bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self,patch_dim,n_heads,ff_dim,n_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TitanMAG(patch_dim,n_heads,ff_dim,dropout) for _ in range(n_layers)])\n",
    "    def forward(self,patches,attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches,attn_mask=attn_mask)\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8119553d-e217-493e-b3ba-ae15e5653a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteLatentTransformer(nn.Module):\n",
    "    def __init__(self,byte_dim,patch_dim,vocab_size,n_heads,ff_dim,n_encoder,n_decoder,n_global,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim,vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim,n_heads,ff_dim,n_layers=n_encoder,dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim,n_heads,ff_dim,n_layers=n_global,dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim,byte_dim,n_heads,ff_dim,n_decoder,dropout=dropout) \n",
    "        self.projection = nn.Linear(byte_dim,patch_dim)\n",
    "\n",
    "    def forward(self,byte_seq,hash_seq,patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq,hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings,dim=1,keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings,patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings,byte_embeddings)\n",
    "        return byte_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d55d678-7a1f-4eca-8747-2cca3f511a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "class LatentGlobalTransformer(nn.Module):\n",
    "    def __init__(self, patch_dim, n_heads, ff_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        config = SimpleNamespace(\n",
    "            d_model = patch_dim,\n",
    "            n_heads = n_heads,\n",
    "            ff_dim = ff_dim,\n",
    "            dropout = dropout,\n",
    "            window_size = 16,  \n",
    "            n_layers = 2,      \n",
    "            alpha = 0.1,\n",
    "            eta = 0.01,\n",
    "            theta = 0.01,\n",
    "            N_p = 10\n",
    "        )\n",
    "        self.layers = nn.ModuleList([TitanMAG(config) for _ in range(n_layers)])\n",
    "    def forward(self, patches, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            patches = layer(patches)\n",
    "        return patches\n",
    "\n",
    "class ByteLatentTitan(nn.Module):\n",
    "    def __init__(self, byte_dim, patch_dim, vocab_size, n_heads, ff_dim, n_encoder, n_decoder, n_global, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.byte_embed = ByteEmbedding(byte_dim, vocab_size)\n",
    "        self.local_encoder = LocalEncoder(byte_dim, n_heads, ff_dim, n_layers=n_encoder, dropout=dropout)\n",
    "        self.global_transformer = LatentGlobalTransformer(patch_dim, n_heads, ff_dim, n_layers=n_global, dropout=dropout)\n",
    "        self.local_decoder = LocalDecoder(patch_dim, byte_dim, n_heads, ff_dim, n_layers=n_decoder, dropout=dropout)\n",
    "        self.projection = nn.Linear(byte_dim, patch_dim)\n",
    "\n",
    "    def forward(self, byte_seq, hash_seq, patch_seq):\n",
    "        byte_embeddings = self.byte_embed(byte_seq, hash_seq)\n",
    "        if patch_seq is None:\n",
    "            patch_embeddings = torch.mean(byte_embeddings, dim=1, keepdim=True)\n",
    "            patch_embeddings = self.local_encoder(byte_embeddings, patch_embeddings)\n",
    "            patch_embeddings = self.projection(patch_embeddings)\n",
    "        else:\n",
    "            patch_embeddings = patch_seq\n",
    "        patch_embeddings = self.global_transformer(patch_embeddings)\n",
    "        byte_output = self.local_decoder(patch_embeddings, byte_embeddings)\n",
    "        return byte_output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define model hyperparameters\n",
    "    byte_dim = 64\n",
    "    patch_dim = 128\n",
    "    vocab_size = 256  # both for byte and hash embedding\n",
    "    n_heads = 8\n",
    "    ff_dim = 256\n",
    "    n_encoder = 2\n",
    "    n_decoder = 2\n",
    "    n_global = 2\n",
    "    dropout = 0.1\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim, n_encoder, n_decoder, n_global, dropout)\n",
    "    \n",
    "    # Create dummy inputs\n",
    "    batch_size = 2\n",
    "    seq_len = 128\n",
    "    # byte_seq: integers in [0, 256)\n",
    "    byte_seq = torch.randint(0, 256, (batch_size, seq_len))\n",
    "    # hash_seq: integers in [0, 256)\n",
    "    hash_seq = torch.randint(0, 256, (batch_size, seq_len))\n",
    "    # We set patch_seq to None so the model computes its own patch embeddings.\n",
    "    patch_seq = None\n",
    "\n",
    "    # Run a forward pass\n",
    "    output = model(byte_seq, hash_seq, patch_seq)\n",
    "    \n",
    "    print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44d02aee-e471-4fa0-a891-413b333fa23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autoregressive training with sampling strategies...\n",
      "\n",
      "Epoch [1] Total Parameters: 17271296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/6: 100%|████████████████████████████| 1262/1262 [26:39<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Average Loss: 2.8119\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:    World Economic Forum. During the protests, ad hoc leaderless ano\n",
      "Generated: World Economic Forum. During the protests, ad hoc leaderless anola\n",
      " ily unga�f�daniaf\"StuV70oLi\n",
      "\n",
      " n)mgovea wEd1�c.\n",
      ".te igfUFohcs. omcDoOc�eorftrdrpo thine,kcc4%8alSlncs af es\n",
      "ng wmon.dd an o)lyrcoxgayssipedz�rc..7t\n",
      "nfSkyd iganbe  a di):om,x�a aknfceswg\n",
      "\n",
      "kssft-hoOk.\n",
      ".4lycL.k\n",
      " rto.8ar. s2u.\"T st th lm\n",
      "\n",
      "Epoch [2] Total Parameters: 17271296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/6: 100%|████████████████████████████| 1262/1262 [26:42<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] Average Loss: 2.6164\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:    o play the role of facilitator to help achieve a consensus witho\n",
      "Generated: o play the role of facilitator to help achieve a consensus withoh,1\u0010tmhll tmibu,'Prytitcck.o:apouo.c\n",
      "nofeafetsthmmus.a\n",
      "  in  oby2ks,rdinc rg d  st a rtms,beamafrdan,' ed ,xds\n",
      "  ldonf\",nnchlies)1u�\"6�dgyf hugire wshicc ltasiperffot.\n",
      "  a;�\n",
      "Ss,  idi.6T uwni oggamein-ssogfsthybui)bch,q\n",
      " shis,-z\n",
      "Nemoe om\n",
      "\n",
      "Epoch [3] Total Parameters: 17271296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/6: 100%|████████████████████████████| 1262/1262 [26:54<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] Average Loss: 2.6084\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:    . Anarchists usually form small groups (5–20 individuals) to e\n",
      "Generated: . Anarchists usually form small groups (5–20 individuals) to e-on-e ngob lrws\n",
      "Ky\n",
      "19 ppplwanizcamagatimady-ro usefrycelubem- t ave ifommugmbo so astrodu an a.r asyeby,7as ondcliarty-gak ind wb o adtir ad\n",
      "ss an urcrats\n",
      "\n",
      "K c\n",
      "Scr a,2265chap.2 ivir\n",
      " t  whn\n",
      "oery m, nod tald illad.crpo a ba,/k\"�rstnc,d  \n",
      "\n",
      "Epoch [4] Total Parameters: 17271296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|████████████████████████████| 1262/1262 [27:00<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] Average Loss: 2.6047\n",
      "\n",
      "--- Sample Generation ---\n",
      "Prompt:    s of their group without the need of a leader or a leading group\n",
      "Generated: s of their group without the need of a leader or a leading groupwsavlyctwowgtrontaue, e adinravdlembn,'s  ebe asb ie,2,�\n",
      "Mo mi l mancujuy itedmo rak\"alunl ical  lw t,  ofuosugiogncub t.\n",
      "thttee,bvems\n",
      " pap,kmaiove-id spis\n",
      "S,:  lls rw rd.\n",
      "2dsudvedvonas.atlu,;�Theces s chicubo on tuud  e d-vsopidrnnd pp\n",
      "\n",
      "Epoch [5] Total Parameters: 17271296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6:   5%|█▌                            | 64/1262 [01:22<25:48,  1.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 198\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m     train()\n",
      "Cell \u001b[0;32mIn[16], line 167\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Forward pass. (Our autoregressive objective: predict next token)\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m output \u001b[38;5;241m=\u001b[39m model(byte_seq, hash_seq, patch_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Shift outputs and targets by one.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m logits \u001b[38;5;241m=\u001b[39m output[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Predictions for positions 1 ... end.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m, in \u001b[0;36mByteLatentTitan.forward\u001b[0;34m(self, byte_seq, hash_seq, patch_seq)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     patch_embeddings \u001b[38;5;241m=\u001b[39m patch_seq\n\u001b[0;32m---> 40\u001b[0m patch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_transformer(patch_embeddings)\n\u001b[1;32m     41\u001b[0m byte_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_decoder(patch_embeddings, byte_embeddings)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m byte_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[15], line 20\u001b[0m, in \u001b[0;36mLatentGlobalTransformer.forward\u001b[0;34m(self, patches, attn_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, patches, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 20\u001b[0m         patches \u001b[38;5;241m=\u001b[39m layer(patches)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m patches\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mTitanMAG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m x_flat \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,d_model)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_memory\u001b[38;5;241m.\u001b[39mupdate_memory(x_flat)\n\u001b[1;32m     17\u001b[0m persistent_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent(batch_size)\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([persistent_tokens,x],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mTitanMemory.update_memory\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m B \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(B):\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_memory(x[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     28\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mTitanMemory.update_memory\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     28\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\n\u001b[0;32m---> 29\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[1;32m     31\u001b[0m v_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(k,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM)\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m l2_loss(v_pred,v)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================== Sampling Utilities =====================\n",
    "\n",
    "def sample_from_logits(logits, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    \"\"\"\n",
    "    Given a 1D tensor of logits, apply temperature scaling,\n",
    "    then filter using top-k and/or nucleus (top-p) sampling, and sample one token.\n",
    "    \"\"\"\n",
    "    # Temperature scaling\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Top-k filtering\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_value = values[-1]\n",
    "        logits[logits < min_value] = -float('Inf')\n",
    "    \n",
    "    # Nucleus (top-p) filtering\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        probs = torch.softmax(sorted_logits, dim=-1)\n",
    "        cumulative_probs = torch.cumsum(probs, dim=-1)\n",
    "        \n",
    "        # Remove tokens with cumulative probability above threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the mask to keep at least one token\n",
    "        sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "        sorted_indices_to_remove[0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = -float('Inf')\n",
    "    \n",
    "    # Convert to probabilities and sample\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(model, prompt, max_length, device, temperature=1.0, top_k=0, top_p=0.0):\n",
    "    model.eval()\n",
    "    generated = prompt.clone()\n",
    "    for _ in range(max_length - prompt.size(1)):\n",
    "        # In our model, we use the same sequence for both byte_seq and hash_seq.\n",
    "        hash_seq = generated.clone()\n",
    "        with torch.no_grad():\n",
    "            output = model(generated, hash_seq, patch_seq=None)\n",
    "        # Get logits for the last time step. (Assuming batch size 1)\n",
    "        next_logits = output[:, -1, :]  # shape: (1, vocab_size)\n",
    "        # Squeeze to get a 1D tensor of logits.\n",
    "        next_token = sample_from_logits(next_logits.squeeze(0),\n",
    "                                        temperature=temperature,\n",
    "                                        top_k=top_k,\n",
    "                                        top_p=top_p)\n",
    "        # Append the sampled token.\n",
    "        next_token = next_token.unsqueeze(0)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "    return generated\n",
    "\n",
    "# ===================== WikiByteDataset Definition =====================\n",
    "\n",
    "class WikiByteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Converts Wikipedia text samples to fixed-length byte sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, hf_dataset, seq_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset[idx]['text']\n",
    "        # Convert text to bytes (ignoring errors)\n",
    "        text_bytes = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "        # If the text is shorter than seq_len, pad with spaces (ASCII 32)\n",
    "        if len(text_bytes) < self.seq_len:\n",
    "            text_bytes = text_bytes + b\" \" * (self.seq_len - len(text_bytes))\n",
    "        else:\n",
    "            # Randomly select a contiguous segment of length seq_len\n",
    "            start = torch.randint(0, len(text_bytes) - self.seq_len + 1, (1,)).item()\n",
    "            text_bytes = text_bytes[start:start+self.seq_len]\n",
    "        byte_seq = list(text_bytes)\n",
    "        # For this example, we use the same sequence as a dummy hash sequence.\n",
    "        hash_seq = byte_seq.copy()\n",
    "        return {\n",
    "            \"byte_seq\": torch.tensor(byte_seq, dtype=torch.long),\n",
    "            \"hash_seq\": torch.tensor(hash_seq, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    byte_seqs = torch.stack([item['byte_seq'] for item in batch])\n",
    "    hash_seqs = torch.stack([item['hash_seq'] for item in batch])\n",
    "    return byte_seqs, hash_seqs\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Model/training hyperparameters\n",
    "    byte_dim = 128\n",
    "    patch_dim = 256\n",
    "    vocab_size = 256      # Bytes: 0-255\n",
    "    n_heads = 8\n",
    "    ff_dim = 2048\n",
    "    n_encoder = 6\n",
    "    n_decoder = 6\n",
    "    n_global = 12\n",
    "    dropout = 0.1\n",
    "    seq_len = 128        # Fixed sequence length (in bytes)\n",
    "    batch_size = 256\n",
    "    epochs = 6\n",
    "    lr = 1e-4\n",
    "\n",
    "    # Sampling hyperparameters\n",
    "    temperature = 1.2  # Increase temperature for more diversity.\n",
    "    top_k = 40         # Consider only top 40 tokens.\n",
    "    top_p = 0.9        # Nucleus sampling threshold.\n",
    "\n",
    "    # Load a subset of Wikipedia.\n",
    "    hf_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:5%]\")\n",
    "    dataset = WikiByteDataset(hf_dataset, seq_len=seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    # Instantiate your model. (Make sure ByteLatentTitan and its dependencies are defined/imported.)\n",
    "    model = ByteLatentTitan(byte_dim, patch_dim, vocab_size, n_heads, ff_dim,\n",
    "                            n_encoder, n_decoder, n_global, dropout)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    print(\"Starting autoregressive training with sampling strategies...\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        # Print total parameter count for the model at the beginning of the epoch.\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"\\nEpoch [{epoch+1}] Total Parameters: {total_params}\")\n",
    "        \n",
    "        for byte_seq, hash_seq in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            byte_seq = byte_seq.to(device)\n",
    "            hash_seq = hash_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass. (Our autoregressive objective: predict next token)\n",
    "            output = model(byte_seq, hash_seq, patch_seq=None)\n",
    "            # Shift outputs and targets by one.\n",
    "            logits = output[:, :-1, :]  # Predictions for positions 1 ... end.\n",
    "            target = byte_seq[:, 1:]    # Ground truth tokens (shifted by one).\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}] Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ----- Autoregressive Generation Sample -----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample = dataset[0]\n",
    "            prompt_tokens = sample['byte_seq'][:seq_len//2].unsqueeze(0).to(device)\n",
    "            generated = generate_text(model, prompt_tokens, max_length=300, device=device,\n",
    "                                      temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "            generated_list = generated.squeeze(0).cpu().tolist()\n",
    "            try:\n",
    "                generated_text = bytes(generated_list).decode(\"utf-8\", errors=\"replace\")\n",
    "            except Exception as e:\n",
    "                generated_text = str(generated_list)\n",
    "            prompt_text = bytes(prompt_tokens.squeeze(0).cpu().tolist()).decode(\"utf-8\", errors=\"replace\")\n",
    "            print(\"\\n--- Sample Generation ---\")\n",
    "            print(\"Prompt:   \", prompt_text)\n",
    "            print(\"Generated:\", generated_text)\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2fb02-ba2e-4a26-b2e4-56f3c0861d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
