{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d91d8c67-b351-4968-9579-297f7ff9616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4c538b4-7b3e-4998-8cb6-ea3c1d874f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MistralConfig:\n",
    "    vocab_size = 1000\n",
    "    d_model = 128\n",
    "    d_ff = 1024\n",
    "    layers = 6\n",
    "    n_head = 4\n",
    "    kv_head = 2\n",
    "    max_pos_embed = 512\n",
    "    sliding_window = 256\n",
    "    hidden = 'silu'\n",
    "    eps = 1e-6\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 16\n",
    "    seq_len = 64\n",
    "    head_dim = d_model // n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b8edd7-c021-430d-a382-f232b4c75fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,d_model,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.weights = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = torch.mean(x**2,dim=-1,keepdim=True)\n",
    "        rms = torch.sqrt(mean+self.eps)\n",
    "        x = (x / rms) * self.weights\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f6edd9-1fa2-4816-93df-8b5698402337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(head_dim,max_pos_embed,theta=10000.0):\n",
    "    freqs = 1.0 / theta ** (torch.arange(0,head_dim,2).float() / head_dim)\n",
    "    pos = torch.arange(max_pos_embed)\n",
    "    angles = torch.outer(pos,freqs)\n",
    "    return torch.polar(torch.ones_like(angles),angles)\n",
    "\n",
    "def apply_rotary_embed(x,freqs_cis):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1],-1,2))\n",
    "    freqs_cis = freqs_cis.reshape(1,x.shape[1],1,-1)\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs_cis)\n",
    "    x_rotated = x_rotated.reshape(x.shape)\n",
    "    return x_rotated.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ab7f74-49cb-4d7c-ac44-6945299f058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1219f4-3155-4214-a709-ed49254f9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x,n_rep):\n",
    "    batch_size,seq_len,kv_head,head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,:,None,:].expand(batch_size,seq_len,kv_head,n_rep,head_dim).reshape(batch_size,seq_len,kv_head * n_rep, head_dim)\n",
    "        )\n",
    "    \n",
    "class MistralAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.kv_head = config.kv_head\n",
    "        self.head_dim = config.head_dim\n",
    "        self.sliding_window = config.sliding_window\n",
    "        self.n_rep = self.n_head // self.kv_head\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.d_model,self.d_model,bias=False)\n",
    "        self.k_proj = nn.Linear(self.d_model,self.kv_head * self.head_dim , bias=False)\n",
    "        self.v_proj = nn.Linear(self.d_model,self.kv_head * self.head_dim ,bias=False)\n",
    "        self.o_proj = nn.Linear(self.d_model,self.d_model,bias=False)\n",
    "\n",
    "        self.cache_k = None\n",
    "        self.cache_v = None\n",
    "\n",
    "    def _init_cache(self, batch_size):\n",
    "        device = self.q_proj.weight.device\n",
    "        self.cache_k = torch.zeros(\n",
    "            (batch_size, self.sliding_window, self.kv_head, self.head_dim),\n",
    "            device=device\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (batch_size, self.sliding_window, self.kv_head, self.head_dim),\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "    def _roll_cache(self):\n",
    "        self.cache_k = torch.roll(self.cache_k, shifts=-self.config.seq_len, dims=1)\n",
    "        self.cache_v = torch.roll(self.cache_v, shifts=-self.config.seq_len, dims=1)\n",
    "    \n",
    "    def forward(self, x, freqs_cis):\n",
    "        batch_size,seq_len,_ = x.shape\n",
    "\n",
    "        if self.cache_k is None or self.cache_k.size(0) != batch_size:\n",
    "            self._init_cache(batch_size)\n",
    "            \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(batch_size, seq_len, self.n_head, self.head_dim)\n",
    "        k = k.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "    \n",
    "        q = apply_rotary_embed(q, freqs_cis)\n",
    "        k = apply_rotary_embed(k,freqs_cis)\n",
    "        \n",
    "        self._roll_cache()\n",
    "        \n",
    "        valid_len = min(seq_len,self.sliding_window)\n",
    "        self.cache_k[:,-valid_len:,:,:] = k.detach()\n",
    "        self.cache_v[:,-valid_len:,:,:] = v.detach()\n",
    "\n",
    "        keys = repeat_kv(self.cache_k, self.n_rep)\n",
    "        values = repeat_kv(self.cache_v, self.n_rep)\n",
    "        \n",
    "        q = q.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        attn_weights = (q @ keys.transpose(-2,-1)) / math.sqrt(self.config.head_dim)\n",
    "        attn_weights = F.softmax(attn_weights,dim=-1)\n",
    "        output = (attn_weights @ values).transpose(1,2).reshape(batch_size,seq_len,-1)\n",
    "        \n",
    "        return self.o_proj(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdad286e-af5c-465c-9a15-3379dc144ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        d_ff = int(2 * config.d_ff / 3)\n",
    "        self.gate_proj = nn.Linear(config.d_model,d_ff,bias=False)\n",
    "        self.layer1 = nn.Linear(config.d_model,d_ff,bias=False)\n",
    "        self.layer2 = nn.Linear(d_ff,config.d_model,bias=False)\n",
    "        self.act = F.silu\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer2(self.act(self.gate_proj(x)) * self.layer1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadaee4c-28f9-4c71-8d6d-e637490afd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.attn = MistralAttention(config)\n",
    "        self.mlp = MistralMLP(config)\n",
    "        self.norm1 = RMSNorm(config.d_model,config.eps)\n",
    "        self.norm2 = RMSNorm(config.d_model,config.eps)\n",
    "\n",
    "    def forward(self,x,freqs_cis):\n",
    "        x= x + self.attn(self.norm1(x),freqs_cis)\n",
    "        out = x + self.mlp(self.norm2(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6705b53-e83b-4dac-ae0e-8748071bf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "        def __init__(self,config):\n",
    "            super().__init__()\n",
    "            assert config.vocab_size != -1\n",
    "            self.config = config\n",
    "            self.embed = InputEmbedding(config.vocab_size,config.d_model)\n",
    "            self.layers = nn.ModuleList([MistralBlock(config) for _ in range(config.layers)])\n",
    "            self.norm = RMSNorm(config.d_model,config.eps)\n",
    "            self.output = nn.Linear(config.d_model,config.vocab_size,bias=False)\n",
    "            self.freqs_cis = precompute_freqs_cis(config.head_dim,config.max_pos_embed).to(config.device)\n",
    "        def reset_cache(self):\n",
    "            for layer in self.layers:\n",
    "                layer.attn.cache_k = None\n",
    "                layer.attn.cache_v = None\n",
    "            \n",
    "        def forward(self,x):\n",
    "            batch_size,seq_len = x.shape\n",
    "            x = self.embed(x)\n",
    "            freq_cis = self.freqs_cis[:seq_len]\n",
    "            \n",
    "            for layer in self.layers:\n",
    "                x = layer(x,freq_cis)\n",
    "            x = self.norm(x)\n",
    "            logits = self.output(x)\n",
    "            return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d838a94-6877-4b68-9964-bf0836309caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([16, 64, 1000])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = MistralConfig()\n",
    "    model = Transformer(config).to(config.device)\n",
    "    dummy_input = torch.randint(config.vocab_size, (config.batch_size, config.seq_len), device=config.device)\n",
    "    logits = model(dummy_input)\n",
    "    print(f\"Logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded5b0f-3df1-4df4-961f-f2bea973bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.2663\n",
      "Sample Output after Epoch 1:\n",
      "The,  ddddlllllll ttte ishllf tttss st tp st t ts t t t t t thanenef theeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Average Loss: 0.0444\n",
      "Sample Output after Epoch 2:\n",
      "Theeedddequurhhrrdddex?'dddd'sdddrrrddhheelllhiqullhiquququ?? dddddodood qummmollllllllllllllll quququq\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Average Loss: 0.0368\n",
      "Sample Output after Epoch 3:\n",
      "Thedd HAUME:\n",
      "\n",
      "CLIZHARLARICES:\n",
      "\n",
      "Soodd bbbbbbbbbbb:\n",
      "\n",
      "KEEEN ttttttterros teeerron t ttteerron thereeed the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Average Loss: 0.0344\n",
      "Sample Output after Epoch 4:\n",
      "Therrryy   nnneee,\n",
      "\n",
      "\n",
      "DUCENNII:\n",
      "Whous the the the the the ttrus tttttttttttttttttttttttttttttttttttttttt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Average Loss: 0.0330\n",
      "Sample Output after Epoch 5:\n",
      "The my hillll thee theee the the the the theeee theeee theee theeeeeeeee thee theeeee theee theeee thee\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:  95%|██████████▍| 66524/69708 [12:17<00:36, 87.28it/s, Loss=0.0324]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self, text):\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return torch.tensor([self.stoi[c] for c in text], dtype=torch.long)\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return ''.join([self.itos[token] for token in tokens])\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.chars)\n",
    "\n",
    "def download_and_preprocess_dataset():\n",
    "    import requests\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "\n",
    "    tokenizer = CustomTokenizer(text)\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return encoded, tokenizer\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len]\n",
    "        y = self.data[idx + 1:idx + self.seq_len + 1]\n",
    "        return x, y\n",
    "\n",
    "def generate_sample_output(model, tokenizer, config, start_text=\"The\", seq_len=100):\n",
    "    model.eval()\n",
    "    model.reset_cache()\n",
    "    \n",
    "    # Encode and add batch dimension correctly\n",
    "    encoded = tokenizer.encode(start_text)\n",
    "    generated = encoded.unsqueeze(0).to(config.device)  # Shape [1, seq_len]\n",
    "    \n",
    "    for _ in range(seq_len):\n",
    "        context = generated[:, -config.sliding_window:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(context)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "        generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated[0].tolist())\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, config, tokenizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for i, (x, y) in enumerate(progress_bar):\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "\n",
    "            # Clear gradients FIRST\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # Log epoch average loss\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "        \n",
    "        # Generate a sample output\n",
    "        sample_output = generate_sample_output(model, tokenizer, config, start_text=\"The\", seq_len=100)\n",
    "        print(f\"Sample Output after Epoch {epoch + 1}:\\n{sample_output}\\n\")\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    # Load and preprocess the dataset\n",
    "    data, tokenizer = download_and_preprocess_dataset()\n",
    "\n",
    "    # Update the configuration with the vocabulary size\n",
    "    config = MistralConfig()\n",
    "    config.vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "    # Create the dataset and dataloader\n",
    "    seq_len = config.seq_len\n",
    "    dataset = ShakespeareDataset(data, seq_len)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the model, criterion, and optimizer\n",
    "    model = Transformer(config).to(config.device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, dataloader, optimizer, criterion, config, tokenizer, epochs=10)\n",
    "\n",
    "    # Save the model and tokenizer\n",
    "    torch.save(model.state_dict(), \"mistral_transformer.pth\")\n",
    "    torch.save(tokenizer, \"custom_tokenizer.pth\")\n",
    "    print(\"Model and tokenizer saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac687f19-fe62-40fa-8fe1-d9d25c10b6f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m model \u001b[38;5;241m=\u001b[39m MoEModel(input_dim, hidden_dim, num_experts, num_classes, top_k)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m output \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Print the output\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m, in \u001b[0;36mMoEModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Apply the MoE layer\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoe_layer(x)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Apply the final classification layer\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m, in \u001b[0;36mMoELayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m expert_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts[i](x) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperts))]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Select only the top-K experts' outputs for each input in the batch\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Create a tensor to hold the top-K expert outputs\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m top_k_expert_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([expert_outputs[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_k_indices], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Weighted sum of the top-K expert outputs\u001b[39;00m\n\u001b[1;32m     37\u001b[0m weighted_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(top_k_expert_outputs \u001b[38;5;241m*\u001b[39m top_k_values\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the MoE Layer\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts, top_k=2):\n",
    "        super(MoELayer, self).__init__()\n",
    "        \n",
    "        # Define the experts as a list of fully connected layers\n",
    "        self.experts = nn.ModuleList([nn.Linear(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        \n",
    "        # Define the gating network (a simple feed-forward neural network)\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        \n",
    "        # Number of experts to select\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the gating weights (probabilities) from the gating network\n",
    "        gating_weights = self.gate(x)  # Shape: [batch_size, num_experts]\n",
    "        \n",
    "        # Apply softmax to get probabilities (normalized weights for each expert)\n",
    "        gating_probs = F.softmax(gating_weights, dim=-1)  # Shape: [batch_size, num_experts]\n",
    "        \n",
    "        # Get the indices of the top-K experts for each input\n",
    "        top_k_values, top_k_indices = torch.topk(gating_probs, self.top_k, dim=-1)\n",
    "        \n",
    "        # Gather the top-K expert outputs\n",
    "        expert_outputs = [self.experts[i](x) for i in range(len(self.experts))]\n",
    "        \n",
    "        # Select only the top-K experts' outputs for each input in the batch\n",
    "        # Create a tensor to hold the top-K expert outputs\n",
    "        top_k_expert_outputs = torch.stack([expert_outputs[i] for i in top_k_indices], dim=1)\n",
    "        \n",
    "        # Weighted sum of the top-K expert outputs\n",
    "        weighted_output = torch.sum(top_k_expert_outputs * top_k_values.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        return weighted_output\n",
    "\n",
    "# Define the full MoE model\n",
    "class MoEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_experts, num_classes, top_k=2):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.moe_layer = MoELayer(input_dim, hidden_dim, num_experts, top_k)\n",
    "        self.fc_out = nn.Linear(hidden_dim, num_classes)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the MoE layer\n",
    "        x = self.moe_layer(x)\n",
    "        # Apply the final classification layer\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "# Example of using the MoE Model\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = 128   # Input dimension (e.g., feature size)\n",
    "    hidden_dim = 64   # Hidden dimension\n",
    "    num_experts = 8   # Number of experts\n",
    "    num_classes = 10  # Number of output classes (for classification)\n",
    "    top_k = 3         # Top-K experts to select\n",
    "\n",
    "    # Create a random input tensor (batch size of 32)\n",
    "    batch_size = 32\n",
    "    x = torch.randn(batch_size, input_dim)\n",
    "\n",
    "    # Instantiate the MoE model\n",
    "    model = MoEModel(input_dim, hidden_dim, num_experts, num_classes, top_k)\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "\n",
    "    # Print the output\n",
    "    print(output.shape)  # Expected output: [batch_size, num_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb7fc4-9a71-440c-b557-da9f00c8a366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
