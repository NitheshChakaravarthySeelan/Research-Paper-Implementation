{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91d8c67-b351-4968-9579-297f7ff9616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4c538b4-7b3e-4998-8cb6-ea3c1d874f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MistralConfig:\n",
    "    vocab_size = 1000\n",
    "    d_model = 128\n",
    "    d_ff = 1024\n",
    "    layers = 6\n",
    "    n_head = 4\n",
    "    kv_head = 2\n",
    "    max_pos_embed = 512\n",
    "    sliding_window = 256\n",
    "    hidden = 'silu'\n",
    "    eps = 1e-6\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 16\n",
    "    seq_len = 64\n",
    "\n",
    "    @property\n",
    "    def head_dim(self):\n",
    "        return self.d_model // self.n_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6b8edd7-c021-430d-a382-f232b4c75fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,d_model,eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.weights = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps \n",
    "\n",
    "    def forward(self,x):\n",
    "        mean = torch.mean(x**2,dim=-1,keepdim=True) / self.d_model\n",
    "        rms = torch.sqrt(mean+self.eps)\n",
    "        x = (x / rms) * self.weights\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9f6edd9-1fa2-4816-93df-8b5698402337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(head_dim,max_pos_embed,theta=10000.0):\n",
    "    freqs = 1.0 / theta ** (torch.arange(0,head_dim,2).float() / head_dim)\n",
    "    pos = torch.arange(max_pos_embed)\n",
    "    angles = torch.outer(pos,freqs)\n",
    "    return torch.polar(torch.ones_like(angles),angles)\n",
    "\n",
    "def apply_rotary_embed(xq,xk,freqs_cis):\n",
    "    xq_complex = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))\n",
    "    xk_complex = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))\n",
    "    freq_cis = freqs_cis.unsqueeze(0).unsqueeze(0)\n",
    "    print(xq_complex.shape)\n",
    "    print(freq_cis.shape)\n",
    "    xq_out = torch.view_as_real(xq_complex * freq_cis).flatten(2)\n",
    "    xk_out = torch.view_as_real(xk_complex * freq_cis).flatten(2)\n",
    "    xq_out = xq_out.reshape(*xq.shape)\n",
    "    xk_out = xk_out.reshape(*xk.shape)\n",
    "    return xq_out.type_as(xq),xk_out.type_as(xk)\n",
    "\n",
    "def repeat_kv(x,n_rep):\n",
    "    batch_size,seq_len,kv_head,head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:,:,:,None,:].expand(batch_size,seq_len,kv_head,n_rep,head_dim).reshape(batch_size,seq_len,kv_head * n_rep, head_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33ab7f74-49cb-4d7c-ac44-6945299f058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size,d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c67f57-ccd3-4616-b29a-d2f68ceb4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''class MistralAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.kv_head = config.kv_head\n",
    "        self.head_dim = config.head_dim\n",
    "        self.sliding_window = config.sliding_window\n",
    "        self.n_rep = self.n_head // self.kv_head\n",
    "\n",
    "        self.wq = nn.Linear(d_model,n_head*head_dim,bias=False)\n",
    "        self.wk = nn.Linear(d_model,kv_head * head_dim,bias=False)\n",
    "        self.wv = nn.Linear(d_model,kv_head * head_dim,bias=False)\n",
    "        self.wo = nn.Linear(d_model,kv_head * head_dim,bias=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(q,k,v,mask=None):\n",
    "        attn = q @ k.transpose(-1,-2) / config.head_dim ** 0.5\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask==0,-1e9)\n",
    "        attn = torch.softmax(attn,dim=-1)\n",
    "        return (attn @ v) , attn\n",
    "        \n",
    "    def forward(self,x,freqs_cis,cache=None,mask=None):\n",
    "        assert mask is None or cache is None\n",
    "        seq_len_sum , _ = x.shape\n",
    "        xq,xk,xv = self.wq(x),self.wk(x),self.wv(x)\n",
    "        xq = xq.view(seq_len_sum,self.n_head,self.head_dim)\n",
    "        xk = xk.view(seq_len_sum,self.kv_head,self.head_dim)\n",
    "        xv = xv.view(seq_len_sum,self.kv_head,self.head_dim)\n",
    "        xq,xk = apply_rotary_embed(xq,xk,freqs_cis=freqs_cis)\n",
    "\n",
    "        if cache is None:\n",
    "            key,val = xk,xv\n",
    "        elif cache.prefill:\n",
    "            key,val = cache.interleave_kv(xk,xv)\n",
    "            cache.update(xk,xv)\n",
    "        else:\n",
    "            cache.update(xk,xv)\n",
    "            key,val = cache.key,cache.value\n",
    "            key = key.view(seq_len_sum * cache.max_seq_len,self.kv_head,self.head_dim)\n",
    "            val = val.view(seq_len_sum * cache.max_seq_len,self.kv_head,self.head_dim)\n",
    "\n",
    "        key,val = repeat_kv(key,val,self.n_rep,dim=1)\n",
    "        xq,key,val = xq[None,...],key[None,...],val[None,...]\n",
    "        output = MistralAttention.attention(xq,key,val,mask if cache is None else cache.mask)\n",
    "        output = output.view(seq_len_sum , self.n_head * self.head_dim)\n",
    "        return self.wo(output)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b1219f4-3155-4214-a709-ed49254f9a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.kv_head = config.kv_head\n",
    "        self.head_dim = config.head_dim\n",
    "        self.sliding_window = config.sliding_window\n",
    "        self.n_rep = self.n_head // self.kv_head\n",
    "        \n",
    "        self.q_proj = nn.Linear(self.d_model,self.d_model,bias=False)\n",
    "        self.k_proj = nn.Linear(self.d_model,self.kv_head * self.head_dim , bias=False)\n",
    "        self.v_proj = nn.Linear(self.d_model,self.kv_head * self.head_dim ,bias=False)\n",
    "        self.o_proj = nn.Linear(self.d_model,self.d_model,bias=False)\n",
    "\n",
    "        self.register_buffer(\"cache_k\",torch.zeros((config.batch_size,self.sliding_window,self.kv_head,self.head_dim),device=config.device))\n",
    "        self.register_buffer(\"cache_v\",torch.zeros((config.batch_size,self.sliding_window,self.kv_head,self.head_dim),device=config.device))\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(q,k,v,mask=None):\n",
    "        attn = q @ k.transpose(-1,-2) / config.head_dim ** 0.5\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask==0,-1e9)\n",
    "        attn = torch.softmax(attn,dim=-1)\n",
    "        return (attn @ v) , attn\n",
    "    \n",
    "    def forward(self, x, freqs_complex, mask=None):\n",
    "        if mask is None:\n",
    "            mask = torch.ones(x.shape[0], 1, x.shape[1], x.shape[1], device=x.device)\n",
    "    \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_head, self.head_dim)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.kv_head, self.head_dim)\n",
    "        v = v.view(v.shape[0], v.shape[1], self.kv_head, self.head_dim)\n",
    "    \n",
    "        q, _ = apply_rotary_embed(q, q, freqs_cis=freqs_complex)\n",
    "        k, _ = apply_rotary_embed(k, k, freqs_cis=freqs_complex)\n",
    "    \n",
    "        self.cache_k = torch.cat((self.cache_k[:, 1:], k[:, :, -1:]), dim=1)\n",
    "        self.cache_v = torch.cat((self.cache_v[:, 1:], v[:, :, -1:]), dim=1)\n",
    "    \n",
    "        keys = repeat_kv(self.cache_k, self.n_rep)\n",
    "        values = repeat_kv(self.cache_v, self.n_rep)\n",
    "\n",
    "        seq_len = q.shape[-3]\n",
    "        sliding_mask = torch.zeros((seq_len, seq_len), device=x.device)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.sliding_window + 1)\n",
    "            end = i + 1\n",
    "            sliding_mask[i, start:end] = 1\n",
    "\n",
    "        q = q.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        # Combine the input mask and sliding mask\n",
    "        mask = mask.unsqueeze(0).unsqueeze(1) & sliding_mask.unsqueeze(0).unsqueeze(1)\n",
    "        \n",
    "        x, self.attn = MistralAttention.attention(q, keys, values, mask)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.n_head * self.head_dim)\n",
    "        return self.o_proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdad286e-af5c-465c-9a15-3379dc144ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralMLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        d_ff = int(2 * config.d_ff / 3)\n",
    "        self.gate_proj = nn.Linear(config.d_model,d_ff,bias=False)\n",
    "        self.layer1 = nn.Linear(config.d_model,d_ff,bias=False)\n",
    "        self.layer2 = nn.Linear(d_ff,config.d_model,bias=False)\n",
    "        self.act = F.silu\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.layer2(self.act(self.layer1(x)) * self.gate_proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eadaee4c-28f9-4c71-8d6d-e637490afd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.attn = MistralAttention(config)\n",
    "        self.mlp = MistralMLP(config)\n",
    "        self.norm1 = RMSNorm(config.d_model,eps=config.eps)\n",
    "        self.norm2 = RMSNorm(config.d_model,eps = config.eps)\n",
    "\n",
    "    def forward(self,x,freqs_complex):\n",
    "        print(f\"x shape before attention: {x.shape}\")\n",
    "        h = x + self.attn(self.norm1(x),freqs_complex)\n",
    "        out = h + self.mlp(self.norm2(h))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6705b53-e83b-4dac-ae0e-8748071bf57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "        def __init__(self,config):\n",
    "            super().__init__()\n",
    "            assert config.vocab_size != -1\n",
    "            self.device = config.device\n",
    "            self.norm = RMSNorm(config.d_model,eps = config.eps)\n",
    "            self.output = nn.Linear(config.d_model,config.vocab_size,bias=False)\n",
    "            self.freq_complex = precompute_freqs_cis(config.d_model,config.max_pos_embed)\n",
    "            self.embed = InputEmbedding(config.vocab_size,config.d_model)\n",
    "            self.n_layers = nn.ModuleList()\n",
    "            for layer in range(config.layers):\n",
    "                self.n_layers.append(MistralBlock(config))\n",
    "\n",
    "        def forward(self,x,start_pos=0):\n",
    "            batch_size,seq_len = x.shape\n",
    "            x = self.embed(x)\n",
    "            freq_complex = self.freq_complex[start_pos:start_pos+seq_len]\n",
    "            for layer in self.n_layers:\n",
    "                x = layer(x,freq_complex)\n",
    "            x = self.norm(x)\n",
    "            logits = self.output(x)\n",
    "            return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d838a94-6877-4b68-9964-bf0836309caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape before attention: torch.Size([16, 64, 128])\n",
      "torch.Size([16, 64, 4, 16])\n",
      "torch.Size([1, 1, 64, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (64) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(config)\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(config\u001b[38;5;241m.\u001b[39mvocab_size, (config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mseq_len), device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 5\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(dummy_input)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[35], line 19\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, start_pos)\u001b[0m\n\u001b[1;32m     17\u001b[0m freq_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_complex[start_pos:start_pos\u001b[38;5;241m+\u001b[39mseq_len]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers:\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x,freq_complex)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m     21\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m, in \u001b[0;36mMistralBlock.forward\u001b[0;34m(self, x, freqs_complex)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,freqs_complex):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx shape before attention: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     h \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x),freqs_complex)\n\u001b[1;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(h))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[47], line 39\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, x, freqs_complex, mask)\u001b[0m\n\u001b[1;32m     36\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     37\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_head, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m---> 39\u001b[0m q, _ \u001b[38;5;241m=\u001b[39m apply_rotary_embed(q, q, freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_complex)\n\u001b[1;32m     40\u001b[0m k, _ \u001b[38;5;241m=\u001b[39m apply_rotary_embed(k, k, freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_complex)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k[:, \u001b[38;5;241m1\u001b[39m:], k[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 13\u001b[0m, in \u001b[0;36mapply_rotary_embed\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(xq_complex\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(freq_cis\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 13\u001b[0m xq_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xq_complex \u001b[38;5;241m*\u001b[39m freq_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     14\u001b[0m xk_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(xk_complex \u001b[38;5;241m*\u001b[39m freq_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m xq_out \u001b[38;5;241m=\u001b[39m xq_out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mxq\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (64) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = MistralConfig()\n",
    "    model = Transformer(config).to(config.device)\n",
    "    dummy_input = torch.randint(config.vocab_size, (config.batch_size, config.seq_len), device=config.device)\n",
    "    logits = model(dummy_input)\n",
    "    print(f\"Logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34aa27d4-4ec3-40a7-ae41-80ea790a3195",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 142\u001b[0m\n\u001b[1;32m    140\u001b[0m model \u001b[38;5;241m=\u001b[39m Transformer(config)\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    141\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(config\u001b[38;5;241m.\u001b[39mvocab_size, (config\u001b[38;5;241m.\u001b[39mbatch_size, config\u001b[38;5;241m.\u001b[39mseq_len), device\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 142\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(dummy_input)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[51], line 133\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    130\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreqs_cis[:seq_len]\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 133\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, freqs_cis)\n\u001b[1;32m    135\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[51], line 110\u001b[0m, in \u001b[0;36mMistralBlock.forward\u001b[0;34m(self, x, freqs_cis)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, freqs_cis):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), freqs_cis)\n\u001b[1;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[51], line 84\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, x, freqs_cis)\u001b[0m\n\u001b[1;32m     81\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_k\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mkv_head, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     82\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_v\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_head \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mkv_head, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m keys\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m     85\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attn_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     87\u001b[0m output \u001b[38;5;241m=\u001b[39m (attn_weights \u001b[38;5;241m@\u001b[39m values)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(batch_size, seq_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class MistralConfig:\n",
    "    vocab_size: int = 1000\n",
    "    d_model: int = 128\n",
    "    d_ff: int = 1024\n",
    "    layers: int = 6\n",
    "    n_head: int = 4\n",
    "    kv_head: int = 2\n",
    "    max_pos_embed: int = 512\n",
    "    sliding_window: int = 256\n",
    "    hidden: str = \"silu\"\n",
    "    eps: float = 1e-6\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size: int = 16\n",
    "    seq_len: int = 64\n",
    "    \n",
    "    @property\n",
    "    def head_dim(self):\n",
    "        return self.d_model // self.n_head\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)\n",
    "        return self.weight * (x / rms)\n",
    "\n",
    "def precompute_freqs_cis(head_dim, max_position_embeddings, theta=10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
    "    t = torch.arange(max_position_embeddings)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    return torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "def apply_rotary_embed(x, freqs_cis):\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.reshape(1, x.shape[1], 1, -1)\n",
    "    x_rotated = torch.view_as_real(x_complex * freqs_cis).flatten(3)\n",
    "    return x_rotated.type_as(x)\n",
    "\n",
    "class MistralAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.q_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(config.d_model, config.kv_head * config.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.d_model, config.kv_head * config.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        \n",
    "        self.register_buffer(\"cache_k\", torch.zeros(\n",
    "            (config.batch_size, config.sliding_window, config.kv_head, config.head_dim),\n",
    "            device=config.device))\n",
    "        self.register_buffer(\"cache_v\", torch.zeros(\n",
    "            (config.batch_size, config.sliding_window, config.kv_head, config.head_dim),\n",
    "            device=config.device))\n",
    "\n",
    "    def forward(self, x, freqs_cis):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.config.n_head, self.config.head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.config.kv_head, self.config.head_dim)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.config.kv_head, self.config.head_dim)\n",
    "\n",
    "        q = apply_rotary_embed(q, freqs_cis)\n",
    "        k = apply_rotary_embed(k, freqs_cis)\n",
    "\n",
    "        self.cache_k = torch.roll(self.cache_k, shifts=-seq_len, dims=1)\n",
    "        self.cache_v = torch.roll(self.cache_v, shifts=-seq_len, dims=1)\n",
    "        \n",
    "        self.cache_k[:, -seq_len:] = k\n",
    "        self.cache_v[:, -seq_len:] = v\n",
    "        \n",
    "        keys = self.cache_k.repeat_interleave(self.config.n_head // self.config.kv_head, dim=2)\n",
    "        values = self.cache_v.repeat_interleave(self.config.n_head // self.config.kv_head, dim=2)\n",
    "\n",
    "        attn_weights = (q @ keys.transpose(-2, -1)) / math.sqrt(self.config.head_dim)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        \n",
    "        output = (attn_weights @ values).transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
    "        return self.o_proj(output)\n",
    "\n",
    "class MistralMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * config.d_ff / 3)\n",
    "        self.gate_proj = nn.Linear(config.d_model, hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.d_model, hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, config.d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "class MistralBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = MistralAttention(config)\n",
    "        self.mlp = MistralMLP(config)\n",
    "        self.norm1 = RMSNorm(config.d_model, config.eps)\n",
    "        self.norm2 = RMSNorm(config.d_model, config.eps)\n",
    "\n",
    "    def forward(self, x, freqs_cis):\n",
    "        x = x + self.attn(self.norm1(x), freqs_cis)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.layers = nn.ModuleList([MistralBlock(config) for _ in range(config.layers)])\n",
    "        self.norm = RMSNorm(config.d_model, config.eps)\n",
    "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            config.head_dim, \n",
    "            config.max_pos_embed\n",
    "        ).to(config.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        x = self.embed(x)\n",
    "        freqs_cis = self.freqs_cis[:seq_len]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freqs_cis)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        return self.output(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = MistralConfig()\n",
    "    model = Transformer(config).to(config.device)\n",
    "    dummy_input = torch.randint(config.vocab_size, (config.batch_size, config.seq_len), device=config.device)\n",
    "    logits = model(dummy_input)\n",
    "    print(f\"Logits shape: {logits.shape}\")  # Should output: Logits shape: torch.Size([16, 64, 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded5b0f-3df1-4df4-961f-f2bea973bd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
