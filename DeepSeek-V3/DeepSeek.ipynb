{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53fe9fc-9cab-454f-97da-5cba721c48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b03797-a228-4050-82e5-3d9297f66254",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Modelconfig():\n",
    "    d_model = 256\n",
    "    vocab_size = None\n",
    "    n_head = 4\n",
    "    d_compressed = 128\n",
    "    head_dim = d_model // n_head  # = 32\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    eps = 1e-6\n",
    "    batch_size = 16\n",
    "    d_ff = 512\n",
    "    n_shared = 2\n",
    "    n_routed = 2\n",
    "    top_k = 1\n",
    "    n_mtp_depth = 4\n",
    "    n_layers = 4\n",
    "    dropout = 0.1\n",
    "    lambda_mtp = 0.1\n",
    "    seq_len = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad91aaa-d2e8-4398-8c3c-74100ec7b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652e0e3a-a73a-4c0e-b528-ab984fbbf84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim, seq_len, theta=10000):\n",
    "    freqs = 1.0 / theta ** (torch.arange(0, dim, 2).float() / dim)\n",
    "    pos = torch.arange(seq_len)\n",
    "    angles = torch.outer(pos, freqs)  \n",
    "    return torch.polar(torch.ones_like(angles), angles)\n",
    "\n",
    "def apply_rotary_embed(x, freqs_cis, device):\n",
    "    batch, seq_len, d_model = x.shape\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(batch, seq_len, -1, 2))\n",
    "    \n",
    "    x_complex = x_complex.unsqueeze(2) \n",
    "    freqs_cis = freqs_cis[:seq_len]     \n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2)\n",
    "    x_rotated = x_complex * freqs_cis    \n",
    "    x_rotated = x_rotated.squeeze(2)    \n",
    "    x_out = torch.view_as_real(x_rotated)  \n",
    "    x_out = x_out.reshape(batch, seq_len, d_model)\n",
    "    return x_out.type_as(x).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58dd6677-207b-4cb2-923c-fd5e94b22f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = config.eps \n",
    "        self.weights = nn.Parameter(torch.ones(config.d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
    "        rms = torch.sqrt(mean + self.eps)\n",
    "        return (x / rms) * self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23011e1c-7c6b-44ea-bbee-95fc6eef8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model      \n",
    "        self.n_head = config.n_head           \n",
    "        self.d_compressed = config.d_compressed \n",
    "        self.head_dim = config.head_dim      \n",
    "\n",
    "        self.register_buffer(\"cache_k\", torch.zeros(0, 0, 0, 0))\n",
    "        self.register_buffer(\"cache_v\", torch.zeros(0, 0, 0, 0))\n",
    "        \n",
    "        self.q_c = nn.Linear(self.d_model, self.d_compressed, bias=False)\n",
    "        self.q_r = nn.Linear(self.d_compressed, self.d_model, bias=False)\n",
    "        self.w_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        self.q_u = nn.Linear(self.d_compressed, self.d_model, bias=False)\n",
    "\n",
    "        self.k_c = nn.Linear(self.d_model, self.d_compressed, bias=False)\n",
    "        self.k_u = nn.Linear(self.d_compressed, self.d_model, bias=False)\n",
    "        self.k_r = nn.Linear(self.d_compressed, self.d_model, bias=False)\n",
    "        self.w_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "\n",
    "        self.v_c = nn.Linear(self.d_model, self.d_compressed, bias=False)\n",
    "        self.v_u = nn.Linear(self.d_compressed, self.d_model, bias=False)\n",
    "        self.w_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        \n",
    "    def reset_cache(self):\n",
    "        self.cache_k = torch.zeros(0, 0, 0, 0, device=self.w_q.weight.device)\n",
    "        self.cache_v = torch.zeros(0, 0, 0, 0, device=self.w_q.weight.device)\n",
    "\n",
    "    def forward(self, x, freqs_cis, start_pos):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "        q = q.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q_flat = q.reshape(batch_size, seq_len, -1)\n",
    "        k_flat = k.reshape(batch_size, seq_len, -1)\n",
    "        v_flat = v.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "        compressed_q = self.q_c(q_flat)\n",
    "        up_q = self.q_u(compressed_q)\n",
    "        temp_q = self.q_r(compressed_q).view(batch_size, seq_len, self.n_head, self.head_dim)\n",
    "        rope_q = apply_rotary_embed(temp_q.reshape(batch_size, seq_len, -1), freqs_cis, device=x.device)\n",
    "        q_out = up_q + rope_q \n",
    "\n",
    "        compressed_k = self.k_c(k_flat)\n",
    "        up_k = self.k_u(compressed_k)\n",
    "        temp_k = self.k_r(compressed_k).view(batch_size, seq_len, self.n_head, self.head_dim)\n",
    "        rope_k = apply_rotary_embed(temp_k.reshape(batch_size, seq_len, -1), freqs_cis, device=x.device)\n",
    "        k_out = up_k + rope_k  \n",
    "        \n",
    "        compressed_v = self.v_c(v_flat)\n",
    "        v_out = self.v_u(compressed_v) \n",
    "\n",
    "        if self.cache_k.numel() == 0 or self.cache_k.shape[0] != batch_size:\n",
    "            self.cache_k = k_out.clone()\n",
    "            self.cache_v = v_out.clone()\n",
    "        else:\n",
    "            self.cache_k = torch.cat([self.cache_k, k_out], dim=1)\n",
    "            self.cache_v = torch.cat([self.cache_v, v_out], dim=1)\n",
    "        k_cat = self.cache_k[:, start_pos:start_pos+seq_len]\n",
    "        v_cat = self.cache_v[:, start_pos:start_pos+seq_len]\n",
    "        k_cat = k_cat.view(batch_size, seq_len, self.n_head, self.head_dim).detach()\n",
    "        v_cat = v_cat.view(batch_size, seq_len, self.n_head, self.head_dim).detach()\n",
    "\n",
    "        k_cat = k_cat.reshape(batch_size, seq_len, -1)\n",
    "        v_cat = v_cat.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "        scores = (q_out @ k_cat.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        if self.training or start_pos == 0:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len, device=scores.device), diagonal=1).bool()\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        o = attn_weights @ v_cat\n",
    "        o = self.w_o(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204e224e-c7bd-4ad9-a826-a4efa774fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.fc2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class DeepSeekMOE(nn.Module):\n",
    "    def __init__(self, config, gamma=0.01, alpha=1e-5):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_shared = config.n_shared\n",
    "        self.n_routed = config.n_routed\n",
    "        self.top_k = config.top_k\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.shared_experts = nn.ModuleList([Expert(config) for _ in range(config.n_shared)])\n",
    "        self.routed_experts = nn.ModuleList([Expert(config) for _ in range(config.n_routed)])\n",
    "        self.expert_centroids = nn.Parameter(torch.randn(config.n_routed, self.d_model))\n",
    "        self.register_buffer('bias', torch.zeros(config.n_routed))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        shared_output = sum(expert(x) for expert in self.shared_experts)\n",
    "        scores = torch.sigmoid(x @ self.expert_centroids.T)\n",
    "        adjusted_scores = scores + self.bias[None, None, :]\n",
    "        topk_scores, topk_indices = torch.topk(adjusted_scores, k=self.top_k, dim=-1)\n",
    "        mask = torch.zeros_like(scores).scatter_(-1, topk_indices, 1.0)\n",
    "        g_prime = scores * mask\n",
    "        g = g_prime / (g_prime.sum(dim=-1, keepdim=True) + 1e-6)\n",
    "        experts_output = torch.stack([expert(x) for expert in self.routed_experts], dim=2)\n",
    "        routed_output = (experts_output * g.unsqueeze(-1)).sum(dim=2)\n",
    "        output = x + shared_output + routed_output\n",
    "\n",
    "        if self.training:\n",
    "            expert_counts = mask.sum(dim=(0, 1))\n",
    "            total_tokens = batch_size * seq_len\n",
    "            expected = (total_tokens * self.top_k) / self.n_routed\n",
    "            delta = torch.where(expert_counts > expected, -self.gamma, self.gamma)\n",
    "            with torch.no_grad():\n",
    "                self.bias.add_(delta)\n",
    "            s_prime = F.softmax(scores, dim=-1)\n",
    "            p_i = s_prime.mean(dim=(0, 1))\n",
    "            f_i = (self.n_routed / (self.top_k * total_tokens)) * expert_counts\n",
    "            loss = self.alpha * torch.sum(f_i * p_i)\n",
    "            return output, loss\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "507267e7-0fe5-4eb6-bcd5-34d4afbdbd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.display = nn.Linear(self.d_model, config.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0161ff80-d66b-472e-b804-1a52c2d037d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_layers = config.n_layers\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        self.moe = DeepSeekMOE(config)\n",
    "        self.norm1 = RMSNorm(config)\n",
    "        self.norm2 = RMSNorm(config)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.dropout2 = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x, freqs_cis, start_pos):\n",
    "        attn_out = self.mha(x, freqs_cis, start_pos)\n",
    "        x = x + self.dropout1(attn_out)\n",
    "        x = x + self.norm1(x)\n",
    "        if self.training:\n",
    "            moe_out, moe_loss = self.moe(x)\n",
    "        else:\n",
    "            moe_out = self.moe(x)\n",
    "        x = x + self.dropout2(moe_out)\n",
    "        x = self.norm2(x)\n",
    "        if self.training:\n",
    "            return x, moe_loss\n",
    "        return x, torch.tensor(0.0, device=x.device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0517db4-1b7a-449e-83d7-6d5b9b010caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekV3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_mtp_depth = config.n_mtp_depth\n",
    "        self.lambda_mtp = config.lambda_mtp\n",
    "\n",
    "        self.embedding = InputEmbedding(config)\n",
    "        self.output_head = ProjectionLayer(config)\n",
    "\n",
    "        self.encoder_layer = nn.ModuleList([DeepSeekEncoderLayer(config) for _ in range(config.n_layers)])\n",
    "        self.mtp_projs = nn.ModuleList([nn.Linear(2 * self.d_model, self.d_model) for _ in range(self.n_mtp_depth)])\n",
    "        self.mtp_trms = nn.ModuleList([DeepSeekEncoderLayer(config) for _ in range(self.n_mtp_depth)])\n",
    "        self.rms_norm = nn.ModuleList([RMSNorm(config) for _ in range(self.n_mtp_depth)])\n",
    "\n",
    "    def forward(self, input_ids, freqs_cis, start_pos):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        original_ids = input_ids.clone() \n",
    "        x = self.embedding(input_ids)     \n",
    "        \n",
    "        balance_losses = []\n",
    "\n",
    "        for layer in self.encoder_layer:\n",
    "            if self.training:\n",
    "                x, balance_loss = layer(x, freqs_cis, start_pos)\n",
    "                balance_losses.append(balance_loss.detach())\n",
    "            else:\n",
    "                x, _ = layer(x, freqs_cis, start_pos)\n",
    "        \n",
    "        main_logits = self.output_head(x)   \n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                h_prev = x\n",
    "            mtp_losses = []\n",
    "            for depth in range(self.n_mtp_depth):\n",
    "                if h_prev.size(1) < depth + 2:\n",
    "                    break\n",
    "                future_emb = self.embedding(original_ids[:, depth+1:])\n",
    "                h_trimmed = h_prev[:, :-(depth+1), :]\n",
    "                h_norm = self.rms_norm[depth](h_trimmed)\n",
    "                f_norm = self.rms_norm[depth](future_emb)\n",
    "                combined = torch.cat([h_norm, f_norm], dim=-1)\n",
    "                projected = self.mtp_projs[depth](combined)\n",
    "                mtp_output, _ = self.mtp_trms[depth](projected, freqs_cis, start_pos)\n",
    "                mtp_logits = self.output_head(mtp_output)\n",
    "                targets = original_ids[:, depth+1:]\n",
    "                loss = F.cross_entropy(mtp_logits.view(-1, mtp_logits.size(-1)), \n",
    "                                       targets.reshape(-1))\n",
    "                mtp_losses.append(loss)\n",
    "            total_mtp_loss = self.lambda_mtp * sum(mtp_losses) + sum(balance_losses)\n",
    "        else:\n",
    "            total_mtp_loss = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        return main_logits, total_mtp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6ba2958-4e09-44ad-807d-f1baee517d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Training]: 100%|█| 13648/13648 [06:02<00:00, 37.68batch/s, loss=0.0049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Average Training Loss: 0.2994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Validation]: 100%|██████████████| 1517/1517 [01:02<00:00, 24.08batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Validation Loss: 0.0098\n",
      "\n",
      "Generated Text:\n",
      "bowsprit, bowsprit, bowsprit, o'er-run first. first. first. first. didst, graves, graves, executed benched images, Deposed Clare. Clare. Clare. read'st, read'st, His His His dream, guard, boy's prize prize prize prize prize Lucentio rapture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Training]: 100%|█| 13648/13648 [06:15<00:00, 36.38batch/s, loss=0.0012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2, Average Training Loss: 0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Validation]: 100%|██████████████| 1517/1517 [01:05<00:00, 23.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Validation Loss: 0.0066\n",
      "\n",
      "Generated Text:\n",
      "vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow vow linen linen linen linen linen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Training]: 100%|█| 13648/13648 [06:10<00:00, 36.88batch/s, loss=0.0023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3, Average Training Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Validation]: 100%|██████████████| 1517/1517 [01:06<00:00, 22.97batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Validation Loss: 0.0068\n",
      "\n",
      "Generated Text:\n",
      "Bohemia: Bohemia: Bohemia: Bohemia: womb, womb, womb, womb, womb, womb, womb, what? what? what? what? what? what? what? what? what? what? what? what? what? what? what? what? what? what? what? Ravenspurgh; Ravenspurgh; motion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Training]: 100%|█| 13648/13648 [06:16<00:00, 36.29batch/s, loss=0.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4, Average Training Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Validation]: 100%|██████████████| 1517/1517 [01:05<00:00, 23.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Validation Loss: 0.0069\n",
      "\n",
      "Generated Text:\n",
      "Whiles Whiles Whiles Whiles Whiles Whiles join'd. join'd. god, god, god, god, princess,--goddess!--O, usurers; usurers; usurers; delicates, delicates, kept! sighs; sighs; constable? constable? constable? constable? desert! desert! desert! desert! shrieks 'cum 'cum 'cum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Training]: 100%|█| 13648/13648 [06:18<00:00, 36.07batch/s, loss=0.0008]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5, Average Training Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Validation]: 100%|██████████████| 1517/1517 [01:08<00:00, 22.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Validation Loss: 0.0073\n",
      "\n",
      "Generated Text:\n",
      "French French French French earnestly grief: grief: allies allies allies allies what what what what what what what what what what what what what what what what what what what what what what\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Training]:   9%|▏ | 1239/13648 [00:35<05:55, 34.88batch/s, loss=0.0038]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 178\u001b[0m\n\u001b[1;32m    176\u001b[0m config \u001b[38;5;241m=\u001b[39m Modelconfig()\n\u001b[1;32m    177\u001b[0m config\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab)\n\u001b[0;32m--> 178\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[11], line 137\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m loss_main \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(main_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, config\u001b[38;5;241m.\u001b[39mvocab_size),\n\u001b[1;32m    135\u001b[0m                             batch\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    136\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_main \u001b[38;5;241m+\u001b[39m total_mtp_loss\n\u001b[0;32m--> 137\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    138\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    139\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "DATA_PATH = \"tinyshakespeare.txt\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    print(\"Downloading Tiny Shakespeare...\")\n",
    "    r = requests.get(DATA_URL)\n",
    "    with open(DATA_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(r.text)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "text = \" <eos> \".join(lines)\n",
    "tokens = text.split() \n",
    "text = \" \".join(tokens)\n",
    "\n",
    "special_tokens = [\"<eos>\", \"<pos>\"]\n",
    "\n",
    "text_tokens = text.split()\n",
    "vocab = special_tokens + sorted(list(set(text_tokens) - set(special_tokens)))\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "stoi = {token: i for i, token in enumerate(vocab)}\n",
    "itos = {i: token for i, token in enumerate(vocab)}\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, text, block_size, stoi):\n",
    "        self.data = [stoi[token] for token in text.split() if token in stoi]\n",
    "        self.block_size = block_size\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx:idx+self.block_size], dtype=torch.long)\n",
    "\n",
    "def generate_text(model, start_text, length, temperature=1.0, device='cuda'):\n",
    "    model.eval()\n",
    "    for layer in model.encoder_layer:\n",
    "        layer.mha.reset_cache()\n",
    "    \n",
    "    context = torch.tensor([stoi[token] for token in start_text.split() if token in stoi],\n",
    "                           dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            if context.size(1) > config.seq_len:\n",
    "                context = context[:, -config.seq_len:]\n",
    "            seq_len = context.size(1)\n",
    "            freqs_cis = precompute_freqs_cis(config.d_model, seq_len).to(device)\n",
    "            logits, _ = model(context, freqs_cis, start_pos=0)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "    \n",
    "    return \" \".join([itos[int(idx)] for idx in context.squeeze().tolist()])\n",
    "\n",
    "def main():\n",
    "    block_size = config.seq_len \n",
    "    batch_size = config.batch_size\n",
    "    dataset = ShakespeareDataset(text, block_size, stoi)\n",
    "    \n",
    "    dataset_len = len(dataset)\n",
    "    val_len = int(0.1 * dataset_len)\n",
    "    train_len = dataset_len - val_len\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_len, val_len])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model = DeepSeekV3(config)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
    "    num_epochs = 10\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for layer in model.encoder_layer:\n",
    "            layer.mha.reset_cache()\n",
    "        total_train_loss = 0.0\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", unit=\"batch\")\n",
    "        for i, batch in enumerate(train_bar):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            seq_len = config.seq_len\n",
    "            freqs_cis = precompute_freqs_cis(config.d_model, seq_len).to(device)\n",
    "            start_pos = 0\n",
    "            main_logits, total_mtp_loss = model(batch, freqs_cis, start_pos)\n",
    "            loss_main = F.cross_entropy(main_logits.view(-1, config.vocab_size),\n",
    "                                        batch.view(-1))\n",
    "            loss = loss_main + total_mtp_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "            if (i + 1) % 50 == 0:\n",
    "                sample = generate_text(model, \"Care for\", 50, temperature=1.5, device=device)\n",
    "                train_bar.set_postfix(loss=f\"{loss.item():.4f}\", sample=sample[:50] + \"...\")\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"\\nEpoch {epoch+1}, Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\", unit=\"batch\"):\n",
    "                batch = batch.to(device)\n",
    "                seq_len = config.seq_len\n",
    "                freqs_cis = precompute_freqs_cis(config.d_model, seq_len).to(device)\n",
    "                start_pos = 0\n",
    "                main_logits, total_mtp_loss = model(batch, freqs_cis, start_pos)\n",
    "                loss_main = F.cross_entropy(main_logits.view(-1, config.vocab_size),\n",
    "                                            batch.view(-1))\n",
    "                loss = loss_main + total_mtp_loss\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        generated_text = generate_text(model, \"Care for\", 300, temperature=1.5, device=device)\n",
    "        print(\"\\nGenerated Text:\")\n",
    "        print(generated_text)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Modelconfig()\n",
    "    config.vocab_size = len(vocab)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f2617-3070-44ac-8839-3472aa470338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
