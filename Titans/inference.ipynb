{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c88a755a-5425-47f7-9821-6a425409c1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building vocabulary...\n",
      "Loading checkpoint from titan_checkpoint-3.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15687/1430301575.py:154: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TitanMAGLM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([26000, 256]) from checkpoint, the shape in current model is torch.Size([11674, 256]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([26000, 256]) from checkpoint, the shape in current model is torch.Size([11674, 256]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([26000]) from checkpoint, the shape in current model is torch.Size([11674]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 155\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    156\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# 5. Chatbot Inference Function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TitanMAGLM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([26000, 256]) from checkpoint, the shape in current model is torch.Size([11674, 256]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([26000, 256]) from checkpoint, the shape in current model is torch.Size([11674, 256]).\n\tsize mismatch for lm_head.bias: copying a param with shape torch.Size([26000]) from checkpoint, the shape in current model is torch.Size([11674])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Configuration Class (Same as Used in Training)\n",
    "# -------------------------------\n",
    "@dataclass\n",
    "class TitanConfig:\n",
    "    d_model: int = 256\n",
    "    vocab_size: int = 10000  # Update after building vocab\n",
    "    seq_len: int = 128\n",
    "    n_heads: int = 8\n",
    "    alpha: float = 0.1\n",
    "    eta: float = 0.9\n",
    "    theta: float = 0.01\n",
    "    window_size: int = 256\n",
    "    batch_size: int = 32\n",
    "    n_layers: int = 8\n",
    "    N_p: int = 128\n",
    "    bos_token_id: int = 2\n",
    "    eos_token_id: int = 3\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load WikiText-2 Dataset and Build Vocabulary\n",
    "# -------------------------------\n",
    "def simple_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Load a subset of WikiText dataset\n",
    "print(\"Loading dataset...\")\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Reduce dataset size for faster loading\n",
    "N = 5000\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    wikitext[split] = wikitext[split].select(range(min(N, len(wikitext[split]))))\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "counter = Counter()\n",
    "for line in wikitext[\"train\"][\"text\"]:\n",
    "    if line.strip():\n",
    "        counter.update(simple_tokenizer(line))\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "vocab = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "\n",
    "# Add words appearing at least twice\n",
    "min_freq = 2\n",
    "for token, freq in counter.items():\n",
    "    if freq >= min_freq and token not in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "itos = {idx: token for token, idx in vocab.items()}  # id -> token mapping\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Define Model Components\n",
    "# -------------------------------\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p, config.d_model))\n",
    "    \n",
    "    def forward(self, batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\", torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\", torch.zeros(config.d_model, config.d_model))\n",
    "        self.query = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        return torch.matmul(q, self.M)\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.d_model, num_heads=config.n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]\n",
    "\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "        self.layernorm = nn.LayerNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens, x], dim=1)\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        memory_retrieval = self.long_memory(out)\n",
    "        combined = self.layernorm(out) * self.layernorm(memory_retrieval)\n",
    "        return combined[:, -seq_len:, :]\n",
    "\n",
    "class TitanMAGLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(config.seq_len, config.d_model))\n",
    "        self.titan = TitanMAG(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x) + self.pos_embedding[: x.size(1), :].unsqueeze(0)\n",
    "        return self.lm_head(self.titan(emb))\n",
    "\n",
    "    def generate(self, prompt, max_length=100, k=10):\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                input_ids = torch.tensor([generated[-self.config.seq_len:]], dtype=torch.long).to(next(self.parameters()).device)\n",
    "                logits = self.forward(input_ids)[0, -1, :]\n",
    "                topk_logits, topk_indices = torch.topk(logits, k)\n",
    "                probs = F.softmax(topk_logits, dim=-1)\n",
    "                next_token = topk_indices[torch.multinomial(probs, num_samples=1)].item()\n",
    "                generated.append(next_token)\n",
    "                if next_token == self.config.eos_token_id:\n",
    "                    break\n",
    "        return generated\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Load the Model and Checkpoint\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = TitanConfig(vocab_size=vocab_size)\n",
    "config.bos_token_id = vocab[\"<bos>\"]\n",
    "config.eos_token_id = vocab[\"<eos>\"]\n",
    "\n",
    "model = TitanMAGLM(config).to(device)\n",
    "checkpoint_path = \"titan_checkpoint-3.pth\"\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Chatbot Inference Function\n",
    "# -------------------------------\n",
    "def generate_text(prompt, model, vocab, itos, max_length=100, k=10):\n",
    "    tokens = simple_tokenizer(prompt)\n",
    "    prompt_ids = [vocab[\"<bos>\"]] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    generated_ids = model.generate(prompt_ids, max_length=max_length, k=k)\n",
    "    return \" \".join([itos.get(i, \"<unk>\") for i in generated_ids])\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Interactive Chatbot Loop\n",
    "# -------------------------------\n",
    "print(\"Chatbot is ready! Type 'exit' or 'quit' to stop.\")\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = generate_text(user_input, model, vocab, itos, max_length=100, k=10)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed7cdb-257b-4254-98ee-ddf324e359aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
