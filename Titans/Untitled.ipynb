{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee492658-8c2d-47d3-bbf8-a6c1b30da796",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 507\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    506\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50000\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m))  \u001b[38;5;66;03m# Example input\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 481\u001b[0m, in \u001b[0;36mTitansModel.forward\u001b[0;34m(self, input_ids, positions)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Process through Titans layers\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 481\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Final normalization\u001b[39;00m\n\u001b[1;32m    484\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 372\u001b[0m, in \u001b[0;36mTitansMAC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    370\u001b[0m     step_k \u001b[38;5;241m=\u001b[39m k[:, j, :]\n\u001b[1;32m    371\u001b[0m     step_v \u001b[38;5;241m=\u001b[39m v[:, j, :]\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_term_memory(\n\u001b[1;32m    373\u001b[0m         query\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    374\u001b[0m         key\u001b[38;5;241m=\u001b[39mstep_k, \n\u001b[1;32m    375\u001b[0m         value\u001b[38;5;241m=\u001b[39mstep_v, \n\u001b[1;32m    376\u001b[0m         update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     )\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Retrieve updated memory for final output\u001b[39;00m\n\u001b[1;32m    380\u001b[0m final_memory_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_term_memory(segment_attn_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 182\u001b[0m, in \u001b[0;36mNeuralLongTermMemory.forward\u001b[0;34m(self, query, key, value, update)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_memory(key, value)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Retrieve from memory\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_memory(query)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[2], line 128\u001b[0m, in \u001b[0;36mNeuralLongTermMemory.forward_memory\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m key\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 128\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    129\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norms[i](x)\n\u001b[1;32m    130\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, List, Union\n",
    "\n",
    "\n",
    "class DepthwiseSeparableConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise separable 1D convolution as used in the Titans architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, kernel_size: int = 3, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv1d(\n",
    "            in_channels=in_dim,\n",
    "            out_channels=in_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            groups=in_dim,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.pointwise = nn.Conv1d(\n",
    "            in_channels=in_dim,\n",
    "            out_channels=in_dim,\n",
    "            kernel_size=1,\n",
    "            bias=bias\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input shape: [batch_size, seq_len, dim]\n",
    "        # Required shape for conv1d: [batch_size, dim, seq_len]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        # Return to original shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NeuralLongTermMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Long-Term Memory module as described in Section 3.1.\n",
    "    This module learns to memorize at test time using an MLP architecture.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        memory_depth: int = 2,\n",
    "        hidden_dim: int = None,\n",
    "        activation: nn.Module = nn.SiLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim * 4\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.memory_depth = memory_depth\n",
    "        \n",
    "        # Create MLP layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer (input -> hidden)\n",
    "        self.layers.append(nn.Linear(dim, hidden_dim))\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_dim)])\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(memory_depth - 2):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Output layer (hidden -> output)\n",
    "        self.layers.append(nn.Linear(hidden_dim, dim))\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        # Parameter networks for alpha, theta, and eta (forgetting and surprise mechanisms)\n",
    "        self.alpha_net = nn.Sequential(nn.Linear(dim, 1), nn.Sigmoid())\n",
    "        self.theta_net = nn.Sequential(nn.Linear(dim, 1), nn.Sigmoid())\n",
    "        self.eta_net = nn.Sequential(nn.Linear(dim, 1), nn.Sigmoid())\n",
    "        \n",
    "        # Initialize past surprise\n",
    "        self.past_surprise = None\n",
    "    \n",
    "    def reset_state(self, batch_size: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"Reset the memory state.\"\"\"\n",
    "        self.past_surprise = None\n",
    "        \n",
    "    def compute_gradient(self, mem_weights: List[torch.Tensor], key: torch.Tensor, \n",
    "                          value: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss with respect to the memory weights.\n",
    "        Loss = ||M(key) - value||^2\n",
    "        \"\"\"\n",
    "        # Forward pass through the memory\n",
    "        with torch.enable_grad():\n",
    "            # Create copies of weights that track gradients\n",
    "            weight_copies = [w.detach().clone().requires_grad_(True) for w in mem_weights]\n",
    "            \n",
    "            # Run forward pass\n",
    "            x = key\n",
    "            for i, layer in enumerate(self.layers[:-1]):\n",
    "                # Replace weights with our copies\n",
    "                weight = weight_copies[i]\n",
    "                bias = layer.bias if layer.bias is not None else None\n",
    "                \n",
    "                x = F.linear(x, weight, bias)\n",
    "                x = self.layer_norms[i](x)\n",
    "                x = self.activation(x)\n",
    "            \n",
    "            # Output layer\n",
    "            weight = weight_copies[-1]\n",
    "            bias = self.layers[-1].bias if self.layers[-1].bias is not None else None\n",
    "            output = F.linear(x, weight, bias)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(output, value, reduction='sum')\n",
    "            \n",
    "            # Compute gradients\n",
    "            grads = torch.autograd.grad(loss, weight_copies)\n",
    "            \n",
    "        return grads\n",
    "\n",
    "    def forward_memory(self, key: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the memory without updating weights.\"\"\"\n",
    "        x = key\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            x = self.layer_norms[i](x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        output = self.layers[-1](x)\n",
    "        return output\n",
    "    \n",
    "    def update_memory(self, key: torch.Tensor, value: torch.Tensor) -> None:\n",
    "        \"\"\"Update the memory weights based on the current input.\"\"\"\n",
    "        # Extract memory weights\n",
    "        mem_weights = []\n",
    "        for layer in self.layers:\n",
    "            mem_weights.append(layer.weight)\n",
    "        \n",
    "        # Compute adaptive parameters\n",
    "        alpha = self.alpha_net(key)  # Forgetting rate\n",
    "        theta = self.theta_net(key)  # Learning rate\n",
    "        eta = self.eta_net(key)      # Past surprise decay\n",
    "        \n",
    "        # Compute gradients (surprise)\n",
    "        momentary_surprise = self.compute_gradient(mem_weights, key, value)\n",
    "        \n",
    "        # Update past surprise or initialize it\n",
    "        if self.past_surprise is None:\n",
    "            self.past_surprise = [torch.zeros_like(grad) for grad in momentary_surprise]\n",
    "        \n",
    "        # Update memory weights using Equation 13 and 14 from the paper\n",
    "        for i, (weight, grad) in enumerate(zip(mem_weights, momentary_surprise)):\n",
    "            # Update past surprise with decay (Equation 14)\n",
    "            self.past_surprise[i] = eta * self.past_surprise[i] - theta * grad\n",
    "            \n",
    "            # Update memory weights (Equation 13)\n",
    "            new_weight = (1 - alpha) * weight + self.past_surprise[i]\n",
    "            mem_weights[i].data.copy_(new_weight.data)\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor = None, value: torch.Tensor = None, \n",
    "                update: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the memory module.\n",
    "        \n",
    "        Args:\n",
    "            query: Input tensor to retrieve memory\n",
    "            key: Key tensor for updating memory (optional)\n",
    "            value: Value tensor for updating memory (optional)\n",
    "            update: Whether to update the memory weights\n",
    "            \n",
    "        Returns:\n",
    "            Memory output for the given query\n",
    "        \"\"\"\n",
    "        # If key and value are provided and update is True, update the memory\n",
    "        if key is not None and value is not None and update:\n",
    "            self.update_memory(key, value)\n",
    "        \n",
    "        # Retrieve from memory\n",
    "        output = self.forward_memory(query)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TitansAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal self-attention module for the Titans model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.0,\n",
    "        conv_kernel_size: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        \n",
    "        # Projection matrices\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Convolutional layers for Q, K, V\n",
    "        self.q_conv = DepthwiseSeparableConv1d(dim, kernel_size=conv_kernel_size)\n",
    "        self.k_conv = DepthwiseSeparableConv1d(dim, kernel_size=conv_kernel_size)\n",
    "        self.v_conv = DepthwiseSeparableConv1d(dim, kernel_size=conv_kernel_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Projection with convolutional components\n",
    "        q = self.q_proj(x)\n",
    "        q = self.q_conv(q)\n",
    "        \n",
    "        k = self.k_proj(x)\n",
    "        k = self.k_conv(k)\n",
    "        \n",
    "        v = self.v_proj(x)\n",
    "        v = self.v_conv(v)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scale queries\n",
    "        q = q / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "        \n",
    "        # Apply causal mask if none is provided\n",
    "        if mask is None:\n",
    "            # Create causal mask (lower triangular)\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool, device=x.device))\n",
    "            # Convert to boolean mask with appropriate dimensions\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
    "        \n",
    "        # Apply mask\n",
    "        attn_scores = attn_scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.out_proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TitansMAC(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory as a Context (MAC) architecture for Titans model, as described in Section 4.1.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int = 8,\n",
    "        memory_depth: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "        num_persistent_tokens: int = 8,\n",
    "        segment_size: int = 512,\n",
    "        conv_kernel_size: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.segment_size = segment_size\n",
    "        self.num_persistent_tokens = num_persistent_tokens\n",
    "        \n",
    "        # Persistent memory (learnable tokens)\n",
    "        self.persistent_memory = nn.Parameter(torch.randn(num_persistent_tokens, dim))\n",
    "        \n",
    "        # Neural long-term memory\n",
    "        self.long_term_memory = NeuralLongTermMemory(dim, memory_depth=memory_depth)\n",
    "        \n",
    "        # Attention module\n",
    "        self.attention = TitansAttention(\n",
    "            dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            conv_kernel_size=conv_kernel_size,\n",
    "        )\n",
    "        \n",
    "        # Projection matrices\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4 * dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "        # Gating mechanism for combining memory outputs\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reset_memory(self, batch_size: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"Reset the memory state.\"\"\"\n",
    "        self.long_term_memory.reset_state(batch_size, device)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Process in segments\n",
    "        outputs = []\n",
    "        \n",
    "        # Expand persistent memory for each batch element\n",
    "        persistent_tokens = self.persistent_memory.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Split input into segments\n",
    "        for i in range(0, seq_len, self.segment_size):\n",
    "            segment = x[:, i:i+self.segment_size, :]\n",
    "            \n",
    "            if segment.size(1) == 0:\n",
    "                break\n",
    "                \n",
    "            # Project segment for query, key, and value\n",
    "            q = self.q_proj(segment)\n",
    "            k = self.k_proj(segment)\n",
    "            v = self.v_proj(segment)\n",
    "            \n",
    "            # Retrieve from long-term memory\n",
    "            memory_output = self.long_term_memory(q)\n",
    "            \n",
    "            # Concatenate persistent memory, memory output, and current segment\n",
    "            context = torch.cat([\n",
    "                persistent_tokens,\n",
    "                memory_output,\n",
    "                segment\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Apply attention to the full context\n",
    "            attn_output = self.attention(context)\n",
    "            \n",
    "            # Extract only the segment portion\n",
    "            segment_attn_output = attn_output[:, self.num_persistent_tokens + memory_output.size(1):, :]\n",
    "            \n",
    "            # Update long-term memory with the attention output\n",
    "            for j in range(segment.size(1)):\n",
    "                step_k = k[:, j, :]\n",
    "                step_v = v[:, j, :]\n",
    "                self.long_term_memory(\n",
    "                    query=None,\n",
    "                    key=step_k, \n",
    "                    value=step_v, \n",
    "                    update=True\n",
    "                )\n",
    "            \n",
    "            # Retrieve updated memory for final output\n",
    "            final_memory_output = self.long_term_memory(segment_attn_output)\n",
    "            \n",
    "            # Combine outputs with gating mechanism\n",
    "            combined_features = torch.cat([segment_attn_output, final_memory_output], dim=-1)\n",
    "            gate_values = self.gate(combined_features)\n",
    "            \n",
    "            # Apply gate and residual connection\n",
    "            output = segment + segment_attn_output * gate_values + final_memory_output * (1 - gate_values)\n",
    "            \n",
    "            # Layer norm and feed-forward\n",
    "            norm_output = self.layer_norm1(output)\n",
    "            ffn_output = self.ffn(norm_output)\n",
    "            \n",
    "            # Final output with residual\n",
    "            final_output = output + ffn_output\n",
    "            final_output = self.layer_norm2(final_output)\n",
    "            \n",
    "            outputs.append(final_output)\n",
    "        \n",
    "        # Concatenate segment outputs\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class TitansModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Titans model with configurable number of layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_layers: int = 6,\n",
    "        num_heads: int = 8,\n",
    "        memory_depth: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "        num_persistent_tokens: int = 8,\n",
    "        segment_size: int = 512,\n",
    "        conv_kernel_size: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Token embedding\n",
    "        self.token_embedding = nn.Embedding(50304, dim)  # Default vocab size for compatibility\n",
    "        \n",
    "        # Position embedding (if needed)\n",
    "        self.pos_embedding = nn.Embedding(8192, dim)  # Large enough for most contexts\n",
    "        \n",
    "        # Titans layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            TitansMAC(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                memory_depth=memory_depth,\n",
    "                dropout=dropout,\n",
    "                num_persistent_tokens=num_persistent_tokens,\n",
    "                segment_size=segment_size,\n",
    "                conv_kernel_size=conv_kernel_size,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(dim, 50304)  # Match vocab size\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def reset_memories(self, batch_size: int = 1, device: torch.device = torch.device('cpu')):\n",
    "        \"\"\"Reset all memory states.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.reset_memory(batch_size, device)\n",
    "            \n",
    "    def forward(self, input_ids: torch.Tensor, positions: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the complete model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Tensor of token ids [batch_size, seq_len]\n",
    "            positions: Optional position ids [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            Logits for next token prediction [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        device = input_ids.device\n",
    "        \n",
    "        # Get token embeddings\n",
    "        x = self.token_embedding(input_ids)\n",
    "        \n",
    "        # Add position embeddings if provided\n",
    "        if positions is None:\n",
    "            positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        x = x + self.pos_embedding(positions)\n",
    "        \n",
    "        # Process through Titans layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Final normalization\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "# Example usage\n",
    "model = TitansModel(\n",
    "    dim=512,               # Embedding dimension\n",
    "    num_layers=6,          # Number of Titans layers\n",
    "    num_heads=8,           # Number of attention heads\n",
    "    memory_depth=2,        # Depth of the memory MLP\n",
    "    dropout=0.1,\n",
    "    num_persistent_tokens=8,\n",
    "    segment_size=512       # Size of segments for processing\n",
    ")\n",
    "\n",
    "# Reset memory states at the start of a new sequence\n",
    "model.reset_memories(batch_size=1)\n",
    "\n",
    "# Forward pass\n",
    "input_ids = torch.randint(0, 50000, (1, 1024))  # Example input\n",
    "logits = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37153b74-8c33-48c5-851b-e5d2a267008d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
