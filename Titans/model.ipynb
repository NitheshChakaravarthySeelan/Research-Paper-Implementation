{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7dbd1b9-e92c-4a05-b95e-7c471874dbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9cde302-ccda-4cf2-a2ce-7e0dee6ac6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(pred,target):\n",
    "    return torch.sum((pred - target) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c25033-10ea-4aac-8c2a-96d15bcee4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TitanConfig():\n",
    "    d_model: int = 128\n",
    "    vocab_size: int = 10000   # will update later\n",
    "    seq_len: int = 32\n",
    "    n_heads: int = 4\n",
    "    alpha: float = 0.1\n",
    "    eta: float = 0.9\n",
    "    theta: float = 0.01\n",
    "    window_size: int = 128\n",
    "    batch_size: int = 8\n",
    "    n_layers: int = 2\n",
    "    chunk_size: int = 64    # for MAC variant (not used in MAG)\n",
    "    N_p: int = 10\n",
    "    bos_token_id: int = 2\n",
    "    eos_token_id: int = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691521b4-4182-43e8-acbe-3be9d17dcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\",torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\",torch.zeros(config.d_model,config.d_model))\n",
    "\n",
    "        self.query = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.key = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "        self.value = nn.Linear(config.d_model,config.d_model,bias=False)\n",
    "\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self,x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q,self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self,x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        v_pred = torch.matmul(k,self.M)\n",
    "        loss = l2_loss(v_pred,v)\n",
    "        error = v_pred - v\n",
    "\n",
    "        g = 2 * torch.matmul(error.t(),k)\n",
    "\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1-self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9de44c6-e0fd-4fd6-8883-6b6e2022417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim = config.d_model, num_heads = config.n_heads, batch_first= True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,_ = x.size()\n",
    "        output = []\n",
    "\n",
    "        for i in range(0,seq_len,self.window_size):\n",
    "            x_chunk = x[:,i:i+self.window_size,:]\n",
    "            attn_out,_ = self.attention(x_chunk,x_chunk,x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output,dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "032570ae-7bab-459f-b470-74fe163030b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p,config.d_model))\n",
    "\n",
    "    def forward(self,batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size,-1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40fef564-e9f3-4a0f-8763-b8c4fc797388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class TitanMAC(nn.Module):\\n    def __init__(self,config):\\n        super().__init__()\\n        self.d_model = config.d_model\\n        self.chunk_size = config.chunk_size\\n        self.persistent = PersistentMemory(config)\\n        self.long_memory = TitanMemory(config)\\n        self.attn_layer = nn.ModuleList([\\n        nn.TransformerEncoderLayer(d_model=config.d_model,nhead=config.n_heads,batch_first=True) for _ in range(config.n_layers) ])\\n        \\n    def forward(self,x):\\n        batch_size,seq_len,_ = x.size()\\n        output = []\\n\\n        for i in range(0,seq_len,self.chunk_size):\\n            x_chunk = x[:,i:i+self.chunk_size,:]\\n            x_flat = x_chunk.reshape(-1,self.d_model)\\n            out = self.long_memory(x_flat)\\n            out = out.reshape(batch_size,-1,self.d_model)\\n            persistent = self.persistent(batch_size)\\n\\n            cat_input = torch.cat([persistent,out,x_chunk],dim=1)\\n\\n            for layer in self.attn_layer:\\n                cat_input = layer(cat_input)\\n            out_chunk = cat_input[:,-x_chunk.size(1):,:]\\n            self.long_memory.update_memory(out_chunk.reshape(-1,self.d_model))\\n            retrived = self.long_memory(out_chunk.reshape(-1,self.d_model))\\n            retrived = retrieved.reshape(out_chunk.shape)\\n            out = out_chunk * retrieved\\n            output.append(out)\\n            \\n        output = torch.cat(output,dim=1)\\n        return output'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class TitanMAC(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.chunk_size = config.chunk_size\n",
    "        self.persistent = PersistentMemory(config)\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layer = nn.ModuleList([\n",
    "        nn.TransformerEncoderLayer(d_model=config.d_model,nhead=config.n_heads,batch_first=True) for _ in range(config.n_layers) ])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,_ = x.size()\n",
    "        output = []\n",
    "\n",
    "        for i in range(0,seq_len,self.chunk_size):\n",
    "            x_chunk = x[:,i:i+self.chunk_size,:]\n",
    "            x_flat = x_chunk.reshape(-1,self.d_model)\n",
    "            out = self.long_memory(x_flat)\n",
    "            out = out.reshape(batch_size,-1,self.d_model)\n",
    "            persistent = self.persistent(batch_size)\n",
    "\n",
    "            cat_input = torch.cat([persistent,out,x_chunk],dim=1)\n",
    "\n",
    "            for layer in self.attn_layer:\n",
    "                cat_input = layer(cat_input)\n",
    "            out_chunk = cat_input[:,-x_chunk.size(1):,:]\n",
    "            self.long_memory.update_memory(out_chunk.reshape(-1,self.d_model))\n",
    "            retrived = self.long_memory(out_chunk.reshape(-1,self.d_model))\n",
    "            retrived = retrieved.reshape(out_chunk.shape)\n",
    "            out = out_chunk * retrieved\n",
    "            output.append(out)\n",
    "            \n",
    "        output = torch.cat(output,dim=1)\n",
    "        return output'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7935bd-2f5a-4cca-8d8d-cec8ede97613",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,d_model = x.size()\n",
    "\n",
    "        x_flat = x.reshape(-1,d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        \n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens,x],dim=1)\n",
    "\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        out_flat = out.reshape(-1,self.d_model)\n",
    "        long_term = self.long_memory(out_flat)\n",
    "        long_term = long_term.reshape(batch_size,-1,d_model)\n",
    "\n",
    "        output = y * long_term\n",
    "        output = output[:,-seq_len:,:]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd72a31-b80f-4a7e-99e4-ead2388c89f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class TitanMAL(nn.Module):\\n    def __init__(self,config):\\n        self.d_model = config.d_model\\n        self.long_memory = TitanMemory(config)\\n        self.short_memory = SlidingWindowAttention(config)\\n        self.n_layer = config.n_layer\\n        self.percistent = PersistentMemory(config)\\n        self.attn_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=config.d_model,nhead=config.n_heads,batch_first=True) for _ in range(config.n_layer)])\\n\\n    def forward(self,x):\\n        batch_size,seq_len,d_model = x.size()\\n        persistent_tokens = self.persistent(batch_size)\\n        out = torch.cat([persistent_tokens,x],dim=1)\\n        \\n        y = self.long_memory(out)\\n        o = self.short_memory(y)\\n        o = o[:,-seq_len:,:]\\n\\n        x = x + out\\n        x = self.short_memory(x)\\n\\n        for layer in self.attn_layers:\\n            x = layer(x)\\n        return x'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class TitanMAL(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.d_model = config.d_model\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.short_memory = SlidingWindowAttention(config)\n",
    "        self.n_layer = config.n_layer\n",
    "        self.percistent = PersistentMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=config.d_model,nhead=config.n_heads,batch_first=True) for _ in range(config.n_layer)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len,d_model = x.size()\n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        out = torch.cat([persistent_tokens,x],dim=1)\n",
    "        \n",
    "        y = self.long_memory(out)\n",
    "        o = self.short_memory(y)\n",
    "        o = o[:,-seq_len:,:]\n",
    "\n",
    "        x = x + out\n",
    "        x = self.short_memory(x)\n",
    "\n",
    "        for layer in self.attn_layers:\n",
    "            x = layer(x)\n",
    "        return x'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb45bf2-9c08-4ad4-88ab-c0093be07270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanMAGLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(config.seq_len, config.d_model))\n",
    "        self.titan = TitanMAG(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len] token ids\n",
    "        B, L = x.size()\n",
    "        emb = self.embedding(x)  # [B, L, d_model]\n",
    "        pos = self.pos_embedding[:L, :].unsqueeze(0)  # [1, L, d_model]\n",
    "        emb = emb + pos\n",
    "        out = self.titan(emb)  # [B, L, d_model]\n",
    "        logits = self.lm_head(out)  # [B, L, vocab_size]\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, prompt, max_length=50):\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                input_ids = torch.tensor([generated[-self.config.seq_len:]], dtype=torch.long).to(next(self.parameters()).device)\n",
    "                logits = self.forward(input_ids)  # [1, seq_len, vocab_size]\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits).item()\n",
    "                generated.append(next_token)\n",
    "                if next_token == self.config.eos_token_id:\n",
    "                    break\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e03fbdd-464c-439c-9281-6f4883057672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Vocabulary size: 44368\n",
      "Total parameters: 25128784\n",
      "Trainable parameters: 25128784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 10.6930, Val Loss: 10.6873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simple_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 322\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m    321\u001b[0m predefined_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 322\u001b[0m prompt_text, generated_text \u001b[38;5;241m=\u001b[39m generate_sample_from_predefined_prompt(model, predefined_prompt, vocab, itos, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt_text)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_text)\n",
      "Cell \u001b[0;32mIn[3], line 242\u001b[0m, in \u001b[0;36mgenerate_sample_from_predefined_prompt\u001b[0;34m(model, predefined_prompt, vocab, itos, max_length, k)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_sample_from_predefined_prompt\u001b[39m(model, predefined_prompt, vocab, itos, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Convert the predefined prompt (a string) into token IDs.\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m simple_tokenizer(predefined_prompt)\n\u001b[1;32m    243\u001b[0m     prompt_ids \u001b[38;5;241m=\u001b[39m [vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m+\u001b[39m [vocab\u001b[38;5;241m.\u001b[39mget(token, vocab[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Generate continuation using the model's generate method.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load WikiText-2 using Hugging Face datasets\n",
    "# and train a BPE tokenizer using the tokenizers library.\n",
    "# -------------------------------\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "\n",
    "# Load WikiText-2 (use 50% of the data for faster experimentation)\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={\n",
    "    \"train\": \"train[:20%]\",\n",
    "    \"validation\": \"validation[:10%]\",\n",
    "    \"test\": \"test[:10%]\"\n",
    "})\n",
    "\n",
    "# Prepare an iterator of training lines (skipping empty ones)\n",
    "def line_iterator(split):\n",
    "    for line in wikitext[split][\"text\"]:\n",
    "        if line.strip():\n",
    "            yield line\n",
    "\n",
    "# Initialize a BPE tokenizer\n",
    "bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "trainer = trainers.BpeTrainer(vocab_size=100000, special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "bpe_tokenizer.train_from_iterator(line_iterator(\"train\"), trainer)\n",
    "\n",
    "bpe_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"<bos> $A <eos>\",\n",
    "    pair=\"<bos> $A <eos> $B:1 <eos>:1\",\n",
    "    special_tokens=[\n",
    "        (\"<bos>\", bpe_tokenizer.token_to_id(\"<bos>\")),\n",
    "        (\"<eos>\", bpe_tokenizer.token_to_id(\"<eos>\"))\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "bpe_tokenizer.save(\"bpe_tokenizer.json\")\n",
    "\n",
    "vocab = bpe_tokenizer.get_vocab() \n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Build inverse mapping for generation.\n",
    "itos = {id: token for token, id in vocab.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create WikiText Dataset for Language Modeling using the BPE tokenizer.\n",
    "# -------------------------------\n",
    "class WikiTextLMDataset(Dataset):\n",
    "    def __init__(self, split, tokenizer, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        data = []\n",
    "        for line in wikitext[split][\"text\"]:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            encoding = tokenizer.encode(line)\n",
    "            # encoding.ids already includes the BOS and EOS from post-processing.\n",
    "            data.extend(encoding.ids)\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.seq_len\n",
    "        x = self.data[i : i+self.seq_len]\n",
    "        y = self.data[i+1 : i+self.seq_len+1]\n",
    "        return x, y\n",
    "\n",
    "seq_len = 128\n",
    "train_dataset = WikiTextLMDataset(split=\"train\", tokenizer=bpe_tokenizer, seq_len=seq_len)\n",
    "valid_dataset = WikiTextLMDataset(split=\"validation\", tokenizer=bpe_tokenizer, seq_len=seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "\n",
    "@dataclass\n",
    "class TitanConfig():\n",
    "    d_model: int = 256\n",
    "    vocab_size: int = None \n",
    "    seq_len: int = 128\n",
    "    n_heads: int = 8\n",
    "    alpha: float = 0.1\n",
    "    eta: float = 0.9\n",
    "    theta: float = 0.01\n",
    "    window_size: int = 256\n",
    "    batch_size: int = 32\n",
    "    n_layers: int = 8\n",
    "    chunk_size: int = 64    # for MAC variant (not used here)\n",
    "    N_p: int = 128\n",
    "    bos_token_id: int = 2   # will update later\n",
    "    eos_token_id: int = 3   # will update later\n",
    "\n",
    "config = TitanConfig(vocab_size=vocab_size, d_model=256, seq_len=seq_len, n_layers=8, N_p=128)\n",
    "\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p, config.d_model))\n",
    "    \n",
    "    def forward(self, batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\", torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\", torch.zeros(config.d_model, config.d_model))\n",
    "        self.query = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        y = torch.matmul(q, self.M)\n",
    "        return y\n",
    "\n",
    "    def update_memory(self, x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        v_pred = torch.matmul(k, self.M)\n",
    "        loss = torch.sum((v_pred - v) ** 2)\n",
    "        error = v_pred - v\n",
    "        g = 2 * torch.matmul(error.t(), k)\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1 - self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss\n",
    "\n",
    "# Sliding-Window Attention Module (using standard MultiheadAttention as a proxy)\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.window_size = config.window_size\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.d_model, num_heads=config.n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        output = []\n",
    "        for i in range(0, seq_len, self.window_size):\n",
    "            x_chunk = x[:, i:i+self.window_size, :]\n",
    "            attn_out, _ = self.attention(x_chunk, x_chunk, x_chunk)\n",
    "            output.append(attn_out)\n",
    "        return torch.cat(output, dim=1)\n",
    "\n",
    "# TitanMAG: Gated Memory (MAG) Architecture\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.window_size = config.window_size\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "        self.layernorm = nn.LayerNorm(config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x_flat = x.reshape(-1, d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        \n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        tilde_x = torch.cat([persistent_tokens, x], dim=1)\n",
    "        \n",
    "        out = tilde_x\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        \n",
    "        tilde_x_flat = tilde_x.reshape(-1, d_model)\n",
    "        memory_retrieval = self.long_memory(tilde_x_flat)\n",
    "        memory_retrieval = memory_retrieval.reshape(batch_size, -1, d_model)\n",
    "        \n",
    "        norm_y = self.layernorm(y)\n",
    "        norm_memory = self.layernorm(memory_retrieval)\n",
    "        combined = norm_y * norm_memory\n",
    "        \n",
    "        output = combined[:, -seq_len:, :]\n",
    "        return output\n",
    "\n",
    "# TitanMAGLM: TitanMAG with LM Head for Language Modeling.\n",
    "class TitanMAGLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(config.seq_len, config.d_model))\n",
    "        self.titan = TitanMAG(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        emb = self.embedding(x)\n",
    "        pos = self.pos_embedding[:L, :].unsqueeze(0)\n",
    "        emb = emb + pos\n",
    "        out = self.titan(emb)\n",
    "        logits = self.lm_head(out)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, k=10):\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                input_ids = torch.tensor([generated[-self.config.seq_len:]], dtype=torch.long).to(next(self.parameters()).device)\n",
    "                logits = self.forward(input_ids)\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                topk_logits, topk_indices = torch.topk(next_token_logits, k)\n",
    "                probs = F.softmax(topk_logits, dim=-1)\n",
    "                next_token = topk_indices[torch.multinomial(probs, num_samples=1)].item()\n",
    "                generated.append(next_token)\n",
    "                if next_token == self.config.eos_token_id:\n",
    "                    break\n",
    "        return generated\n",
    "\n",
    "def generate_sample_from_predefined_prompt(model, predefined_prompt, vocab, itos, max_length=256, k=10):\n",
    "    # Convert the predefined prompt (a string) into token IDs.\n",
    "    tokens = simple_tokenizer(predefined_prompt)\n",
    "    prompt_ids = [vocab[\"<bos>\"]] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    \n",
    "    # Generate continuation using the model's generate method.\n",
    "    generated_ids = model.generate(prompt_ids, max_length=max_length, k=k)\n",
    "    \n",
    "    # Convert the prompt IDs back to text.\n",
    "    prompt_text = \" \".join([itos.get(i, \"<unk>\") for i in prompt_ids])\n",
    "    # The generated text is the continuation after the prompt.\n",
    "    continuation_ids = generated_ids[len(prompt_ids):]\n",
    "    generated_text = \" \".join([itos.get(i, \"<unk>\") for i in continuation_ids])\n",
    "    \n",
    "    return prompt_text, generated_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training and Validation Setup with Checkpointing and tqdm\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.vocab_size = vocab_size\n",
    "config.seq_len = seq_len\n",
    "config.bos_token_id = vocab[\"<bos>\"]\n",
    "config.eos_token_id = vocab[\"<eos>\"]\n",
    "\n",
    "model = TitanMAGLM(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "checkpoint_path = \"titan_checkpoint-3.pth\"\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in progress_bar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total parameters:\", total_params)\n",
    "    print(\"Trainable parameters:\", trainable_params)\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate_epoch(model, valid_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Example usage:\n",
    "    predefined_prompt = \"The \"\n",
    "    prompt_text, generated_text = generate_sample_from_predefined_prompt(model, predefined_prompt, vocab, itos, max_length=256, k=10)\n",
    "    print(\"Prompt:\", prompt_text)\n",
    "    print(\"Continuation:\", generated_text)\n",
    "\n",
    "    \n",
    "    # Save checkpoint after each epoch.\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd70799-e14e-4472-b76a-491df950cb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading WikiText-2 dataset...\n",
      "Loading saved BPE tokenizer...\n",
      "Vocabulary size: 44368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3538/4228794241.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 40\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Train Loss: 7.2734, Val Loss: 8.1746\n",
      "Prompt: <bos> the\n",
      "Continuation: . of a . the and , <bos> <eos>\n",
      "Checkpoint saved at epoch 41\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Train Loss: 7.2782, Val Loss: 8.2301\n",
      "Prompt: <bos> the\n",
      "Continuation: and . the the . in in The . , to , <eos>\n",
      "Checkpoint saved at epoch 42\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Train Loss: 7.2714, Val Loss: 8.2189\n",
      "Prompt: <bos> the\n",
      "Continuation: to and of and = \" , . . . , . = the and , the , , the and the of in the in a of <eos>\n",
      "Checkpoint saved at epoch 43\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Train Loss: 7.2471, Val Loss: 8.2265\n",
      "Prompt: <bos> the\n",
      "Continuation: . the , a , , of @-@ a of in and a the <bos> s to <bos> to , the the the the of , the . in . , \" in the = of . a . to the , in , of , . the the , <eos>\n",
      "Checkpoint saved at epoch 44\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Train Loss: 7.2361, Val Loss: 8.2517\n",
      "Prompt: <bos> the\n",
      "Continuation: the the . the and of , of = . of to and = in of . to the , and of of of = <bos> the of . the . , and a in . , <eos>\n",
      "Checkpoint saved at epoch 45\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Train Loss: 7.2607, Val Loss: 8.2340\n",
      "Prompt: <bos> the\n",
      "Continuation: was the a in in the , the . \" a in the the , . , the a @-@ of , . of , the , , was the in the the the of and the the . and , , and . . , . in the = a in to in of . . , the . . , of a the . of . . in the <bos> = , and <eos>\n",
      "Checkpoint saved at epoch 46\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Train Loss: 7.2305, Val Loss: 8.2516\n",
      "Prompt: <bos> the\n",
      "Continuation: and and . and of . , <bos> , \" <bos> to the of , , and in <eos>\n",
      "Checkpoint saved at epoch 47\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Train Loss: 7.2234, Val Loss: 8.2133\n",
      "Prompt: <bos> the\n",
      "Continuation: in , , in , the to . of and , the = . of : of a = a , in the a the = <bos> . of the , , . = . , <eos>\n",
      "Checkpoint saved at epoch 48\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Train Loss: 7.2069, Val Loss: 8.2397\n",
      "Prompt: <bos> the\n",
      "Continuation: of @-@ a and , , in \" of the the and , the of the the a the , was the , \" in . <eos>\n",
      "Checkpoint saved at epoch 49\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Train Loss: 7.2077, Val Loss: 8.1989\n",
      "Prompt: <bos> the\n",
      "Continuation: . , = \" of and a of s <bos> of in , of , in the . , the = of to the = <bos> , , . in to the . , the = in the . . . the . and and = , \" the a of in and to , the to , a the , of , a . . , , and of = of . The the the to of the , and . the . , . the = the the , the . of to the in , the the a , and of , to a of in of the , the = in the , , and = a , of , of of . and = of . = and of the . the of . in to and and to , a . . , in , , and . , , , the , . in of = the . <bos> in to the of . and . <eos>\n",
      "Checkpoint saved at epoch 50\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Train Loss: 7.2103, Val Loss: 8.2404\n",
      "Prompt: <bos> the\n",
      "Continuation: the . . , . to to @-@ , . . of . in , . of the to the . the = , , of = <bos> the and = to \" , . the a to the . , <eos>\n",
      "Checkpoint saved at epoch 51\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Train Loss: 7.2073, Val Loss: 8.2432\n",
      "Prompt: <bos> the\n",
      "Continuation: the = = . the a in the in . and . , . = , . to the in . , . , the . , and the of of , of the and the <eos>\n",
      "Checkpoint saved at epoch 52\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Train Loss: 7.1946, Val Loss: 8.2368\n",
      "Prompt: <bos> the\n",
      "Continuation: and of to , in the the , . in <eos>\n",
      "Checkpoint saved at epoch 53\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Train Loss: 7.1922, Val Loss: 8.2733\n",
      "Prompt: <bos> the\n",
      "Continuation: in , a to , in , , to , of of to , the a = in , <bos> the , . of , . . . . , . . in \" , . the , in the = in \" and and , = in the a , = , of the and of , to , a and the , , a = to of . , the and \" a The = a and in = to . , , of the to and and the , the , , and \" was . the and the the in to of was <bos> . the in = the the = to in in <bos> the , of to of <eos>\n",
      "Checkpoint saved at epoch 54\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Train Loss: 7.1798, Val Loss: 8.2596\n",
      "Prompt: <bos> the\n",
      "Continuation: of of and \" to , , the , in the the the of a , the the the a , of . , the of , of . in and and a in the the to , the to in the was = , to the = a the . <eos>\n",
      "Checkpoint saved at epoch 55\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Train Loss: 7.1692, Val Loss: 8.2747\n",
      "Prompt: <bos> the\n",
      "Continuation: . the the in in the the , in the . the <eos>\n",
      "Checkpoint saved at epoch 56\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Train Loss: 7.1758, Val Loss: 8.2707\n",
      "Prompt: <bos> the\n",
      "Continuation: . \" to the . a in , the the of the the , the the a and and in the and the the <eos>\n",
      "Checkpoint saved at epoch 57\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Train Loss: 7.1682, Val Loss: 8.2764\n",
      "Prompt: <bos> the\n",
      "Continuation: of the in . the the . to , , and of , . \" , . of and s . and of , , the , . a the , , . to and in . the the \" and a = the to , in \" \" <eos>\n",
      "Checkpoint saved at epoch 58\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Train Loss: 7.1736, Val Loss: 8.2888\n",
      "Prompt: <bos> the\n",
      "Continuation: to in . in of the . of and the . in . to a and \" . the . , , and \" , . , , a . the <bos> , the a = the the of \" , , of and \" the the s a = the the the , of a , of the , and in . the and , . , and to of of , the of in . in . , the of the . , and of a , of , = of . , the to and \" <eos>\n",
      "Checkpoint saved at epoch 59\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Train Loss: 7.1554, Val Loss: 8.2879\n",
      "Prompt: <bos> the\n",
      "Continuation: the , in s , . and and a . = the , of the the <eos>\n",
      "Checkpoint saved at epoch 60\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Train Loss: 7.1567, Val Loss: 8.2964\n",
      "Prompt: <bos> the\n",
      "Continuation: the to the in . of a of , , to a the the , . and and . . in to to <bos> in in , of = = the to of the the a , to , and the in in . . and = and , the = . . a , was a . a a . in to . . a <eos>\n",
      "Checkpoint saved at epoch 61\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Train Loss: 7.1687, Val Loss: 8.2792\n",
      "Prompt: <bos> the\n",
      "Continuation: = of = in <eos>\n",
      "Checkpoint saved at epoch 62\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Train Loss: 7.1625, Val Loss: 8.3223\n",
      "Prompt: <bos> the\n",
      "Continuation: the the of of . the \" and , a the , , . and the in s , <bos> and the of of . the , of , to . to the of = the , the and . and = a . the in to the and of . was . , the . , = of of and the , the and , a <bos> to , . . and to , the the . . = = the the . of in . in , . , and a the the , and . <bos> to a the and = . and the the a = of the a in a the . of in the the \" . . and . . . to , , the . the the of in the and . of = , the a a the and , in the to in , = the . . , and . the the of to a , , in in = . and , . = of . a , , in the , = and a , , the of , the \" a . a in and in the , . , to , of a . , , . to the , the of and in , and of the and the , the = , the <eos>\n",
      "Checkpoint saved at epoch 63\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Train Loss: 7.1556, Val Loss: 8.3295\n",
      "Prompt: <bos> the\n",
      "Continuation: the and of and , the the and the and a and the the the <eos>\n",
      "Checkpoint saved at epoch 64\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Train Loss: 7.1478, Val Loss: 8.2932\n",
      "Prompt: <bos> the\n",
      "Continuation: <bos> and in of the in and in , to and the and the , of and <eos>\n",
      "Checkpoint saved at epoch 65\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Train Loss: 7.1532, Val Loss: 8.3375\n",
      "Prompt: <bos> the\n",
      "Continuation: , . , the the in . a in in and in , . and . the in = and = , \" \" , in . of and of , = . a of , , the , , = the in the . , , <bos> = to . , the , a of the the and , , . \" in and in of the of . . the , , , of = and = \" the , , the , , = . The \" the , in , of , and , , of a the was , to , = in . a . and the , the of the of , a the . . . in and . , , . of = in to = the to , , to the , the . the the , a the of and , of a = and and . to , , to , = and . = of . the and in in and in , <bos> and a in and . to the the , in of , a = the \" of the , = the in , the , the the to the . and = . of , <bos> the \" and in , . , the in in = , the a , the , and a the in of a in a a . . of = the the the a , . the to . @-@ and , to\n",
      "Checkpoint saved at epoch 66\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Train Loss: 7.1557, Val Loss: 8.3153\n",
      "Prompt: <bos> the\n",
      "Continuation: , the in <bos> a in <bos> the in and a , of . = the a the of a , and \" to = , a , a , the the the . , , a and a to a , in = a = , , . , \" , the , the a , \" and the , the the of = of the of . the \" to to a . . and <eos>\n",
      "Checkpoint saved at epoch 67\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Train Loss: 7.1387, Val Loss: 8.3658\n",
      "Prompt: <bos> the\n",
      "Continuation: to a , <bos> in <bos> , , of the , . to in in a <eos>\n",
      "Checkpoint saved at epoch 68\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Train Loss: 7.1536, Val Loss: 8.3014\n",
      "Prompt: <bos> the\n",
      "Continuation: the <bos> the in the \" of was , the , the of the , and . = to \" to the \" a to of a the the the a the and and of . the \" , of and a . and to , was , \" of \" the , the of the . to the and the , and , of = <bos> a the a , , in of and . of . to . in . a of of in of . the . to of of , and to the the a , of the = . of the to to the , , . , the . a in . the . . = the of , , in of , \" of the of \" in the the . to , . , of of of , , a , <eos>\n",
      "Checkpoint saved at epoch 69\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Train Loss: 7.1435, Val Loss: 8.3134\n",
      "Prompt: <bos> the\n",
      "Continuation: and . and = <eos>\n",
      "Checkpoint saved at epoch 70\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Train Loss: 7.1294, Val Loss: 8.3714\n",
      "Prompt: <bos> the\n",
      "Continuation: a <eos>\n",
      "Checkpoint saved at epoch 71\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Train Loss: 7.1332, Val Loss: 8.3615\n",
      "Prompt: <bos> the\n",
      "Continuation: the and the s and of = and the , , a . the , the . , the . \" and the . of , , . the , the = to the of = = . to a and the . of the the of in and , a and , of , in . the a in a <bos> , <bos> . . in the a = , and . , a , and the , to a of , the , <bos> and <eos>\n",
      "Checkpoint saved at epoch 72\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Train Loss: 7.1285, Val Loss: 8.3272\n",
      "Prompt: <bos> the\n",
      "Continuation: of , = to . in . and of in the a and in , , to = , = of and , , the . of of . . in <bos> and , and and , <eos>\n",
      "Checkpoint saved at epoch 73\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Train Loss: 7.1282, Val Loss: 8.3795\n",
      "Prompt: <bos> the\n",
      "Continuation: . , and . of = in , . of and and = of , of in in the the . in , , , in . , , \" a a the to the of the and , a of \" a , in in , a of and , , and . and . and \" , a the the the = a , , the , to , <eos>\n",
      "Checkpoint saved at epoch 74\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Train Loss: 7.1185, Val Loss: 8.3022\n",
      "Prompt: <bos> the\n",
      "Continuation: in = . of . , . . and of the . . in = \" to \" of , the = to , , in in , to the and to and the of to in , <eos>\n",
      "Checkpoint saved at epoch 75\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Train Loss: 7.1350, Val Loss: 8.3701\n",
      "Prompt: <bos> the\n",
      "Continuation: a the , and <eos>\n",
      "Checkpoint saved at epoch 76\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Train Loss: 7.1334, Val Loss: 8.3891\n",
      "Prompt: <bos> the\n",
      "Continuation: to the the the , . and in and and , and . and . . and the \" the , in , to and of the the <bos> and . , the of , , of of and in to of in the = in . <bos> , , of . . and = the of . . of the in of the , . = = the . the and the of of . a the in the the = of in . , of , the of and in , a and and . a the the and and \" <bos> , of . , , a , and the , of of . , , the to , = , a in and the . , , , and . = , to of in the = the . in in , the = , , in \" , and . the . . , and in , of = , <eos>\n",
      "Checkpoint saved at epoch 77\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Train Loss: 7.1192, Val Loss: 8.4090\n",
      "Prompt: <bos> the\n",
      "Continuation: of = a to <bos> a a , of the a , the <bos> , and and . to , . the , of of and in , the to . . a , the = , , and . a the a , , and the the the the and , , a , of of of in in . the the = a of \" of , the a = in = of the and the , of of of the in of , . in and of to a and , the the , , a , and the a , a and of the of . in a , the of and to . and , . in . of the , the the of in = of and the the a the a , in of the <eos>\n",
      "Checkpoint saved at epoch 78\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Train Loss: 7.1278, Val Loss: 8.4292\n",
      "Prompt: <bos> the\n",
      "Continuation: . , , <eos>\n",
      "Checkpoint saved at epoch 79\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Train Loss: 7.1236, Val Loss: 8.3709\n",
      "Prompt: <bos> the\n",
      "Continuation: , the and the . in \" , . a a of the a in = to in \" \" and . , \" the the the , to to , , , and of the the the , the the , , = to . . \" the <bos> the \" the in the = of . . the , , , the in \" of the and the the , in the in . the . to and the = , the of and and of in <bos> . to , the and the = the the of and a , the \" to , of . and , . in , to , \" <eos>\n",
      "Checkpoint saved at epoch 80\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Train Loss: 7.1205, Val Loss: 8.3714\n",
      "Prompt: <bos> the\n",
      "Continuation: , . \" \" in in . , and , of and the the and and the the , . , the , of = , , and in the and , the of and the the the , , the <bos> of the and the , in the , , . , , in the in a the the . . of and of , the of , , . and the . . , . the the in , the , the of of . in the the in = to the , . , the . , the . . and to <eos>\n",
      "Checkpoint saved at epoch 81\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Train Loss: 7.1151, Val Loss: 8.4010\n",
      "Prompt: <bos> the\n",
      "Continuation: the , the of to , , the = and the , a = of , and , the . in a the the of , the the the . , in . the <eos>\n",
      "Checkpoint saved at epoch 82\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Train Loss: 7.1172, Val Loss: 8.4743\n",
      "Prompt: <bos> the\n",
      "Continuation: . of and , the the = \" . in , , in . the in = to the , . , \" . , the a the in and = , , <bos> . <bos> of . of and and , the and , . \" . of . the . the the and , to . the the to in . of . the the the a , . . . the , the the of of the the , and the the of of to , the = . to of to = to , a the and of the . the <bos> , the of and of = , the the in of <bos> and , , to the the and and , , <eos>\n",
      "Checkpoint saved at epoch 83\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Train Loss: 7.1121, Val Loss: 8.3712\n",
      "Prompt: <bos> the\n",
      "Continuation: the the a was and to and . a of \" , . , . of the , to \" . the a . the to of = @-@ , in the the , to a the , = = a \" and a of , in a the , the the to , \" and in the the to the to the to in \" <eos>\n",
      "Checkpoint saved at epoch 84\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Train Loss: 7.1175, Val Loss: 8.4676\n",
      "Prompt: <bos> the\n",
      "Continuation: . a , and , <bos> a the , , to , in . to of , , a . the , in to . . . , , the and , to the and a the the the to and and in = of , to . and the in a , and in and a , a the . the the = = . , . of and = = . , a the the , . in , and <bos> to , in the and , the a , \" of . the , , . and the the . the , the to \" in the , <bos> <bos> , , in . a to to , a . the the the the . of <eos>\n",
      "Checkpoint saved at epoch 85\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Train Loss: 7.1201, Val Loss: 8.4070\n",
      "Prompt: <bos> the\n",
      "Continuation: , and a in a the a . , the , to the <eos>\n",
      "Checkpoint saved at epoch 86\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Train Loss: 7.1111, Val Loss: 8.4381\n",
      "Prompt: <bos> the\n",
      "Continuation: to in in , . , . and a the of . of and and , and of \" the the the , to the to the the , in and , to = of the = <bos> of the the of the to , to and , , the of and \" of , <eos>\n",
      "Checkpoint saved at epoch 87\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Train Loss: 7.1074, Val Loss: 8.4193\n",
      "Prompt: <bos> the\n",
      "Continuation: , , to , <bos> , the the of and , . <bos> in in , \" , in the of , , of the of to and the . . the the and to , , and to the to and , = to and . = \" the \" , . , in the , to the the . = = = , <eos>\n",
      "Checkpoint saved at epoch 88\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Train Loss: 7.1157, Val Loss: 8.4208\n",
      "Prompt: <bos> the\n",
      "Continuation: the to the was and and a of a of , , , . , . the . of . a the . the the the in and and the of the . . \" in to to , of the . the . , of , and . to <bos> , the . the , to , the a a . , = of a , and = . and in , . the \" and . , in and = the the the to , . , , the to , the the a <eos>\n",
      "Checkpoint saved at epoch 89\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Train Loss: 7.1065, Val Loss: 8.4909\n",
      "Prompt: <bos> the\n",
      "Continuation: . . the of of the , . a . , , and . . , . of . , the to , the = to the . the . = of . . in , the the the . the . , , and . the the a . of a of . a to the , the , of the . the . the the in the a to , the the a the , in , . . , . . of to , and and . , the and = , . = of . , , . and the to , the the = in , the a . a = , to . , the . in \" . to of of . of the of and . to to and a , , , in the . and the , , , \" to the , the . , . , = of , \" , in \" of and the the , . and the . to and , the in \" in , . , . of and of , . , of = . . the <bos> . to the to the , . , the of the , \" the . the in and the , of , of of , the the the of . = . the the , of and , , , , , the and \" , , the . <eos>\n",
      "Checkpoint saved at epoch 90\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Train Loss: 7.1054, Val Loss: 8.4500\n",
      "Prompt: <bos> the\n",
      "Continuation: the the in . . . and and the <bos> of , of to in and = a in the . = the a \" a a and , in the the a , in to the of of a . to a the a the <eos>\n",
      "Checkpoint saved at epoch 91\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Train Loss: 7.1044, Val Loss: 8.3933\n",
      "Prompt: <bos> the\n",
      "Continuation: . , , the the . the <bos> the in to and a to of = the \" \" in . , . . the the <bos> of a the the the in , \" of in to of \" the the to = , . , \" in the = the = \" , , . in of and the in the a = . and a . . the of the in to the the . the the in in , , and the of . . the the . = of of in = to , of = , in , , . of to <eos>\n",
      "Checkpoint saved at epoch 92\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Train Loss: 7.1012, Val Loss: 8.4113\n",
      "Prompt: <bos> the\n",
      "Continuation: to to and was and in the the , \" a the a a . of , , the the the and to to and , to . of of the and . in = and . the and to a = in the of in , the in , of . and <eos>\n",
      "Checkpoint saved at epoch 93\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Train Loss: 7.1210, Val Loss: 8.4169\n",
      "Prompt: <bos> the\n",
      "Continuation: a the and <bos> of the , of of . in in , \" of in the a in and in , to the . , the a , , = . the = to the . of . . of of , . , in = , , . . the the = , the , to of and to , in of a and the \" in and , of . and the the a a , . of to the = , of of the the a a of . \" the a the , , , in . and of , to of , . the to in , the , the the a in , to , . , \" of the , . the and the = <eos>\n",
      "Checkpoint saved at epoch 94\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Train Loss: 7.1002, Val Loss: 8.4427\n",
      "Prompt: <bos> the\n",
      "Continuation: the . and the , of , , of to in . \" <eos>\n",
      "Checkpoint saved at epoch 95\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Train Loss: 7.1102, Val Loss: 8.4042\n",
      "Prompt: <bos> the\n",
      "Continuation: to , , , , and the . , to the the and and . a to of , the the a . \" the , in the , of in <bos> , , the a , the , . , . , = , the the the the , the . in the to to <eos>\n",
      "Checkpoint saved at epoch 96\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Train Loss: 7.1062, Val Loss: 8.4572\n",
      "Prompt: <bos> the\n",
      "Continuation: the the the the , of , , \" of and , . to of , the a the of a of . , and , , , in , , of of to , . . the , , the was , the and a . of and the the in in and , of <bos> the the \" of of . in . the of of in , = , the the of the the \" . in \" to the to of , in \" the . to the the to . , of a in and a . of the , and , , , the . the = and = and the \" , the of , in to in and , was a the the to of to to the , of <bos> a to , the , to , \" the of , and in the , the the of . , and , to a in . of \" , = , of . . , = the . , of and , , and <bos> a in and a and of of . and = in . of , , was a in , the , the , in and a , of , the was to the to to , the . of . to a the . of the , , the . . of the and the the to \" to , . . and to . to and in . . , ,\n",
      "Checkpoint saved at epoch 97\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Train Loss: 7.0996, Val Loss: 8.4595\n",
      "Prompt: <bos> the\n",
      "Continuation: to <bos> the a a , of . and , the . . \" and . of the in , a to of of = of , the , the a to a , the , the of of , the . and the , , in \" , to the = the = the the , and . of = to of to in , . of to the to was . to . . in the . to a the the in . in to the = the the , in , , . and of of in in the in , , and . in the <bos> of . , . \" . to the , = a of in in . the the . the of \" . . = = of to of . a in the the . to . the and , . and to , to , of , in to of the the and the of a in to a = of the in <eos>\n",
      "Checkpoint saved at epoch 98\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Train Loss: 7.1106, Val Loss: 8.4235\n",
      "Prompt: <bos> the\n",
      "Continuation: a , and of the and the to of , in to of to to and the the the and , of to a , a a , the a to , a = the the the , and and . the to a , of , the to the in , a to a of in , the in , of in the the \" . , in to in the of the and . the in a , = = and = a , the the = , , of to in the of a of <bos> and , in . the . of , = the = . of the , in the = a to , the a , a a and to , the to of of to = = of to <eos>\n",
      "Checkpoint saved at epoch 99\n",
      "Total parameters: 24569680\n",
      "Trainable parameters: 24569680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train Loss: 7.0985, Val Loss: 8.4808\n",
      "Prompt: <bos> the\n",
      "Continuation: , the in and . . of , and , a of . the the , the and a and to = in and , the <bos> <eos>\n",
      "Checkpoint saved at epoch 100\n",
      "Chatbot is ready! Type 'exit' or 'quit' to stop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Define a Simple Tokenizer Function (for prompt processing)\n",
    "# -------------------------------\n",
    "def simple_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load WikiText-2 and Initialize/Train a Custom BPE Tokenizer\n",
    "# -------------------------------\n",
    "print(\"Loading WikiText-2 dataset...\")\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split={\n",
    "    \"train\": \"train[:10%]\",\n",
    "    \"validation\": \"validation[:10%]\",\n",
    "    \"test\": \"test[:10%]\"\n",
    "})\n",
    "\n",
    "# Check if a saved tokenizer exists; otherwise, train one.\n",
    "tokenizer_path = \"bpe_tokenizer.json\"\n",
    "if os.path.exists(tokenizer_path):\n",
    "    print(\"Loading saved BPE tokenizer...\")\n",
    "    bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "else:\n",
    "    print(\"Training a new BPE tokenizer...\")\n",
    "    def line_iterator(split):\n",
    "        for line in wikitext[split][\"text\"]:\n",
    "            if line.strip():\n",
    "                yield line\n",
    "    bpe_tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=100000, special_tokens=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    bpe_tokenizer.train_from_iterator(line_iterator(\"train\"), trainer)\n",
    "    bpe_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<bos> $A <eos>\",\n",
    "        pair=\"<bos> $A <eos> $B:1 <eos>:1\",\n",
    "        special_tokens=[\n",
    "            (\"<bos>\", bpe_tokenizer.token_to_id(\"<bos>\")),\n",
    "            (\"<eos>\", bpe_tokenizer.token_to_id(\"<eos>\"))\n",
    "        ],\n",
    "    )\n",
    "    bpe_tokenizer.save(tokenizer_path)\n",
    "    print(\"Tokenizer saved to\", tokenizer_path)\n",
    "\n",
    "# Build vocabulary and inverse mapping from the BPE tokenizer.\n",
    "vocab = bpe_tokenizer.get_vocab()  # dict: token -> id\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "itos = {id: token for token, id in vocab.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Create WikiText Dataset for LM using the Custom BPE Tokenizer\n",
    "# -------------------------------\n",
    "class WikiTextLMDataset(Dataset):\n",
    "    def __init__(self, split, tokenizer, seq_len):\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        data = []\n",
    "        for line in wikitext[split][\"text\"]:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            # The post-processor adds <bos> and <eos>\n",
    "            encoding = tokenizer.encode(line)\n",
    "            data.extend(encoding.ids)\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.data) - 1) // self.seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.seq_len\n",
    "        x = self.data[i : i+self.seq_len]\n",
    "        y = self.data[i+1 : i+self.seq_len+1]\n",
    "        return x, y\n",
    "\n",
    "seq_len = 64\n",
    "train_dataset = WikiTextLMDataset(split=\"train\", tokenizer=bpe_tokenizer, seq_len=seq_len)\n",
    "valid_dataset = WikiTextLMDataset(split=\"validation\", tokenizer=bpe_tokenizer, seq_len=seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Define Titan Configuration and Model Components\n",
    "# -------------------------------\n",
    "@dataclass\n",
    "class TitanConfig:\n",
    "    d_model: int = 256\n",
    "    vocab_size: int = 100000  # will update below\n",
    "    seq_len: int = 64\n",
    "    n_heads: int = 8\n",
    "    alpha: float = 0.1\n",
    "    eta: float = 0.9\n",
    "    theta: float = 0.01\n",
    "    window_size: int = 128\n",
    "    batch_size: int = 32\n",
    "    n_layers: int = 6\n",
    "    chunk_size: int = 64    # for MAC variant (not used here)\n",
    "    N_p: int = 64\n",
    "    bos_token_id: int = 2   # will update below\n",
    "    eos_token_id: int = 3   # will update below\n",
    "\n",
    "config = TitanConfig(vocab_size=vocab_size, d_model=256, seq_len=seq_len, n_layers=6, N_p=64)\n",
    "\n",
    "# Persistent Memory Module\n",
    "class PersistentMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.persistent = nn.Parameter(torch.randn(config.N_p, config.d_model))\n",
    "    \n",
    "    def forward(self, batch_size):\n",
    "        return self.persistent.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "# Titan Memory Module (Long-Term Memory)\n",
    "class TitanMemory(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.register_buffer(\"M\", torch.eye(config.d_model))\n",
    "        self.register_buffer(\"S\", torch.zeros(config.d_model, config.d_model))\n",
    "        self.query = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.key = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.value = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "        self.alpha = config.alpha\n",
    "        self.eta = config.eta\n",
    "        self.theta = config.theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        return torch.matmul(q, self.M)\n",
    "\n",
    "    def update_memory(self, x):\n",
    "        B = x.size(0)\n",
    "        if B != 1:\n",
    "            for i in range(B):\n",
    "                self.update_memory(x[i:i+1])\n",
    "            return\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        v_pred = torch.matmul(k, self.M)\n",
    "        loss = torch.sum((v_pred - v) ** 2)\n",
    "        error = v_pred - v\n",
    "        g = 2 * torch.matmul(error.t(), k)\n",
    "        self.S = self.eta * self.S - self.theta * g\n",
    "        self.S = torch.clamp(self.S, -1e3, 1e3)\n",
    "        self.M = (1 - self.alpha) * self.M + self.S\n",
    "        self.M = torch.clamp(self.M, -1e3, 1e3)\n",
    "        return loss\n",
    "\n",
    "# Sliding-Window Attention Module (using standard MultiheadAttention as a proxy)\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=config.d_model, num_heads=config.n_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x, x, x)[0]\n",
    "\n",
    "# TitanMAG: Gated Memory (MAG) Architecture\n",
    "class TitanMAG(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.long_memory = TitanMemory(config)\n",
    "        self.attn_layers = nn.ModuleList([SlidingWindowAttention(config) for _ in range(config.n_layers)])\n",
    "        self.persistent = PersistentMemory(config)\n",
    "        self.layernorm = nn.LayerNorm(config.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x_flat = x.reshape(-1, d_model)\n",
    "        with torch.no_grad():\n",
    "            self.long_memory.update_memory(x_flat)\n",
    "        \n",
    "        persistent_tokens = self.persistent(batch_size)\n",
    "        tilde_x = torch.cat([persistent_tokens, x], dim=1)\n",
    "        \n",
    "        out = tilde_x\n",
    "        for layer in self.attn_layers:\n",
    "            out = layer(out)\n",
    "        y = out\n",
    "        \n",
    "        tilde_x_flat = tilde_x.reshape(-1, d_model)\n",
    "        memory_retrieval = self.long_memory(tilde_x_flat)\n",
    "        memory_retrieval = memory_retrieval.reshape(batch_size, -1, d_model)\n",
    "        \n",
    "        norm_y = self.layernorm(y)\n",
    "        norm_memory = self.layernorm(memory_retrieval)\n",
    "        combined = norm_y * norm_memory\n",
    "        \n",
    "        output = combined[:, -seq_len:, :]\n",
    "        return output\n",
    "\n",
    "# TitanMAGLM: TitanMAG with LM Head for Language Modeling.\n",
    "class TitanMAGLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(config.seq_len, config.d_model))\n",
    "        self.titan = TitanMAG(config)\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, L = x.size()\n",
    "        emb = self.embedding(x) + self.pos_embedding[:L, :].unsqueeze(0)\n",
    "        out = self.titan(emb)\n",
    "        logits = self.lm_head(out)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, prompt, max_length=50, k=10, temperature=1.0):\n",
    "        self.eval()\n",
    "        generated = prompt.copy()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                input_ids = torch.tensor([generated[-self.config.seq_len:]], dtype=torch.long).to(next(self.parameters()).device)\n",
    "                logits = self.forward(input_ids)\n",
    "                # Get logits for the last token and apply temperature scaling.\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                topk_logits, topk_indices = torch.topk(next_token_logits, k)\n",
    "                probs = F.softmax(topk_logits, dim=-1)\n",
    "                next_token = topk_indices[torch.multinomial(probs, num_samples=1)].item()\n",
    "                generated.append(next_token)\n",
    "                if next_token == self.config.eos_token_id:\n",
    "                    break\n",
    "        return generated\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training/Validation Setup with Checkpointing and tqdm\n",
    "# -------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.vocab_size = vocab_size\n",
    "config.seq_len = seq_len\n",
    "config.bos_token_id = vocab[\"<bos>\"]\n",
    "config.eos_token_id = vocab[\"<eos>\"]\n",
    "\n",
    "model = TitanMAGLM(config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "checkpoint_path = \"titan_checkpoint-1.pth\"\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for x, y in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_epoch(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for x, y in progress_bar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, config.vocab_size), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def generate_sample_from_predefined_prompt(model, predefined_prompt, vocab, itos, max_length=256, k=10, temperature=1.0):\n",
    "    tokens = simple_tokenizer(predefined_prompt)\n",
    "    prompt_ids = [vocab[\"<bos>\"]] + [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
    "    generated_ids = model.generate(prompt_ids, max_length=max_length, k=k, temperature=temperature)\n",
    "    prompt_text = \" \".join([itos.get(i, \"<unk>\") for i in prompt_ids])\n",
    "    continuation_ids = generated_ids[len(prompt_ids):]\n",
    "    generated_text = \" \".join([itos.get(i, \"<unk>\") for i in continuation_ids])\n",
    "    return prompt_text, generated_text\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Training Loop with Checkpoint Saving and Sampling\n",
    "# -------------------------------\n",
    "num_epochs = 100\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Total parameters:\", total_params)\n",
    "    print(\"Trainable parameters:\", trainable_params)\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate_epoch(model, valid_loader, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    predefined_prompt = \"The \"\n",
    "    prompt_text, generated_text = generate_sample_from_predefined_prompt(model, predefined_prompt, vocab, itos, max_length=256, k=10, temperature=1.0)\n",
    "    print(\"Prompt:\", prompt_text)\n",
    "    print(\"Continuation:\", generated_text)\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "print(\"Chatbot is ready! Type 'exit' or 'quit' to stop.\")\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    prompt_text, response = generate_sample_from_predefined_prompt(model, user_input, vocab, itos, max_length=100, k=10, temperature=1.0)\n",
    "    print(\"Bot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7e56d7-5964-4f99-aac1-a8e4475da010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
